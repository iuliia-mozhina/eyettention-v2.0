{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "nuclear-dream",
      "metadata": {
        "id": "nuclear-dream"
      },
      "source": [
        "# Eyettention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0978c758-ea14-4df4-97d0-6e0c1ed18428",
      "metadata": {
        "id": "0978c758-ea14-4df4-97d0-6e0c1ed18428"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import model\n",
        "import torch\n",
        "from torch.utils import model_zoo\n",
        "import pandas as pd\n",
        "from utils import *\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, RMSprop\n",
        "from transformers import BertTokenizerFast\n",
        "from model import Eyettention\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from torch.nn.functional import cross_entropy, softmax\n",
        "from collections import deque, Counter\n",
        "import pickle\n",
        "from transformers import BertTokenizer\n",
        "from evaluate_e_z_reader_model import *\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import random\n",
        "from scasim import *\n",
        "from uniform_model import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8dfb0f8-8775-4609-b75c-6052916ca9c0",
      "metadata": {
        "id": "c8dfb0f8-8775-4609-b75c-6052916ca9c0"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "#DEVICE = 'cuda'\n",
        "DEVICE = 'cpu'\n",
        "test_mode = 'text'\n",
        "#test_mode = 'subject'\n",
        "scanpath_gen_flag = True\n",
        "atten_type = \"local_g\"\n",
        "save_data_folder = \"./drive/MyDrive/results/BSC/New_Sentence\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4l7DoNdYwUn9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l7DoNdYwUn9",
        "outputId": "8f4c5d47-1914-43f5-9702-59a00bcb0f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e085041-7c16-44b4-9129-bf667f64c552",
      "metadata": {
        "id": "6e085041-7c16-44b4-9129-bf667f64c552"
      },
      "source": [
        "**Training loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28WYhnrePEZj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "28WYhnrePEZj",
        "outputId": "aaad3ae1-f3d9-4aa6-c553-bf0d5ba9b4b1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start evaluating on new sentences.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keeping Bert with pre-trained weights\n",
            "Evaluating for fold 0\n",
            "location_preds_test (256, 39, 51) [[[4.54602969e-08 9.13757336e-09 1.67895797e-09 ... 8.48206128e-08\n",
            "   6.16942444e-08 3.21337791e-12]\n",
            "  [7.09441701e-06 5.73373882e-06 5.15092734e-06 ... 7.95258620e-06\n",
            "   9.57119482e-06 1.95484222e-06]\n",
            "  [3.68721485e-05 3.63957952e-05 1.82124022e-05 ... 1.56488586e-05\n",
            "   3.34967117e-05 2.91079723e-05]\n",
            "  ...\n",
            "  [5.52457832e-02 7.26035563e-03 6.02358428e-04 ... 3.82101387e-02\n",
            "   3.86714382e-04 3.24004382e-01]\n",
            "  [5.52497879e-02 7.26156961e-03 6.02403947e-04 ... 3.82108167e-02\n",
            "   3.86760774e-04 3.24033022e-01]\n",
            "  [5.52525558e-02 7.26296566e-03 6.02525601e-04 ... 3.82121764e-02\n",
            "   3.86857893e-04 3.24046880e-01]]\n",
            "\n",
            " [[4.54602969e-08 9.13757336e-09 1.67895797e-09 ... 8.48206128e-08\n",
            "   6.16942444e-08 3.21337791e-12]\n",
            "  [5.25053838e-06 4.00522822e-06 4.20089555e-06 ... 7.03773276e-06\n",
            "   7.95766391e-06 3.59048249e-06]\n",
            "  [1.67855353e-06 2.39935457e-06 1.86936074e-06 ... 1.38413395e-06\n",
            "   1.86770694e-06 3.25827241e-05]\n",
            "  ...\n",
            "  [5.51940054e-02 7.28453230e-03 6.06049376e-04 ... 3.81421745e-02\n",
            "   3.88319808e-04 3.23805928e-01]\n",
            "  [5.52032366e-02 7.28373043e-03 6.05691515e-04 ... 3.81450243e-02\n",
            "   3.88147309e-04 3.23874533e-01]\n",
            "  [5.52109331e-02 7.28328014e-03 6.05467823e-04 ... 3.81496325e-02\n",
            "   3.88069631e-04 3.23919058e-01]]\n",
            "\n",
            " [[4.54602969e-08 9.13757336e-09 1.67895797e-09 ... 8.48206128e-08\n",
            "   6.16942444e-08 3.21337791e-12]\n",
            "  [6.81472557e-06 5.38895893e-06 4.79877963e-06 ... 7.71826672e-06\n",
            "   9.28444570e-06 1.98882913e-06]\n",
            "  [1.14091690e-05 1.24784674e-05 6.81446727e-06 ... 6.23164897e-06\n",
            "   1.11169475e-05 1.46054008e-05]\n",
            "  ...\n",
            "  [5.51468767e-02 7.29190791e-03 6.08780363e-04 ... 3.81520130e-02\n",
            "   3.89941881e-04 3.23345661e-01]\n",
            "  [5.51614799e-02 7.28955399e-03 6.07711845e-04 ... 3.81382406e-02\n",
            "   3.89202876e-04 3.23547214e-01]\n",
            "  [5.51755652e-02 7.28748832e-03 6.06876682e-04 ... 3.81333232e-02\n",
            "   3.88683577e-04 3.23696911e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[5.66652254e-08 1.18324763e-08 2.24190777e-09 ... 1.05604826e-07\n",
            "   7.73333539e-08 4.64085671e-12]\n",
            "  [5.06436027e-06 3.87467890e-06 4.10594794e-06 ... 6.85411214e-06\n",
            "   7.76553406e-06 3.73274770e-06]\n",
            "  [1.38539153e-05 1.39296026e-05 1.01152600e-05 ... 7.17749435e-06\n",
            "   1.40051689e-05 3.43310785e-05]\n",
            "  ...\n",
            "  [5.52315228e-02 7.25470390e-03 6.01782871e-04 ... 3.82011384e-02\n",
            "   3.86183732e-04 3.24006408e-01]\n",
            "  [5.52427657e-02 7.25440728e-03 6.01481705e-04 ... 3.82055417e-02\n",
            "   3.86067579e-04 3.24072897e-01]\n",
            "  [5.52511401e-02 7.25459214e-03 6.01345731e-04 ... 3.82109992e-02\n",
            "   3.86058557e-04 3.24112445e-01]]\n",
            "\n",
            " [[5.66652290e-08 1.18324772e-08 2.24190799e-09 ... 1.05604833e-07\n",
            "   7.73332047e-08 4.64086582e-12]\n",
            "  [5.40921701e-06 4.06195159e-06 4.12113241e-06 ... 7.20745220e-06\n",
            "   8.16793909e-06 3.50202163e-06]\n",
            "  [2.64064897e-06 3.72100089e-06 2.09130735e-06 ... 1.74115303e-06\n",
            "   2.50909238e-06 8.93865399e-06]\n",
            "  ...\n",
            "  [5.52359708e-02 7.26685207e-03 6.03622117e-04 ... 3.82204503e-02\n",
            "   3.87532520e-04 3.23919266e-01]\n",
            "  [5.52401245e-02 7.26741087e-03 6.03528344e-04 ... 3.82175818e-02\n",
            "   3.87466949e-04 3.23956817e-01]\n",
            "  [5.52433059e-02 7.26814475e-03 6.03512046e-04 ... 3.82160842e-02\n",
            "   3.87457403e-04 3.23980659e-01]]\n",
            "\n",
            " [[5.66652290e-08 1.18324772e-08 2.24190799e-09 ... 1.05604833e-07\n",
            "   7.73332047e-08 4.64086582e-12]\n",
            "  [4.96260827e-06 3.52758798e-06 4.40537815e-06 ... 7.97017401e-06\n",
            "   8.56615134e-06 1.05379822e-05]\n",
            "  [1.21313929e-06 1.53849874e-06 1.69070802e-06 ... 8.91498985e-07\n",
            "   1.74046852e-06 1.32046525e-05]\n",
            "  ...\n",
            "  [5.51631972e-02 7.28986412e-03 6.09555515e-04 ... 3.82279344e-02\n",
            "   3.91216832e-04 3.23014438e-01]\n",
            "  [5.51659465e-02 7.29002617e-03 6.08688395e-04 ... 3.81890573e-02\n",
            "   3.90334782e-04 3.23259503e-01]\n",
            "  [5.51721081e-02 7.28952605e-03 6.07920636e-04 ... 3.81648652e-02\n",
            "   3.89654975e-04 3.23453963e-01]]]\n",
            "label_test (256, 39) [[25 26 27 ... 50 50 50]\n",
            " [25 26 26 ... 50 50 50]\n",
            " [25 26 26 ... 50 50 50]\n",
            " ...\n",
            " [25 26 26 ... 50 50 50]\n",
            " [25 26 27 ... 50 50 50]\n",
            " [25 25 26 ... 50 50 50]]\n",
            "pad_mask_test (256, 39) [[False False False ...  True  True  True]\n",
            " [False False False ...  True  True  True]\n",
            " [False False False ...  True  True  True]\n",
            " ...\n",
            " [False False False ...  True  True  True]\n",
            " [False False False ...  True  True  True]\n",
            " [False False False ...  True  True  True]]\n",
            "uniform_output (256, 39, 51) [[[0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  ...\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]]\n",
            "\n",
            " [[0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  ...\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]]\n",
            "\n",
            " [[0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  ...\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  ...\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]]\n",
            "\n",
            " [[0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  ...\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]]\n",
            "\n",
            " [[0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  ...\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]\n",
            "  [0.01960784 0.01960784 0.01960784 ... 0.01960784 0.01960784 0.01960784]]]\n",
            "uniform_nll -5.672425270080566 [-5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566, -5.672425270080566]\n",
            "######### Eyettention 2.0 model evaluation ##########\n",
            "MSE for durations 0.06998403425222932\n",
            "MSE for landing positions 0.04278604221417481\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-b7b313caaca7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    386\u001b[0m                                         \u001b[0msn_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msn_attention_mask_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                                         \u001b[0;31m# compute the scan path generated from the model when the first CLS token is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m \t\t\t\t\tsp_dnn, _, dur_dnn, land_pos_dnn = dnn.scanpath_generation(sn_emd=sn_input_ids_test,\n\u001b[0m\u001b[1;32m    389\u001b[0m                                                                                                                  \u001b[0msn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msn_attention_mask_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                                                                                                                  \u001b[0mword_ids_sn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mscanpath_generation\u001b[0;34m(self, sn_emd, sn_mask, word_ids_sn, sn_word_len, le, sn_word_freq, sn_pred, sp_fix_dur, sp_landing_pos, max_pred_len)\u001b[0m\n\u001b[1;32m    337\u001b[0m                             ):\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# compute the scan path generated from the model when the first CLS taken is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_mask_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msn_emd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_ids_sn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_word_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_word_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msn_mask_word\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0msn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sn_emd, sn_mask, word_ids_sn, sn_word_len, sn_word_freq, sn_pred)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_emd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_ids_sn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_word_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_word_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Word-Sequence Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msn_emd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mhidden_rep_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword_ids_sn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1138\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 )\n\u001b[1;32m    689\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    691\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    623\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\tgpu = 0\n",
        "\n",
        "\ttorch.set_default_tensor_type('torch.FloatTensor')\n",
        "\tavailbl = torch.cuda.is_available()\n",
        "\tif availbl:\n",
        "\t\tdevice = f'cuda:{gpu}'\n",
        "\telse:\n",
        "\t\tdevice = 'cpu'\n",
        "\t#torch.cuda.set_device(gpu)\n",
        "\n",
        "\tcf = {\"model_pretrained\": \"bert-base-chinese\",\n",
        "\t\t\t\"lr\": 1e-3,\n",
        "\t\t\t\"max_grad_norm\": 10,\n",
        "\t\t\t\"n_epochs\": 150,  # 1000\n",
        "\t\t\t\"n_folds\": 5,\n",
        "\t\t\t\"dataset\": 'BSC',\n",
        "\t\t\t\"atten_type\": 'local-g',\n",
        "\t\t\t\"batch_size\": 256,\n",
        "\t\t\t\"max_sn_len\": 27, #include start token and end token\n",
        "\t\t\t\"max_sp_len\": 40, #include start token and end token\n",
        "\t\t\t\"norm_type\": \"z-score\",\n",
        "\t\t\t\"earlystop_patience\": 20,\n",
        "\t\t\t\"max_pred_len\": 60\n",
        "\t\t\t}\n",
        "\n",
        "\t#Encode the label into interger categories, setting the exclusive category 'cf[\"max_sn_len\"]-1' as the end sign\n",
        "\tle = LabelEncoder()\n",
        "\tle.fit(np.append(np.arange(-cf[\"max_sn_len\"]+3, cf[\"max_sn_len\"]-1), cf[\"max_sn_len\"]-1))\n",
        "\t#le.classes_\n",
        "\n",
        "\t#load corpus\n",
        "\tword_info_df, pos_info_df, eyemovement_df = load_corpus(cf[\"dataset\"])\n",
        "\t#Make list with sentence index\n",
        "\tsn_list = np.unique(eyemovement_df.sn.values).tolist()\n",
        "\t#Make list with reader index\n",
        "\treader_list = np.unique(eyemovement_df.id.values).tolist()\n",
        "\n",
        "\t#Split training&test sets by text or reader, depending on configuration\n",
        "\tif test_mode == 'text':\n",
        "\t\tprint('Start evaluating on new sentences.')\n",
        "\t\tsplit_list = sn_list\n",
        "\telif test_mode == 'subject':\n",
        "\t\tprint('Start evaluating on new readers.')\n",
        "\t\tsplit_list = reader_list\n",
        "\n",
        "\tn_folds = cf[\"n_folds\"]\n",
        "\tkf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
        "\tfold_indx = 0\n",
        "\t#for scanpath generation\n",
        "\tsp_dnn_list = []\n",
        "\tsp_human_list = []\n",
        "\tfor train_idx, test_idx in kf.split(split_list):\n",
        "\n",
        "\t\tloss_dict = {'val_loss':[], 'train_loss':[], 'test_ll':[], 'test_ll_SE':[], 'test_mse_dur':[], 'test_mse_dur_SE':[], 'test_mse_land_pos':[], 'test_mse_land_pos_SE':[], 'central_scasim_dnn':[], 'central_scasim_dnn_SE':[], 'central_scasim_human':[], 'central_scasim_human_SE':[], 'scasim_dnn':[], 'scasim_dnn_SE':[], 'scasim_human':[], 'scasim_human_SE':[], 'uniform_scasim':[], 'uniform_scasim_SE':[], 'uniform_central_scasim':[], 'uniform_central_scasim_SE':[], 'uniform_nll_SE':[], 'uniform_nll':[], 'uniform_mse_dur_SE':[], 'uniform_mse_dur':[], 'uniform_mse_land_pos_SE':[], 'uniform_mse_land_pos':[],\n",
        "\t\t             'ez_reader_scasim':[], 'ez_reader_scasim_SE':[], 'ez_reader_central_scasim':[], 'ez_reader_central_scasim_SE':[], 'ez_reader_mse_dur_SE':[], 'ez_reader_mse_dur':[], 'ez_reader_mse_land_pos_SE':[], 'ez_reader_mse_land_pos':[]}\n",
        "\t\tlist_train = [split_list[i] for i in train_idx]\n",
        "\t\tlist_test = [split_list[i] for i in test_idx]\n",
        "\n",
        "\t\t# create train validation split for training the models:\n",
        "\t\tkf_val = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
        "\t\tfor train_index, val_index in kf_val.split(list_train):\n",
        "\t\t\t# we only evaluate a single fold\n",
        "\t\t\tbreak\n",
        "\t\tlist_train_net = [list_train[i] for i in train_index]\n",
        "\t\tlist_val_net = [list_train[i] for i in val_index]\n",
        "\n",
        "\t\tif test_mode == 'text':\n",
        "\t\t\tsn_list_train = list_train_net\n",
        "\t\t\tsn_list_val = list_val_net\n",
        "\t\t\tsn_list_test = list_test\n",
        "\t\t\treader_list_train, reader_list_val, reader_list_test = reader_list, reader_list, reader_list\n",
        "\n",
        "\t\telif test_mode == 'subject':\n",
        "\t\t\treader_list_train = list_train_net\n",
        "\t\t\treader_list_val = list_val_net\n",
        "\t\t\treader_list_test = list_test\n",
        "\t\t\tsn_list_train, sn_list_val, sn_list_test = sn_list, sn_list, sn_list\n",
        "\n",
        "\t\t#initialize tokenizer\n",
        "\t\ttokenizer = BertTokenizer.from_pretrained(cf['model_pretrained'])\n",
        "\t\t#Preparing batch data\n",
        "\t\tdataset_train = BSCdataset(word_info_df, eyemovement_df, cf, reader_list_train, sn_list_train, tokenizer)\n",
        "\t\ttrain_dataloaderr = DataLoader(dataset_train, batch_size = cf[\"batch_size\"], shuffle = True, drop_last=True)\n",
        "\n",
        "\t\tdataset_val = BSCdataset(word_info_df, eyemovement_df, cf, reader_list_val, sn_list_val, tokenizer)\n",
        "\t\tval_dataloaderr = DataLoader(dataset_val, batch_size = cf[\"batch_size\"], shuffle = False, drop_last=True)\n",
        "\n",
        "\t\tdataset_test = BSCdataset(word_info_df, eyemovement_df, cf, reader_list_test, sn_list_test, tokenizer)\n",
        "\t\ttest_dataloaderr = DataLoader(dataset_test, batch_size = cf[\"batch_size\"], shuffle = False, drop_last=False)\n",
        "\n",
        "\t\t#z-score normalization for gaze features\n",
        "\t\tfix_dur_mean, fix_dur_std = calculate_mean_std(dataloader=train_dataloaderr, feat_key=\"sp_fix_dur\", padding_value=0, scale=1000)\n",
        "\t\tlanding_pos_mean, landing_pos_std = calculate_mean_std(dataloader=train_dataloaderr, feat_key=\"sp_landing_pos\", padding_value=0)\n",
        "\t\tsn_word_len_mean, sn_word_len_std = calculate_mean_std(dataloader=train_dataloaderr, feat_key=\"sn_word_len\")\n",
        "\n",
        "\t\t# load model\n",
        "\t\tdnn = Eyettention(cf)\n",
        "\n",
        "\t\t#training\n",
        "\t\tepisode = 0\n",
        "\t\toptimizer = Adam(dnn.parameters(), lr=cf[\"lr\"])\n",
        "\t\tdnn.train()\n",
        "\t\tdnn.to(device)\n",
        "\t\tav_score = deque(maxlen=100)\n",
        "\t\tav_location_score = deque(maxlen=100)\n",
        "\t\tav_duration_score = deque(maxlen=100)\n",
        "\t\tav_land_pos_score = deque(maxlen=100)\n",
        "\t\told_score = 1e10\n",
        "\t\tsave_ep_couter = 0\n",
        "\t\tprint('Start training')\n",
        "\t\tprint(\"fold_indx\", fold_indx)\n",
        "\t\tfor episode_i in range(episode, cf[\"n_epochs\"]+1):\n",
        "\t\t\tdnn.train()\n",
        "\t\t\tprint('episode:', episode_i)\n",
        "\t\t\tcounter = 0\n",
        "\t\t\tfor batchh in train_dataloaderr:\n",
        "\t\t\t\tcounter += 1\n",
        "\t\t\t\tbatchh.keys()\n",
        "\t\t\t\tsn_ids = batchh[\"sn_ids\"].to(device)\n",
        "\t\t\t\tsn_input_ids = batchh[\"sn_input_ids\"].to(device)\n",
        "\t\t\t\tsn_attention_mask = batchh[\"sn_attention_mask\"].to(device)\n",
        "\t\t\t\tsp_input_ids = batchh[\"sp_input_ids\"].to(device)\n",
        "\t\t\t\tsp_attention_mask = batchh[\"sp_attention_mask\"].to(device)\n",
        "\t\t\t\tsp_pos = batchh[\"sp_pos\"].to(device)\n",
        "\t\t\t\tsp_landing_pos = batchh[\"sp_landing_pos\"].to(device) # [256, 40]\n",
        "\t\t\t\tsp_fix_dur = (batchh[\"sp_fix_dur\"]/1000).to(device) # [256, 40]\n",
        "\t\t\t\tsn_word_len = batchh[\"sn_word_len\"].to(device)\n",
        "\n",
        "\t\t\t\t# normalize gaze features (z-score normalisation)\n",
        "\t\t\t\tmask = ~torch.eq(sp_fix_dur, 0)\n",
        "\t\t\t\tsp_fix_dur = (sp_fix_dur-fix_dur_mean)/fix_dur_std * mask\n",
        "\t\t\t\tsp_fix_dur = torch.nan_to_num(sp_fix_dur) # [256, 40]\n",
        "\t\t\t\tsp_landing_pos = (sp_landing_pos - landing_pos_mean)/landing_pos_std * mask\n",
        "\t\t\t\tsp_landing_pos = torch.nan_to_num(sp_landing_pos)\n",
        "\t\t\t\tsn_word_len = (sn_word_len - sn_word_len_mean)/sn_word_len_std\n",
        "\t\t\t\tsn_word_len = torch.nan_to_num(sn_word_len)\n",
        "\n",
        "\t\t\t\t# zero old gradients\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\t# predict output with DNN\n",
        "\t\t\t\tlocation_preds, duration_preds, landing_pos_preds, atten_weights = dnn(sn_emd=sn_input_ids,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_mask=sn_attention_mask,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_emd=sp_input_ids,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_pos=sp_pos,\n",
        "\t\t\t\t\t\t\t\t\t\t\tword_ids_sn=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\tword_ids_sp=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_fix_dur=sp_fix_dur,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_landing_pos=sp_landing_pos,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_word_len = sn_word_len,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_pred=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_word_freq=None\n",
        "\t\t\t\t\t\t\t\t\t\t\t                                            )#[batch, step, dec_o_dim]\n",
        "\n",
        "\t\t\t\tlocation_preds = location_preds.permute(0,2,1)              #[batch, dec_o_dim, step]\n",
        "\n",
        "\t\t\t\t#prepare label and mask\n",
        "\t\t\t\t# Compute loss for fixation locations\n",
        "\t\t\t\tpad_mask, label = load_label(sp_pos, cf, le, device)\n",
        "\t\t\t\tloss = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\t\t\t\tbatch_location_error = torch.mean(torch.masked_select(loss(location_preds, label), ~pad_mask))\n",
        "\n",
        "\t\t\t\t# Compute loss for fixation durations\n",
        "\t\t\t\tduration_labels = sp_fix_dur[:, :39] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\tduration_preds = duration_preds.squeeze(-1)  # Remove extra dimension (from [256, 39, 1] to [256, 39])\n",
        "\t\t\t\tdur_loss = nn.MSELoss(reduction=\"none\")\n",
        "\t\t\t\tbatch_duration_error = torch.mean(dur_loss(duration_preds, duration_labels))\n",
        "\n",
        "\t\t\t\t# Compute loss for landing position\n",
        "\t\t\t\tlanding_pos_labels = sp_landing_pos[:, :39] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\tlanding_pos_preds = landing_pos_preds.squeeze(-1)  # Remove extra dimension (from [256, 39, 1] to [256, 39])\n",
        "\t\t\t\tland_pos_loss = nn.MSELoss(reduction=\"none\")\n",
        "\t\t\t\tbatch_land_pos_error = torch.mean(land_pos_loss(landing_pos_preds, landing_pos_labels))\n",
        "\n",
        "\t\t\t\t# Combined loss for both location and duration\n",
        "\t\t\t\tbatch_error = batch_location_error + batch_duration_error + batch_land_pos_error\n",
        "\n",
        "\t\t\t\t# backpropagate loss\n",
        "\t\t\t\tbatch_error.backward()\n",
        "\t\t\t\t# clip gradients\n",
        "\t\t\t\tgradient_clipping(dnn, cf[\"max_grad_norm\"])\n",
        "\n",
        "\t\t\t\t#learn\n",
        "\t\t\t\toptimizer.step()\n",
        "\t\t\t\tav_location_score.append(batch_location_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tav_duration_score.append(batch_duration_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tav_land_pos_score.append(batch_land_pos_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tav_score.append(batch_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tprint('counter:',counter)\n",
        "\t\t\t\tprint('\\rSample {}\\tLocation Loss: {:.10f}\\tDuration Loss: {:.10f}\\tLanding position Loss: {:.10f}'.format(\n",
        "          counter, np.mean(av_location_score), np.mean(av_duration_score), np.mean(av_land_pos_score)), end=\" \")\n",
        "\t\t\tloss_dict['train_loss'].append(np.mean(av_score))\n",
        "\n",
        "\t\t\tlocation_val_loss = []\n",
        "\t\t\tduration_val_loss = []\n",
        "\t\t\tland_pos_val_loss = []\n",
        "\t\t\tval_loss = []\n",
        "\t\t\tdnn.eval()\n",
        "\t\t\tfor batchh in val_dataloaderr:\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tsn_ids_val = batchh[\"sn_ids\"].to(device)\n",
        "\t\t\t\t\tsn_input_ids_val = batchh[\"sn_input_ids\"].to(device)\n",
        "\t\t\t\t\tsn_attention_mask_val = batchh[\"sn_attention_mask\"].to(device)\n",
        "\t\t\t\t\tsp_input_ids_val = batchh[\"sp_input_ids\"].to(device)\n",
        "\t\t\t\t\tsp_attention_mask_val = batchh[\"sp_attention_mask\"].to(device)\n",
        "\t\t\t\t\tsp_pos_val = batchh[\"sp_pos\"].to(device)\n",
        "\t\t\t\t\tsp_landing_pos_val = batchh[\"sp_landing_pos\"].to(device)\n",
        "\t\t\t\t\tsp_fix_dur_val = (batchh[\"sp_fix_dur\"]/1000).to(device)\n",
        "\t\t\t\t\tsn_word_len_val = batchh[\"sn_word_len\"].to(device)\n",
        "\n",
        "\t\t\t\t\t#normalize gaze features\n",
        "\t\t\t\t\tmask = ~torch.eq(sp_fix_dur_val, 0)\n",
        "\t\t\t\t\tsp_fix_dur_val = (sp_fix_dur_val-fix_dur_mean)/fix_dur_std * mask\n",
        "\t\t\t\t\tsp_landing_pos_val = (sp_landing_pos_val - landing_pos_mean)/landing_pos_std * mask\n",
        "\t\t\t\t\tsp_fix_dur_val = torch.nan_to_num(sp_fix_dur_val)\n",
        "\t\t\t\t\tsp_landing_pos_val = torch.nan_to_num(sp_landing_pos_val)\n",
        "\t\t\t\t\tsn_word_len_val = (sn_word_len_val - sn_word_len_mean)/sn_word_len_std\n",
        "\t\t\t\t\tsn_word_len_val = torch.nan_to_num(sn_word_len_val)\n",
        "\n",
        "\t\t\t\t\tlocation_preds_val, duration_preds_val, landing_pos_preds_val, atten_weights_val = dnn(sn_emd=sn_input_ids_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_mask=sn_attention_mask_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_emd=sp_input_ids_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_pos=sp_pos_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sn=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sp=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_fix_dur=sp_fix_dur_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_landing_pos=sp_landing_pos_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_len = sn_word_len_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_pred = None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_freq = None)#[batch, step, dec_o_dim]\n",
        "\t\t\t\t\tlocation_preds_val = location_preds_val.permute(0,2,1)              #[batch, dec_o_dim, step\n",
        "\n",
        "\t\t\t\t\t# Compute location prediction error\n",
        "\t\t\t\t\tloss = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\t\t\t\t\tpad_mask_val, label_val = load_label(sp_pos_val, cf, le, device)\n",
        "\t\t\t\t\tlocation_error_val = torch.mean(torch.masked_select(loss(location_preds_val, label_val), ~pad_mask_val))\n",
        "\t\t\t\t\tlocation_val_loss.append(location_error_val.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\t\t\t# Compute duration prediction error\n",
        "\t\t\t\t\tduration_labels_val = sp_fix_dur_val[:, :39] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\t\tduration_preds_val = duration_preds_val.squeeze(-1)\n",
        "\t\t\t\t\tduration_error_val = torch.mean(dur_loss(duration_preds_val, duration_labels_val))\n",
        "\t\t\t\t\tduration_val_loss.append(duration_error_val.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\t\t\t# Compute loss for landing position\n",
        "\t\t\t\t\tlanding_pos_labels_val = sp_landing_pos_val[:, :39] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\t\tlanding_pos_preds_val = landing_pos_preds_val.squeeze(-1)  # Remove extra dimension (from [256, 39, 1] to [256, 39])\n",
        "\t\t\t\t\tland_pos_error_val = torch.mean(land_pos_loss(landing_pos_preds_val, landing_pos_labels_val))\n",
        "\t\t\t\t\tland_pos_val_loss.append(land_pos_error_val.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\t\t\tcombined_loss = location_error_val + duration_error_val + land_pos_error_val\n",
        "\t\t\t\t\tval_loss.append(combined_loss.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\tprint('\\nValidation loss for locations {} \\n'.format(np.mean(location_val_loss)))\n",
        "\t\t\tprint('\\nValidation loss for duration {} \\n'.format(np.mean(duration_val_loss)))\n",
        "\t\t\tprint('\\nValidation loss for landing position {} \\n'.format(np.mean(land_pos_val_loss)))\n",
        "\t\t\tloss_dict['val_loss'].append(np.mean(val_loss))\n",
        "\n",
        "\t\t\tif np.mean(val_loss) < old_score:\n",
        "\t\t\t\t# save model if val loss is smallest\n",
        "\t\t\t\ttorch.save(dnn.state_dict(), '{}/BSC_3head_arch_new_sentence_{}.pth'.format(save_data_folder, fold_indx))\n",
        "\t\t\t\told_score = np.mean(val_loss)\n",
        "\t\t\t\tprint('\\nsaved model state dict\\n')\n",
        "\t\t\t\tsave_ep_couter = episode_i\n",
        "\t\t\telse:\n",
        "\t\t\t\t#early stopping\n",
        "\t\t\t\tif episode_i - save_ep_couter >= cf[\"earlystop_patience\"]:\n",
        "\t\t\t\t\tbreak\n",
        "\t\tfold_indx += 1\n",
        "\n",
        "\t\t#evaluation\n",
        "\t\tdnn.eval()\n",
        "\t\tres_llh=[]\n",
        "\t\tres_mse_dur = []\n",
        "\t\tres_mse_land_pos = []\n",
        "\t\tres_central_scasim_human = []\n",
        "\t\tres_central_scasim_dnn = []\n",
        "\t\tres_scasim_human = []\n",
        "\t\tres_scasim_dnn = []\n",
        "\t\tuniform_central_scasim_scores = []\n",
        "\t\tuniform_scasim_scores = []\n",
        "\t\tuniform_nll_scores = []\n",
        "\t\tuniform_mse_dur_scores = []\n",
        "\t\tuniform_mse_land_pos_scores = []\n",
        "\t\tez_reader_central_scasim_scores = []\n",
        "\t\tez_reader_scasim_scores = []\n",
        "\t\tez_reader_nll_scores = []\n",
        "\t\tez_reader_mse_dur_scores = []\n",
        "\t\tez_reader_mse_land_pos_scores = []\n",
        "\t\tdnn.load_state_dict(torch.load(os.path.join(save_data_folder, f'BSC_3head_arch_new_sentence_{fold_indx}.pth'), map_location='cpu'))\n",
        "\t\tdnn.to(device)\n",
        "\t\tbatch_indx = 0\n",
        "\t\tprint(\"Evaluating for fold\", fold_indx)\n",
        "\t\tfor batchh in test_dataloaderr:\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tsn_ids_test = batchh[\"sn_ids\"].to(device)\n",
        "\t\t\t\tsn_input_ids_test = batchh[\"sn_input_ids\"].to(device)\n",
        "\t\t\t\tsn_attention_mask_test = batchh[\"sn_attention_mask\"].to(device)\n",
        "\t\t\t\tsp_input_ids_test = batchh[\"sp_input_ids\"].to(device)\n",
        "\t\t\t\tsp_attention_mask_test = batchh[\"sp_attention_mask\"].to(device)\n",
        "\t\t\t\tsp_pos_test = batchh[\"sp_pos\"].to(device) # 28: '<Sep>', 29: '<'Pad'>'\n",
        "\t\t\t\tsp_landing_pos_test = batchh[\"sp_landing_pos\"].to(device)\n",
        "\t\t\t\tsp_fix_dur_test = (batchh[\"sp_fix_dur\"]/1000).to(device)\n",
        "\t\t\t\tsn_word_len_test = batchh[\"sn_word_len\"].to(device)\n",
        "\n",
        "\t\t\t\t#normalize gaze features\n",
        "\t\t\t\tmask = ~torch.eq(sp_fix_dur_test, 0)\n",
        "\t\t\t\tsp_fix_dur_test = (sp_fix_dur_test-fix_dur_mean)/fix_dur_std * mask\n",
        "\t\t\t\tsp_landing_pos_test = (sp_landing_pos_test - landing_pos_mean)/landing_pos_std * mask\n",
        "\t\t\t\tsp_fix_dur_test = torch.nan_to_num(sp_fix_dur_test)\n",
        "\t\t\t\tsp_landing_pos_test = torch.nan_to_num(sp_landing_pos_test)\n",
        "\t\t\t\tsn_word_len_test = (sn_word_len_test - sn_word_len_mean)/sn_word_len_std\n",
        "\t\t\t\tsn_word_len_test = torch.nan_to_num(sn_word_len_test)\n",
        "\n",
        "\t\t\t\tlocation_preds_test, duration_preds_test, landing_pos_preds_test, atten_weights_test = dnn(sn_emd=sn_input_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_mask=sn_attention_mask_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_emd=sp_input_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_pos=sp_pos_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sn=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sp=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_fix_dur=sp_fix_dur_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_landing_pos=sp_landing_pos_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_len = sn_word_len_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_pred = None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_freq = None\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t) #[batch, step, dec_o_dim]\n",
        "\n",
        "\n",
        "\t\t\t\t########## Evaluate location predictions ##########\n",
        "\t\t\t\tm = nn.Softmax(dim=2)\n",
        "\t\t\t\tlocation_preds_test = m(location_preds_test).detach().to('cpu').numpy()\n",
        "\n",
        "\t\t\t\t#prepare label and mask\n",
        "\t\t\t\tpad_mask_test, label_test = load_label(sp_pos_test, cf, le, 'cpu')\n",
        "\t\t\t\t#compute log likelihood for the batch samples\n",
        "\t\t\t\tres_batch = eval_log_llh(location_preds_test, label_test, pad_mask_test)\n",
        "\t\t\t\tres_llh.append(np.array(res_batch))\n",
        "\n",
        "\t\t\t\tuniform_output = construct_uniform_tensor(location_preds_test)\n",
        "\t\t\t\tuniform_nll = eval_log_llh(uniform_output, label_test, pad_mask_test)\n",
        "\t\t\t\tuniform_nll_scores.append(np.array(uniform_nll))\n",
        "\t\t\t\tprint(\"uniform_nll\", np.mean(uniform_nll), uniform_nll)\n",
        "\n",
        "\t\t\t\tprint(\"######### Eyettention 2.0 model evaluation ##########\")\n",
        "\t\t\t\tduration_preds_test = duration_preds_test.squeeze(-1)\n",
        "\t\t\t\tduration_labels_test = sp_fix_dur_test[:, :39]\n",
        "\t\t\t\ttest_mask = mask[:, :39]\n",
        "\t\t\t\tmse_dur = eval_mse(duration_preds_test, duration_labels_test, test_mask)\n",
        "\t\t\t\tprint(\"MSE for durations\", np.mean(mse_dur))\n",
        "\t\t\t\tres_mse_dur.append(np.array(mse_dur))\n",
        "\n",
        "\t\t\t\tlanding_pos_preds_test = landing_pos_preds_test.squeeze(-1)\n",
        "\t\t\t\tlanding_pos_labels_test = sp_landing_pos_test[:, :39]\n",
        "\t\t\t\tmse_landing_pos = eval_mse(landing_pos_preds_test, landing_pos_labels_test, test_mask)\n",
        "\t\t\t\tprint(\"MSE for landing positions\", np.mean(mse_landing_pos))\n",
        "\t\t\t\tres_mse_land_pos.append(np.array(mse_landing_pos))\n",
        "\n",
        "\t\t\t\tif bool(scanpath_gen_flag) == True:\n",
        "\t\t\t\t\tsn_len = (torch.sum(sn_attention_mask_test, axis=1) - 2).detach().to('cpu').numpy()\n",
        "\t\t\t\t\t# compute the scan path generated from the model when the first CLS token is given\n",
        "\t\t\t\t\tsp_dnn, _, dur_dnn, land_pos_dnn = dnn.scanpath_generation(sn_emd=sn_input_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_mask=sn_attention_mask_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t word_ids_sn=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_word_len = sn_word_len_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t le=le,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_word_freq = None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_pred = None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sp_fix_dur=sp_fix_dur_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sp_landing_pos = sp_landing_pos_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t max_pred_len=cf['max_pred_len'])\n",
        "\n",
        "\t\t\t\t\tsp_dnn, sp_human = prepare_scanpath(sp_dnn.detach().to('cpu').numpy(),\n",
        "                                              dur_dnn.detach().to('cpu').numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tland_pos_dnn.detach().to('cpu').numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_len, sp_pos_test,\n",
        "                                              sp_fix_dur_test, sp_landing_pos_test, cf, sn_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfix_dur_mean, fix_dur_std, landing_pos_mean, landing_pos_std)\n",
        "\n",
        "\t\t\t\t\tsp_dnn_list.extend(sp_dnn)\n",
        "\t\t\t\t\tsp_human_list.extend(sp_human)\n",
        "\n",
        "\t\t\t\t\tsp_dnn = convert_sp_to_lists(sp_dnn)\n",
        "\t\t\t\t\tsp_human = convert_sp_to_lists(sp_human)\n",
        "\t\t\t\t\tsp_human = modify_landing_pos(sp_human.copy())\n",
        "\t\t\t\t\tsp_dnn = modify_landing_pos(sp_dnn.copy())\n",
        "\t\t\t\t\trandom_sp = sample_random_sp(\"BSC\", sp_human)\n",
        "\t\t\t\t\trandom_sp = convert_sp_to_lists(random_sp)\n",
        "\t\t\t\t\trandom_sp = modify_landing_pos(random_sp.copy())\n",
        "\n",
        "\t\t\t\t\tscasim_scores_dnn = compute_scasim(sp_dnn, sp_human)\n",
        "\t\t\t\t\tres_scasim_dnn.append(scasim_scores_dnn)\n",
        "\t\t\t\t\tprint(\"Mean scasim dnn\", np.mean(scasim_scores_dnn))\n",
        "\t\t\t\t\tscasim_scores_human = compute_scasim(sp_human, random_sp)\n",
        "\t\t\t\t\tres_scasim_human.append(scasim_scores_human)\n",
        "\t\t\t\t\tprint(\"Mean scasim human\", np.mean(scasim_scores_human))\n",
        "\n",
        "\t\t\t\t\tcentral_scasim_scores_dnn = compute_central_scasim(\"BSC_most_central_sp.txt\", sp_dnn)\n",
        "\t\t\t\t\tcentral_scasim_scores_human = compute_central_scasim(\"BSC_most_central_sp.txt\", sp_human)\n",
        "\t\t\t\t\tres_central_scasim_dnn.append(np.array(central_scasim_scores_dnn))\n",
        "\t\t\t\t\tres_central_scasim_human.append(np.array(central_scasim_scores_human))\n",
        "\t\t\t\t\tprint(\"Mean central scasim dnn\", np.mean(central_scasim_scores_dnn))\n",
        "\t\t\t\t\tprint(\"Mean central scasim human\", np.mean(central_scasim_scores_human))\n",
        "\n",
        "\t\t\t\t\tprint(\"######### Uniform baseline model evaluation ##########\")\n",
        "\t\t\t\t\tmean_dur_uniform, std_dur_uniform, mean_land_pos_uniform, std_land_pos_uniform = compute_mean_std_uniform(\"baseline/uniform/BSC_uniform_results.csv\")\n",
        "\t\t\t\t\tuniform_central_scasim, uniform_scasim, dur_mse_scores, land_pos_mse_scores = evaluate_uniform_model(\"BSC\", sp_human, landing_pos_mean, landing_pos_std, fix_dur_mean, fix_dur_std, mean_land_pos_uniform, std_land_pos_uniform, mean_dur_uniform, std_dur_uniform)\n",
        "\t\t\t\t\tuniform_central_scasim_scores.append(np.array(uniform_central_scasim))\n",
        "\t\t\t\t\tuniform_scasim_scores.append(np.array(uniform_scasim))\n",
        "\t\t\t\t\tuniform_mse_dur_scores.append(np.array(dur_mse_scores))\n",
        "\t\t\t\t\tuniform_mse_land_pos_scores.append(np.array(land_pos_mse_scores))\n",
        "\t\t\t\t\tprint(\"Uniform mean central Scasim score:\", np.mean(uniform_central_scasim))\n",
        "\t\t\t\t\tprint(\"Uniform mean Scasim score:\", np.mean(uniform_scasim))\n",
        "\t\t\t\t\tprint(\"MSE for uniform durations\", np.mean(dur_mse_scores))\n",
        "\t\t\t\t\tprint(\"MSE for uniform landing pos\", np.mean(land_pos_mse_scores))\n",
        "\n",
        "\t\t\t\t\tprint(\"######### E-Z Reader model evaluation ##########\")\n",
        "\t\t\t\t\tmean_dur_ez_reader, std_dur_ez_reader, mean_land_pos_ez_reader, std_land_pos_ez_reader = compute_mean_std_ez_reader(\"baseline/E-Z_Reader/BSCSimulationResults.txt\")\n",
        "\t\t\t\t\tcentral_scasim_ez_reader, scasim_ez_reader, dur_mse_ez_reader, land_pos_mse_ez_reader = evaluate_ez_reader(\"BSC\", \"baseline/E-Z_Reader/BSCSimulationResults.txt\", sp_human, landing_pos_mean, landing_pos_std, fix_dur_mean, fix_dur_std, mean_land_pos_ez_reader, std_land_pos_ez_reader, mean_dur_ez_reader, std_dur_ez_reader)\n",
        "\t\t\t\t\tez_reader_central_scasim_scores.append(np.array(central_scasim_ez_reader))\n",
        "\t\t\t\t\tprint(\"Central scasim E-Z Reader\", np.mean(central_scasim_ez_reader))\n",
        "\t\t\t\t\tez_reader_scasim_scores.append(np.array(scasim_ez_reader))\n",
        "\t\t\t\t\tprint(\"Mean scasim score E-Z Reader\", np.mean(scasim_ez_reader))\n",
        "\t\t\t\t\tez_reader_mse_dur_scores.append(np.array(dur_mse_ez_reader))\n",
        "\t\t\t\t\tprint(\"MSE for durations E-Z Reader\", np.mean(dur_mse_ez_reader))\n",
        "\t\t\t\t\tez_reader_mse_land_pos_scores.append(np.array(land_pos_mse_ez_reader))\n",
        "\t\t\t\t\tprint(\"MSE for landing pos E-Z Reader\", np.mean(land_pos_mse_ez_reader))\n",
        "\n",
        "\t\t\t\tbatch_indx +=1\n",
        "\n",
        "\t\tres_llh = np.concatenate(res_llh).ravel()\n",
        "\t\tloss_dict['test_ll'].append(res_llh)\n",
        "\t\tres_mse_dur = np.concatenate(res_mse_dur).ravel()\n",
        "\t\tloss_dict['test_mse_dur'].append(res_mse_dur)\n",
        "\t\tres_mse_land_pos = np.concatenate(res_mse_land_pos).ravel()\n",
        "\t\tloss_dict['test_mse_land_pos'].append(res_mse_land_pos)\n",
        "\n",
        "\t\tres_central_scasim_dnn = np.concatenate(res_central_scasim_dnn).ravel()\n",
        "\t\tloss_dict['central_scasim_dnn'].append(res_central_scasim_dnn)\n",
        "\t\tres_central_scasim_human = np.concatenate(res_central_scasim_human).ravel()\n",
        "\t\tloss_dict['central_scasim_human'].append(res_central_scasim_human)\n",
        "\t\tres_scasim_dnn = np.concatenate(res_scasim_dnn).ravel()\n",
        "\t\tloss_dict['scasim_dnn'].append(res_scasim_dnn)\n",
        "\t\tres_scasim_human = np.concatenate(res_scasim_human).ravel()\n",
        "\t\tloss_dict['scasim_human'].append(res_scasim_human)\n",
        "\n",
        "\t\tloss_dict['fix_dur_mean'] = fix_dur_mean\n",
        "\t\tloss_dict['fix_dur_std'] = fix_dur_std\n",
        "\t\tloss_dict['landing_pos_mean'] = landing_pos_mean\n",
        "\t\tloss_dict['landing_pos_std'] = landing_pos_std\n",
        "\t\tloss_dict['sn_word_len_mean'] = sn_word_len_mean\n",
        "\t\tloss_dict['sn_word_len_std'] = sn_word_len_std\n",
        "\n",
        "\t\tuniform_central_scasim_scores = np.concatenate(uniform_central_scasim_scores).ravel()\n",
        "\t\tloss_dict['uniform_central_scasim'].append(uniform_central_scasim_scores)\n",
        "\t\tuniform_scasim_scores = np.concatenate(uniform_scasim_scores).ravel()\n",
        "\t\tloss_dict['uniform_scasim'].append(uniform_scasim_scores)\n",
        "\n",
        "\t\tuniform_mse_dur_scores = np.concatenate(uniform_mse_dur_scores).ravel()\n",
        "\t\tloss_dict['uniform_mse_dur'].append(uniform_mse_dur_scores)\n",
        "\t\tuniform_mse_land_pos_scores = np.concatenate(uniform_mse_land_pos_scores).ravel()\n",
        "\t\tloss_dict['uniform_mse_land_pos'].append(uniform_mse_land_pos_scores)\n",
        "\t\tuniform_nll_scores = np.concatenate(uniform_nll_scores).ravel()\n",
        "\t\tloss_dict['uniform_nll'].append(uniform_nll_scores)\n",
        "\n",
        "\t\tez_reader_central_scasim_scores = np.concatenate(ez_reader_central_scasim_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_central_scasim'].append(ez_reader_central_scasim_scores)\n",
        "\t\tez_reader_scasim_scores = np.concatenate(ez_reader_scasim_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_scasim'].append(ez_reader_scasim_scores)\n",
        "\n",
        "\t\tez_reader_mse_dur_scores = np.concatenate(ez_reader_mse_dur_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_mse_dur'].append(ez_reader_mse_dur_scores)\n",
        "\t\tez_reader_mse_land_pos_scores = np.concatenate(ez_reader_mse_land_pos_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_mse_land_pos'].append(ez_reader_mse_land_pos_scores)\n",
        "\n",
        "\t\tprint('Test likelihood is {}'.format(np.mean(res_llh)))\n",
        "\t\tloss_dict['test_ll_SE'].append(np.std(res_llh)/ np.sqrt(len(res_llh)))\n",
        "\t\tprint(\"Standard error for NLL\", np.std(res_llh)/ np.sqrt(len(res_llh)))\n",
        "\n",
        "\t\tprint('Test MSE for durations is {}'.format(np.mean(res_mse_dur)))\n",
        "\t\tloss_dict['test_mse_dur_SE'].append(np.std(res_mse_dur)/ np.sqrt(len(res_mse_dur)))\n",
        "\t\tprint(\"Standard error for MSE dur\", np.std(res_mse_dur) / np.sqrt(len(res_mse_dur)))\n",
        "\n",
        "\t\tprint('Test MSE for landing positions is {}'.format(np.mean(res_mse_land_pos)))\n",
        "\t\tloss_dict['test_mse_land_pos_SE'].append(np.std(res_mse_land_pos)/ np.sqrt(len(res_mse_land_pos)))\n",
        "\t\tprint(\"Standard error for MSE land pos\", np.std(res_mse_land_pos) / np.sqrt(len(res_mse_land_pos)))\n",
        "\n",
        "\t\tprint(\"Central Scasim dnn\", np.mean(loss_dict['central_scasim_dnn']))\n",
        "\t\tloss_dict['central_scasim_dnn_SE'].append(np.std(res_central_scasim_dnn)/ np.sqrt(len(res_central_scasim_dnn)))\n",
        "\t\tprint(\"Standard error for Central scasim DNN\", np.std(res_central_scasim_dnn) / np.sqrt(len(res_central_scasim_dnn)))\n",
        "\n",
        "\t\tprint(\"Central Scasim human\", np.mean(loss_dict['central_scasim_human']))\n",
        "\t\tloss_dict['central_scasim_human_SE'].append(np.std(res_central_scasim_human)/ np.sqrt(len(res_central_scasim_human)))\n",
        "\t\tprint(\"Standard error for Central scasim human\", np.std(res_central_scasim_human) / np.sqrt(len(res_central_scasim_human)))\n",
        "\n",
        "\t\tprint(\"Scasim dnn\", np.mean(loss_dict['scasim_dnn']))\n",
        "\t\tloss_dict['scasim_dnn_SE'].append(np.std(res_scasim_dnn)/ np.sqrt(len(res_scasim_dnn)))\n",
        "\t\tprint(\"Standard error for scasim dnn\", np.std(res_scasim_dnn) / np.sqrt(len(res_scasim_dnn)))\n",
        "\n",
        "\t\tprint(\"Scasim human\", np.mean(loss_dict['scasim_human']))\n",
        "\t\tloss_dict['scasim_human_SE'].append(np.std(res_scasim_human)/ np.sqrt(len(res_scasim_human)))\n",
        "\t\tprint(\"Standard error for scasim human\", np.std(res_scasim_human) / np.sqrt(len(res_scasim_human)))\n",
        "\n",
        "\t\tprint(\"Uniform central scasim\", np.mean(loss_dict['uniform_central_scasim']))\n",
        "\t\tloss_dict['uniform_central_scasim_SE'].append(np.std(uniform_central_scasim_scores)/ np.sqrt(len(uniform_central_scasim_scores)))\n",
        "\t\tprint(\"Standard error for uniform central scasim\", np.std(uniform_central_scasim_scores) / np.sqrt(len(uniform_central_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform scasim\", np.mean(loss_dict['uniform_scasim']))\n",
        "\t\tloss_dict['uniform_scasim_SE'].append(np.std(uniform_scasim_scores)/ np.sqrt(len(uniform_scasim_scores)))\n",
        "\t\tprint(\"Standard error for uniform scasim\", np.std(uniform_scasim_scores) / np.sqrt(len(uniform_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform MSE durations\", np.mean(loss_dict['uniform_mse_dur']))\n",
        "\t\tloss_dict['uniform_mse_dur_SE'].append(np.std(dur_mse_scores)/ np.sqrt(len(dur_mse_scores)))\n",
        "\t\tprint(\"Standard error for uniform MSE durations\", np.std(dur_mse_scores) / np.sqrt(len(dur_mse_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform MSE landing pos\", np.mean(loss_dict['uniform_mse_land_pos']))\n",
        "\t\tloss_dict['uniform_mse_land_pos_SE'].append(np.std(land_pos_mse_scores)/ np.sqrt(len(land_pos_mse_scores)))\n",
        "\t\tprint(\"Standard error for uniform MSE landing pos\", np.std(land_pos_mse_scores) / np.sqrt(len(land_pos_mse_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform NLL\", np.mean(loss_dict['uniform_nll']))\n",
        "\t\tloss_dict['uniform_nll_SE'].append(np.std(uniform_nll_scores)/ np.sqrt(len(uniform_nll_scores)))\n",
        "\t\tprint(\"Standard error for uniform NLL\", np.std(uniform_nll_scores) / np.sqrt(len(uniform_nll_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader central scasim\", np.mean(loss_dict['ez_reader_central_scasim']))\n",
        "\t\tloss_dict['ez_reader_central_scasim_SE'].append(np.std(ez_reader_central_scasim_scores)/ np.sqrt(len(ez_reader_central_scasim_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader central scasim\", np.std(ez_reader_central_scasim_scores) / np.sqrt(len(ez_reader_central_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader scasim\", np.mean(loss_dict['ez_reader_scasim']))\n",
        "\t\tloss_dict['ez_reader_scasim_SE'].append(np.std(ez_reader_scasim_scores)/ np.sqrt(len(ez_reader_scasim_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader scasim\", np.std(ez_reader_scasim_scores) / np.sqrt(len(ez_reader_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader MSE durations\", np.mean(loss_dict['ez_reader_mse_dur']))\n",
        "\t\tloss_dict['ez_reader_mse_dur_SE'].append(np.std(ez_reader_mse_dur_scores)/ np.sqrt(len(ez_reader_mse_dur_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader MSE durations\", np.std(ez_reader_mse_dur_scores) / np.sqrt(len(ez_reader_mse_dur_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader MSE landing pos\", np.mean(loss_dict['ez_reader_mse_land_pos']))\n",
        "\t\tloss_dict['ez_reader_mse_land_pos_SE'].append(np.std(ez_reader_mse_land_pos_scores)/ np.sqrt(len(ez_reader_mse_land_pos_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader MSE landing pos\", np.std(ez_reader_mse_land_pos_scores) / np.sqrt(len(ez_reader_mse_land_pos_scores)))\n",
        "\n",
        "\t\t#save results\n",
        "\t\twith open('{}/res_BSC_{}_eyettention_Fold{}.pickle'.format(save_data_folder, test_mode, fold_indx), 'wb') as handle:\n",
        "\t\t\tpickle.dump(loss_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\t\tfold_indx += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical tests**"
      ],
      "metadata": {
        "id": "93pvVPGoLVc5"
      },
      "id": "93pvVPGoLVc5"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/BSC/New_Reader/res_BSC_subject_eyettention_Fold{fold_index}.pickle', 'rb') as handle:\n",
        "      print(fold_index)\n",
        "      fold_results = pickle.load(handle)\n",
        "\n",
        "      # NLL\n",
        "      test_ll = fold_results['test_ll'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/New_Reader/res_BSC_original_eyettention_fold{fold_index}.pickle', 'rb') as original:\n",
        "        original_results = pickle.load(original)\n",
        "        original_ll = original_results['test_ll'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(test_ll, original_ll)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in NLL scores between Eyettention 2.0 and original Eyettention scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and original Eyettention NLL scores.\")\n",
        "\n",
        "      uniform_nll = fold_results['uniform_nll'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(test_ll, uniform_nll)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in NLL scores between predicted and uniformly generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between the predicted and uniform NLL scores.\")\n",
        "\n",
        "      # Scasim\n",
        "      scasim_dnn = fold_results['scasim_dnn'][0]\n",
        "      scasim_human = fold_results['scasim_human'][0]\n",
        "      ez_reader_scasim = fold_results['ez_reader_scasim'][0]\n",
        "      uniform_scasim = fold_results['uniform_scasim'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(scasim_dnn, ez_reader_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader generated Scasim scores.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(scasim_dnn, uniform_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform Scasim scores.\")\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(scasim_dnn, scasim_human)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the human scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and human Scasim scores.\")\n",
        "\n",
        "      # Central Scasim\n",
        "      central_scasim_dnn = fold_results['central_scasim_dnn'][0]\n",
        "      central_scasim_human = fold_results['central_scasim_human'][0]\n",
        "      ez_reader_central_scasim = fold_results['ez_reader_central_scasim'][0]\n",
        "      uniform_central_scasim = fold_results['uniform_central_scasim'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, ez_reader_central_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader Central Scasim scores.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, uniform_central_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform Central Scasim scores.\")\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, central_scasim_human)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the human scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and human Central Scasim scores.\")\n",
        "\n",
        "      # MSE durations\n",
        "      test_mse_dur = fold_results['test_mse_dur'][0]\n",
        "      ez_reader_mse_dur = fold_results['ez_reader_mse_dur'][0]\n",
        "      uniform_mse_dur = fold_results['uniform_mse_dur'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_dur, ez_reader_mse_dur)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader MSE scores for durations.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_dur, uniform_mse_dur)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform MSE scores for durations.\")\n",
        "\n",
        "      # MSE landing pos\n",
        "      test_mse_land_pos = fold_results['test_mse_land_pos'][0]\n",
        "      ez_reader_mse_land_pos = fold_results['ez_reader_mse_land_pos'][0]\n",
        "      uniform_mse_land_pos = fold_results['uniform_mse_land_pos'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_land_pos, ez_reader_mse_land_pos)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader MSE scores for landing pos.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_land_pos, uniform_mse_land_pos)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform MSE scores for landing pos.\")\n",
        "\n",
        "      fold_index += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EuS0Od5cNWk",
        "outputId": "86b416fb-5beb-45db-d24f-97e1313c9696"
      },
      "id": "5EuS0Od5cNWk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and original Eyettention NLL scores.\n",
            "Reject null hypothesis: Statistically significant difference in NLL scores between predicted and uniformly generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the human scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the human scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and the uniform generated scanpaths.\n",
            "1\n",
            "Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and original Eyettention NLL scores.\n",
            "Reject null hypothesis: Statistically significant difference in NLL scores between predicted and uniformly generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the human scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the human scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and the uniform generated scanpaths.\n",
            "2\n",
            "Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and original Eyettention NLL scores.\n",
            "Reject null hypothesis: Statistically significant difference in NLL scores between predicted and uniformly generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the human scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the uniform generated scanpaths.\n",
            "Fail to reject null hypothesis: No statistically significant difference between predicted and human Central Scasim scores.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and the uniform generated scanpaths.\n",
            "3\n",
            "Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and original Eyettention NLL scores.\n",
            "Reject null hypothesis: Statistically significant difference in NLL scores between predicted and uniformly generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the human scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the human scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and the uniform generated scanpaths.\n",
            "4\n",
            "Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and original Eyettention NLL scores.\n",
            "Reject null hypothesis: Statistically significant difference in NLL scores between predicted and uniformly generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the human scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the human scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and the uniform generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and E-Z Reader generated scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and the uniform generated scanpaths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical tests for Eyettention Reader model ###"
      ],
      "metadata": {
        "id": "3pkTage1pUEO"
      },
      "id": "3pkTage1pUEO"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/BSC/New_Sentence/res_BSC_text_eyettention_Fold{fold_index}.pickle', 'rb') as handle:\n",
        "      print(fold_index)\n",
        "      fold_results = pickle.load(handle)\n",
        "\n",
        "      # NLL\n",
        "      test_ll = fold_results['test_ll'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "        reader_results = pickle.load(original)\n",
        "        reader_ll = reader_results['test_ll'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(test_ll, reader_ll)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in NLL scores between Eyettention 2.0 and Eyettention Reader scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader NLL scores.\")\n",
        "\n",
        "      # Scasim\n",
        "      scasim_dnn = fold_results['scasim_dnn'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "        reader_results = pickle.load(original)\n",
        "        reader_scasim = reader_results['scasim_dnn'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(scasim_dnn, reader_scasim)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader Scasim scores.\")\n",
        "\n",
        "      # Central Scasim\n",
        "      central_scasim_dnn = fold_results['central_scasim_dnn'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "        reader_results = pickle.load(original)\n",
        "        reader_central_scasim = reader_results['central_scasim_dnn'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, reader_central_scasim)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader Central Scasim scores.\")\n",
        "\n",
        "\n",
        "      # MSE durations\n",
        "      test_mse_dur = fold_results['test_mse_dur'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "        reader_results = pickle.load(original)\n",
        "        reader_test_mse_dur = reader_results['test_mse_dur'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(test_mse_dur, reader_test_mse_dur)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in MSE scores for dur between Eyettention 2.0 and Eyettention Reader scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader MSE scores for dur.\")\n",
        "\n",
        "\n",
        "      # MSE landing pos\n",
        "      test_mse_land_pos = fold_results['test_mse_land_pos'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "        reader_results = pickle.load(original)\n",
        "        reader_mse_land_pos = reader_results['test_mse_land_pos'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(test_mse_land_pos, reader_mse_land_pos)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in MSE scores for land pos between Eyettention 2.0 and Eyettention Reader scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader MSE scores for land pos.\")\n",
        "\n",
        "      fold_index += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTvCduYjotxs",
        "outputId": "3fed0acc-cd0e-4840-8699-0559b3ff4386"
      },
      "id": "TTvCduYjotxs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Reject null hypothesis: Statistically significant difference in NLL scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader MSE scores for dur.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for land pos between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "1\n",
            "Reject null hypothesis: Statistically significant difference in NLL scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for dur between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for land pos between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "2\n",
            "Reject null hypothesis: Statistically significant difference in NLL scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader MSE scores for dur.\n",
            "Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader MSE scores for land pos.\n",
            "3\n",
            "Reject null hypothesis: Statistically significant difference in NLL scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for dur between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for land pos between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "4\n",
            "Reject null hypothesis: Statistically significant difference in NLL scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in Central Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Reject null hypothesis: Statistically significant difference in MSE scores for dur between Eyettention 2.0 and Eyettention Reader scanpaths.\n",
            "Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader MSE scores for land pos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "      print(fold_index)\n",
        "      reader_results = pickle.load(original)\n",
        "      print(\"Mean NLL\", np.mean(reader_results['test_ll'][0]))\n",
        "      print(\"Standard error NLL\", np.std(reader_results['test_ll'][0]) / np.sqrt(len(reader_results['test_ll'][0])))\n",
        "\n",
        "      print(\"Mean MSE dur\", np.mean(reader_results['test_mse_dur'][0]))\n",
        "      print(\"Standard error MSE dur\", np.std(reader_results['test_mse_dur'][0]) / np.sqrt(len(reader_results['test_mse_dur'][0])))\n",
        "\n",
        "      print(\"Mean MSE landing pos\", np.mean(reader_results['test_mse_land_pos'][0]))\n",
        "      print(\"Standard error MSE landing pos\", np.std(reader_results['test_mse_land_pos'][0]) / np.sqrt(len(reader_results['test_mse_land_pos'][0])))\n",
        "\n",
        "      print(\"Mean scasim\", np.mean(reader_results['scasim_dnn'][0]))\n",
        "      print(\"Standard error scasim\", np.std(reader_results['scasim_dnn'][0]) / np.sqrt(len(reader_results['scasim_dnn'][0])))\n",
        "\n",
        "      print(\"Mean Central scasim\", np.mean(reader_results['central_scasim_dnn'][0]))\n",
        "      print(\"Standard error Central scasim\", np.std(reader_results['central_scasim_dnn'][0]) / np.sqrt(len(reader_results['central_scasim_dnn'][0])))\n",
        "\n",
        "      fold_index += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QoUIf6wLtyb",
        "outputId": "4e7b5c58-0aee-4c2a-dc32-ec3191ce95d6"
      },
      "id": "2QoUIf6wLtyb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Mean NLL -1.7063532229504164\n",
            "Standard error NLL 0.01668078753773475\n",
            "Mean MSE dur 0.026748189886272592\n",
            "Standard error MSE dur 0.0016587339361378115\n",
            "Mean MSE landing pos 0.01597299778599057\n",
            "Standard error MSE landing pos 0.00031287554719322194\n",
            "Mean scasim 1497.9135135135134\n",
            "Standard error scasim 16.59213294453015\n",
            "Mean Central scasim 1119.1123123123123\n",
            "Standard error Central scasim 11.15437896429574\n",
            "1\n",
            "Mean NLL -1.6739646501951082\n",
            "Standard error NLL 0.01540914844591275\n",
            "Mean MSE dur 0.027532242878109923\n",
            "Standard error MSE dur 0.0009107881491094707\n",
            "Mean MSE landing pos 0.017209847378478842\n",
            "Standard error MSE landing pos 0.00030188373714650293\n",
            "Mean scasim 1602.6235438381361\n",
            "Standard error scasim 17.444006162214325\n",
            "Mean Central scasim 1192.1771919068055\n",
            "Standard error Central scasim 10.65965850346854\n",
            "2\n",
            "Mean NLL -1.6659939160000978\n",
            "Standard error NLL 0.0146455686428165\n",
            "Mean MSE dur 0.028344671909403384\n",
            "Standard error MSE dur 0.001414337033659194\n",
            "Mean MSE landing pos 0.017460817964729192\n",
            "Standard error MSE landing pos 0.0003895428018365338\n",
            "Mean scasim 1605.4299575500304\n",
            "Standard error scasim 17.91386745705258\n",
            "Mean Central scasim 1173.7689508793208\n",
            "Standard error Central scasim 11.024375247623341\n",
            "3\n",
            "Mean NLL -1.5992242306377342\n",
            "Standard error NLL 0.014576616192985467\n",
            "Mean MSE dur 0.02575320542533939\n",
            "Standard error MSE dur 0.000890126171575056\n",
            "Mean MSE landing pos 0.017192534122205963\n",
            "Standard error MSE landing pos 0.00033525583182291417\n",
            "Mean scasim 1562.1064871481028\n",
            "Standard error scasim 17.879959152947563\n",
            "Mean Central scasim 1186.0416156670747\n",
            "Standard error Central scasim 12.283164260057383\n",
            "4\n",
            "Mean NLL -1.663116664937798\n",
            "Standard error NLL 0.014766350952615736\n",
            "Mean MSE dur 0.025774543410987616\n",
            "Standard error MSE dur 0.0007489008936140284\n",
            "Mean MSE landing pos 0.016321106003735174\n",
            "Standard error MSE landing pos 0.0002996371923271074\n",
            "Mean scasim 1541.5820622330689\n",
            "Standard error scasim 16.64419453118263\n",
            "Mean Central scasim 1170.031726662599\n",
            "Standard error Central scasim 10.564486795784891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Analysing the results\n",
        "test_ll_all_folds = []\n",
        "test_mse_dur_all_folds = []\n",
        "test_mse_land_pos_all_folds = []\n",
        "scasim_dnn_all_folds = []\n",
        "scasim_human_all_folds = []\n",
        "central_scasim_dnn_all_folds = []\n",
        "central_scasim_human_all_folds = []\n",
        "\n",
        "uniform_ll_all_folds = []\n",
        "uniform_mse_dur_all_folds = []\n",
        "uniform_mse_land_pos_all_folds = []\n",
        "uniform_scasim_all_folds = []\n",
        "uniform_central_scasim_all_folds = []\n",
        "\n",
        "ez_reader_mse_dur_all_folds = []\n",
        "ez_reader_mse_land_pos_all_folds = []\n",
        "ez_reader_scasim_all_folds = []\n",
        "ez_reader_central_scasim_all_folds = []\n",
        "\n",
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/BSC/NRS/res_BSC_eyettention_NRS_Fold{fold_index}.pickle', 'rb') as original:\n",
        "      reader_results = pickle.load(original)\n",
        "      test_ll_all_folds.extend(reader_results['test_ll'][0])\n",
        "      test_mse_dur_all_folds.extend(reader_results['test_mse_dur'][0])\n",
        "      test_mse_land_pos_all_folds.extend(reader_results['test_mse_land_pos'][0])\n",
        "      scasim_dnn_all_folds.extend(reader_results['scasim_dnn'][0])\n",
        "      scasim_human_all_folds.extend(reader_results['scasim_human'][0])\n",
        "      central_scasim_dnn_all_folds.extend(reader_results['central_scasim_dnn'][0])\n",
        "      central_scasim_human_all_folds.extend(reader_results['central_scasim_human'][0])\n",
        "\n",
        "      uniform_ll_all_folds.extend(reader_results['uniform_nll'][0])\n",
        "      uniform_mse_dur_all_folds.extend(reader_results['uniform_mse_dur'][0])\n",
        "      uniform_mse_land_pos_all_folds.extend(reader_results['uniform_mse_land_pos'][0])\n",
        "      uniform_scasim_all_folds.extend(reader_results['uniform_scasim'][0])\n",
        "      uniform_central_scasim_all_folds.extend(reader_results['uniform_central_scasim'][0])\n",
        "\n",
        "      ez_reader_mse_dur_all_folds.extend(reader_results['ez_reader_mse_dur'][0])\n",
        "      ez_reader_mse_land_pos_all_folds.extend(reader_results['ez_reader_mse_land_pos'][0])\n",
        "      ez_reader_scasim_all_folds.extend(reader_results['ez_reader_scasim'][0])\n",
        "      ez_reader_central_scasim_all_folds.extend(reader_results['ez_reader_central_scasim'][0])\n",
        "\n",
        "      fold_index += 1\n",
        "\n",
        "# Calculate overall statistics\n",
        "def calculate_mean_and_se(values):\n",
        "    mean_val = np.mean(values)\n",
        "    se_val = np.std(values) / np.sqrt(len(values))\n",
        "    return mean_val, se_val\n",
        "\n",
        "mean_nll, se_nll = calculate_mean_and_se(test_ll_all_folds)\n",
        "mean_mse_dur, se_mse_dur = calculate_mean_and_se(test_mse_dur_all_folds)\n",
        "mean_mse_land_pos, se_mse_land_pos = calculate_mean_and_se(test_mse_land_pos_all_folds)\n",
        "mean_scasim, se_scasim = calculate_mean_and_se(scasim_dnn_all_folds)\n",
        "mean_central_scasim, se_central_scasim = calculate_mean_and_se(central_scasim_dnn_all_folds)\n",
        "mean_scasim_human, se_scasim_human = calculate_mean_and_se(scasim_human_all_folds)\n",
        "mean_central_scasim_human, se_central_scasim_human = calculate_mean_and_se(central_scasim_human_all_folds)\n",
        "\n",
        "mean_nll_uniform, se_nll_uniform = calculate_mean_and_se(uniform_ll_all_folds)\n",
        "mean_mse_dur_uniform, se_mse_dur_uniform = calculate_mean_and_se(uniform_mse_dur_all_folds)\n",
        "mean_mse_land_pos_uniform, se_mse_land_pos_uniform = calculate_mean_and_se(uniform_mse_land_pos_all_folds)\n",
        "mean_scasim_uniform, se_scasim_uniform = calculate_mean_and_se(uniform_scasim_all_folds)\n",
        "mean_central_scasim_uniform, se_central_scasim_uniform = calculate_mean_and_se(uniform_central_scasim_all_folds)\n",
        "\n",
        "mean_nll_ez_reader, se_nll_ez_reader = calculate_mean_and_se(ez_reader_mse_dur_all_folds)\n",
        "mean_mse_dur_ez_reader, se_mse_dur_ez_reader = calculate_mean_and_se(ez_reader_mse_dur_all_folds)\n",
        "mean_mse_land_pos_ez_reader, se_mse_land_pos_ez_reader = calculate_mean_and_se(ez_reader_mse_land_pos_all_folds)\n",
        "mean_scasim_ez_reader, se_scasim_ez_reader = calculate_mean_and_se(ez_reader_scasim_all_folds)\n",
        "mean_central_scasim_ez_reader, se_central_scasim_ez_reader = calculate_mean_and_se(ez_reader_central_scasim_all_folds)\n",
        "\n",
        "\n",
        "print(\"Overall Mean NLL:\", mean_nll)\n",
        "print(\"Overall Standard error NLL:\", se_nll)\n",
        "\n",
        "print(\"Overall Mean MSE dur:\", mean_mse_dur)\n",
        "print(\"Overall Standard error MSE dur:\", se_mse_dur)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos:\", mean_mse_land_pos)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos)\n",
        "\n",
        "print(\"Overall Mean scasim:\", mean_scasim)\n",
        "print(\"Overall Standard error scasim:\", se_scasim)\n",
        "\n",
        "print(\"Overall Mean Central scasim:\", mean_central_scasim)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim)\n",
        "\n",
        "print(\"Human Mean scasim:\", mean_scasim_human)\n",
        "print(\"Human Standard error scasim:\", se_scasim_human)\n",
        "\n",
        "print(\"Human Mean central scasim:\", mean_central_scasim_human)\n",
        "print(\"Human Standard error central scasim:\", se_central_scasim_human)\n",
        "\n",
        "\n",
        "print(\"Overall Mean NLL Uniform:\", mean_nll_uniform)\n",
        "print(\"Overall Standard error NLL:\", se_nll_uniform)\n",
        "\n",
        "print(\"Overall Mean MSE dur Uniform:\", mean_mse_dur_uniform)\n",
        "print(\"Overall Standard error MSE dur Uniform:\", se_mse_dur_uniform)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos Uniform:\", mean_mse_land_pos_uniform)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos_uniform)\n",
        "\n",
        "print(\"Overall Mean scasim Uniform:\", mean_scasim_uniform)\n",
        "print(\"Overall Standard error scasim:\", se_scasim_uniform)\n",
        "\n",
        "print(\"Overall Mean Central scasim Uniform:\", mean_central_scasim_uniform)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim_uniform)\n",
        "\n",
        "\n",
        "print(\"Overall Mean MSE dur E-Z Reader:\", mean_mse_dur_ez_reader)\n",
        "print(\"Overall Standard error MSE dur E-Z Reader:\", se_mse_dur_ez_reader)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos E-Z Reader:\", mean_mse_land_pos_ez_reader)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos_ez_reader)\n",
        "\n",
        "print(\"Overall Mean scasim E-Z Reader:\", mean_scasim_ez_reader)\n",
        "print(\"Overall Standard error scasim:\", se_scasim_ez_reader)\n",
        "\n",
        "print(\"Overall Mean Central scasim E-Z Reader:\", mean_central_scasim_ez_reader)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim_ez_reader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp6ZOF4LN1fa",
        "outputId": "715ad8a1-fb31-4763-f174-9561e5c1e06e"
      },
      "id": "zp6ZOF4LN1fa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Mean NLL: -1.8679076865880513\n",
            "Overall Standard error NLL: 0.0224090032865253\n",
            "Overall Mean MSE dur: 0.0338717173917869\n",
            "Overall Standard error MSE dur: 0.001607876421019088\n",
            "Overall Mean MSE landing pos: 0.021182888840535943\n",
            "Overall Standard error MSE landing pos: 0.0005743238866711326\n",
            "Overall Mean scasim: 1781.6722408026756\n",
            "Overall Standard error scasim: 26.76816169022476\n",
            "Overall Mean Central scasim: 1325.6845039018951\n",
            "Overall Standard error Central scasim: 21.18388082927654\n",
            "Human Mean scasim: 2005.7915273132664\n",
            "Human Standard error scasim: 25.726573687899954\n",
            "Human Mean central scasim: 1448.376811594203\n",
            "Human Standard error central scasim: 20.415768197837465\n",
            "Overall Mean NLL Uniform: 9.334238482127056\n",
            "Overall Standard error NLL: 0.2567375439310384\n",
            "Overall Mean MSE dur Uniform: 4.600963492898176\n",
            "Overall Standard error MSE dur Uniform: 0.08529559185309658\n",
            "Overall Mean MSE landing pos Uniform: 2.199971367009075\n",
            "Overall Standard error MSE landing pos: 0.030916148712469713\n",
            "Overall Mean scasim Uniform: 5218.913625304136\n",
            "Overall Standard error scasim: 115.66268900141387\n",
            "Overall Mean Central scasim Uniform: 4957.9209245742095\n",
            "Overall Standard error Central scasim: 116.9117551397478\n",
            "Overall Mean MSE dur E-Z Reader: 4.133273407571456\n",
            "Overall Standard error MSE dur E-Z Reader: 0.0802275652272341\n",
            "Overall Mean MSE landing pos E-Z Reader: 1.9840548668275861\n",
            "Overall Standard error MSE landing pos: 0.02599560750999745\n",
            "Overall Mean scasim E-Z Reader: 3016.0588235294117\n",
            "Overall Standard error scasim: 25.438787621511192\n",
            "Overall Mean Central scasim E-Z Reader: 2760.8717647058825\n",
            "Overall Standard error Central scasim: 22.19928210511537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Analysing the results\n",
        "test_ll_all_folds = []\n",
        "test_mse_dur_all_folds = []\n",
        "test_mse_land_pos_all_folds = []\n",
        "scasim_dnn_all_folds = []\n",
        "scasim_human_all_folds = []\n",
        "central_scasim_dnn_all_folds = []\n",
        "central_scasim_human_all_folds = []\n",
        "\n",
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "      reader_results = pickle.load(original)\n",
        "      test_ll_all_folds.extend(reader_results['test_ll'][0])\n",
        "      test_mse_dur_all_folds.extend(reader_results['test_mse_dur'][0])\n",
        "      test_mse_land_pos_all_folds.extend(reader_results['test_mse_land_pos'][0])\n",
        "      scasim_dnn_all_folds.extend(reader_results['scasim_dnn'][0])\n",
        "      scasim_human_all_folds.extend(reader_results['scasim_human'][0])\n",
        "      central_scasim_dnn_all_folds.extend(reader_results['central_scasim_dnn'][0])\n",
        "      central_scasim_human_all_folds.extend(reader_results['central_scasim_human'][0])\n",
        "\n",
        "      fold_index += 1\n",
        "\n",
        "# Calculate overall statistics\n",
        "def calculate_mean_and_se(values):\n",
        "    mean_val = np.mean(values)\n",
        "    se_val = np.std(values) / np.sqrt(len(values))\n",
        "    return mean_val, se_val\n",
        "\n",
        "mean_nll, se_nll = calculate_mean_and_se(test_ll_all_folds)\n",
        "mean_mse_dur, se_mse_dur = calculate_mean_and_se(test_mse_dur_all_folds)\n",
        "mean_mse_land_pos, se_mse_land_pos = calculate_mean_and_se(test_mse_land_pos_all_folds)\n",
        "mean_scasim, se_scasim = calculate_mean_and_se(scasim_dnn_all_folds)\n",
        "mean_central_scasim, se_central_scasim = calculate_mean_and_se(central_scasim_dnn_all_folds)\n",
        "mean_scasim_human, se_scasim_human = calculate_mean_and_se(scasim_human_all_folds)\n",
        "mean_central_scasim_human, se_central_scasim_human = calculate_mean_and_se(central_scasim_human_all_folds)\n",
        "\n",
        "\n",
        "print(\"Overall Mean NLL:\", mean_nll)\n",
        "print(\"Overall Standard error NLL:\", se_nll)\n",
        "\n",
        "print(\"Overall Mean MSE dur:\", mean_mse_dur)\n",
        "print(\"Overall Standard error MSE dur:\", se_mse_dur)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos:\", mean_mse_land_pos)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos)\n",
        "\n",
        "print(\"Overall Mean scasim:\", mean_scasim)\n",
        "print(\"Overall Standard error scasim:\", se_scasim)\n",
        "\n",
        "print(\"Overall Mean Central scasim:\", mean_central_scasim)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim)\n",
        "\n",
        "print(\"Human Mean scasim:\", mean_scasim_human)\n",
        "print(\"Human Standard error scasim:\", se_scasim_human)\n",
        "\n",
        "print(\"Human Mean central scasim:\", mean_central_scasim_human)\n",
        "print(\"Human Standard error central scasim:\", se_central_scasim_human)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8TKfhG_8Nb6",
        "outputId": "e25fb9f9-d6d8-4d8c-b395-c17130917115"
      },
      "id": "e8TKfhG_8Nb6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Mean NLL: -1.6619030220077797\n",
            "Overall Standard error NLL: 0.006828714418786473\n",
            "Overall Mean MSE dur: 0.02683212491957418\n",
            "Overall Standard error MSE dur: 0.0005290665547081549\n",
            "Overall Mean MSE landing pos: 0.016828922451644954\n",
            "Overall Standard error MSE landing pos: 0.00014755721016053438\n",
            "Overall Mean scasim: 1561.7417863227063\n",
            "Overall Standard error scasim: 7.749995588290035\n",
            "Overall Mean Central scasim: 1168.043562910684\n",
            "Overall Standard error Central scasim: 4.9962378088620785\n",
            "Human Mean scasim: 2043.5821367729375\n",
            "Human Standard error scasim: 8.834500910232864\n",
            "Human Mean central scasim: 1483.103188123631\n",
            "Human Standard error central scasim: 7.285316093898042\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}