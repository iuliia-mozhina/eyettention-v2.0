{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "nuclear-dream",
      "metadata": {
        "id": "nuclear-dream"
      },
      "source": [
        "# Eyettention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0978c758-ea14-4df4-97d0-6e0c1ed18428",
      "metadata": {
        "id": "0978c758-ea14-4df4-97d0-6e0c1ed18428"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import model\n",
        "import torch\n",
        "from torch.utils import model_zoo\n",
        "import pandas as pd\n",
        "from utils import *\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, RMSprop\n",
        "from transformers import BertTokenizerFast\n",
        "from model import Eyettention\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from torch.nn.functional import cross_entropy, softmax\n",
        "from collections import deque, Counter\n",
        "import pickle\n",
        "from transformers import BertTokenizer\n",
        "from evaluate_e_z_reader_model import *\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import random\n",
        "from scasim import *\n",
        "from uniform_model import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8dfb0f8-8775-4609-b75c-6052916ca9c0",
      "metadata": {
        "id": "c8dfb0f8-8775-4609-b75c-6052916ca9c0"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "#DEVICE = 'cuda'\n",
        "DEVICE = 'cpu'\n",
        "test_mode = 'text'\n",
        "#test_mode = 'subject'\n",
        "scanpath_gen_flag = True\n",
        "atten_type = \"local_g\"\n",
        "save_data_folder = \"./drive/MyDrive/results/BSC/New_Sentence\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4l7DoNdYwUn9",
      "metadata": {
        "id": "4l7DoNdYwUn9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e085041-7c16-44b4-9129-bf667f64c552",
      "metadata": {
        "id": "6e085041-7c16-44b4-9129-bf667f64c552"
      },
      "source": [
        "**Training loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28WYhnrePEZj",
      "metadata": {
        "id": "28WYhnrePEZj"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\tgpu = 0\n",
        "\n",
        "\ttorch.set_default_tensor_type('torch.FloatTensor')\n",
        "\tavailbl = torch.cuda.is_available()\n",
        "\tif availbl:\n",
        "\t\tdevice = f'cuda:{gpu}'\n",
        "\telse:\n",
        "\t\tdevice = 'cpu'\n",
        "\t#torch.cuda.set_device(gpu)\n",
        "\n",
        "\tcf = {\"model_pretrained\": \"bert-base-chinese\",\n",
        "\t\t\t\"lr\": 1e-3,\n",
        "\t\t\t\"max_grad_norm\": 10,\n",
        "\t\t\t\"n_epochs\": 150,  # 1000\n",
        "\t\t\t\"n_folds\": 5,\n",
        "\t\t\t\"dataset\": 'BSC',\n",
        "\t\t\t\"atten_type\": 'local-g',\n",
        "\t\t\t\"batch_size\": 256,\n",
        "\t\t\t\"max_sn_len\": 27, #include start token and end token\n",
        "\t\t\t\"max_sp_len\": 40, #include start token and end token\n",
        "\t\t\t\"norm_type\": \"z-score\",\n",
        "\t\t\t\"earlystop_patience\": 20,\n",
        "\t\t\t\"max_pred_len\": 60\n",
        "\t\t\t}\n",
        "\n",
        "\t#Encode the label into interger categories, setting the exclusive category 'cf[\"max_sn_len\"]-1' as the end sign\n",
        "\tle = LabelEncoder()\n",
        "\tle.fit(np.append(np.arange(-cf[\"max_sn_len\"]+3, cf[\"max_sn_len\"]-1), cf[\"max_sn_len\"]-1))\n",
        "\t#le.classes_\n",
        "\n",
        "\t#load corpus\n",
        "\tword_info_df, pos_info_df, eyemovement_df = load_corpus(cf[\"dataset\"])\n",
        "\t#Make list with sentence index\n",
        "\tsn_list = np.unique(eyemovement_df.sn.values).tolist()\n",
        "\t#Make list with reader index\n",
        "\treader_list = np.unique(eyemovement_df.id.values).tolist()\n",
        "\n",
        "\t#Split training&test sets by text or reader, depending on configuration\n",
        "\tif test_mode == 'text':\n",
        "\t\tprint('Start evaluating on new sentences.')\n",
        "\t\tsplit_list = sn_list\n",
        "\telif test_mode == 'subject':\n",
        "\t\tprint('Start evaluating on new readers.')\n",
        "\t\tsplit_list = reader_list\n",
        "\n",
        "\tn_folds = cf[\"n_folds\"]\n",
        "\tkf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
        "\tfold_indx = 0\n",
        "\t#for scanpath generation\n",
        "\tsp_dnn_list = []\n",
        "\tsp_human_list = []\n",
        "\tfor train_idx, test_idx in kf.split(split_list):\n",
        "\n",
        "\t\tloss_dict = {'val_loss':[], 'train_loss':[], 'test_ll':[], 'test_ll_SE':[], 'test_mse_dur':[], 'test_mse_dur_SE':[], 'test_mse_land_pos':[], 'test_mse_land_pos_SE':[], 'central_scasim_dnn':[], 'central_scasim_dnn_SE':[], 'central_scasim_human':[], 'central_scasim_human_SE':[], 'scasim_dnn':[], 'scasim_dnn_SE':[], 'scasim_human':[], 'scasim_human_SE':[], 'uniform_scasim':[], 'uniform_scasim_SE':[], 'uniform_central_scasim':[], 'uniform_central_scasim_SE':[], 'uniform_nll_SE':[], 'uniform_nll':[], 'uniform_mse_dur_SE':[], 'uniform_mse_dur':[], 'uniform_mse_land_pos_SE':[], 'uniform_mse_land_pos':[],\n",
        "\t\t             'ez_reader_scasim':[], 'ez_reader_scasim_SE':[], 'ez_reader_central_scasim':[], 'ez_reader_central_scasim_SE':[], 'ez_reader_mse_dur_SE':[], 'ez_reader_mse_dur':[], 'ez_reader_mse_land_pos_SE':[], 'ez_reader_mse_land_pos':[]}\n",
        "\t\tlist_train = [split_list[i] for i in train_idx]\n",
        "\t\tlist_test = [split_list[i] for i in test_idx]\n",
        "\n",
        "\t\t# create train validation split for training the models:\n",
        "\t\tkf_val = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
        "\t\tfor train_index, val_index in kf_val.split(list_train):\n",
        "\t\t\t# we only evaluate a single fold\n",
        "\t\t\tbreak\n",
        "\t\tlist_train_net = [list_train[i] for i in train_index]\n",
        "\t\tlist_val_net = [list_train[i] for i in val_index]\n",
        "\n",
        "\t\tif test_mode == 'text':\n",
        "\t\t\tsn_list_train = list_train_net\n",
        "\t\t\tsn_list_val = list_val_net\n",
        "\t\t\tsn_list_test = list_test\n",
        "\t\t\treader_list_train, reader_list_val, reader_list_test = reader_list, reader_list, reader_list\n",
        "\n",
        "\t\telif test_mode == 'subject':\n",
        "\t\t\treader_list_train = list_train_net\n",
        "\t\t\treader_list_val = list_val_net\n",
        "\t\t\treader_list_test = list_test\n",
        "\t\t\tsn_list_train, sn_list_val, sn_list_test = sn_list, sn_list, sn_list\n",
        "\n",
        "\t\t#initialize tokenizer\n",
        "\t\ttokenizer = BertTokenizer.from_pretrained(cf['model_pretrained'])\n",
        "\t\t#Preparing batch data\n",
        "\t\tdataset_train = BSCdataset(word_info_df, eyemovement_df, cf, reader_list_train, sn_list_train, tokenizer)\n",
        "\t\ttrain_dataloaderr = DataLoader(dataset_train, batch_size = cf[\"batch_size\"], shuffle = True, drop_last=True)\n",
        "\n",
        "\t\tdataset_val = BSCdataset(word_info_df, eyemovement_df, cf, reader_list_val, sn_list_val, tokenizer)\n",
        "\t\tval_dataloaderr = DataLoader(dataset_val, batch_size = cf[\"batch_size\"], shuffle = False, drop_last=True)\n",
        "\n",
        "\t\tdataset_test = BSCdataset(word_info_df, eyemovement_df, cf, reader_list_test, sn_list_test, tokenizer)\n",
        "\t\ttest_dataloaderr = DataLoader(dataset_test, batch_size = cf[\"batch_size\"], shuffle = False, drop_last=False)\n",
        "\n",
        "\t\t#z-score normalization for gaze features\n",
        "\t\tfix_dur_mean, fix_dur_std = calculate_mean_std(dataloader=train_dataloaderr, feat_key=\"sp_fix_dur\", padding_value=0, scale=1000)\n",
        "\t\tlanding_pos_mean, landing_pos_std = calculate_mean_std(dataloader=train_dataloaderr, feat_key=\"sp_landing_pos\", padding_value=0)\n",
        "\t\tsn_word_len_mean, sn_word_len_std = calculate_mean_std(dataloader=train_dataloaderr, feat_key=\"sn_word_len\")\n",
        "\n",
        "\t\t# load model\n",
        "\t\tdnn = Eyettention(cf)\n",
        "\n",
        "\t\t#training\n",
        "\t\tepisode = 0\n",
        "\t\toptimizer = Adam(dnn.parameters(), lr=cf[\"lr\"])\n",
        "\t\tdnn.train()\n",
        "\t\tdnn.to(device)\n",
        "\t\tav_score = deque(maxlen=100)\n",
        "\t\tav_location_score = deque(maxlen=100)\n",
        "\t\tav_duration_score = deque(maxlen=100)\n",
        "\t\tav_land_pos_score = deque(maxlen=100)\n",
        "\t\told_score = 1e10\n",
        "\t\tsave_ep_couter = 0\n",
        "\t\tprint('Start training')\n",
        "\t\tprint(\"fold_indx\", fold_indx)\n",
        "\t\tfor episode_i in range(episode, cf[\"n_epochs\"]+1):\n",
        "\t\t\tdnn.train()\n",
        "\t\t\tprint('episode:', episode_i)\n",
        "\t\t\tcounter = 0\n",
        "\t\t\tfor batchh in train_dataloaderr:\n",
        "\t\t\t\tcounter += 1\n",
        "\t\t\t\tbatchh.keys()\n",
        "\t\t\t\tsn_ids = batchh[\"sn_ids\"].to(device)\n",
        "\t\t\t\tsn_input_ids = batchh[\"sn_input_ids\"].to(device)\n",
        "\t\t\t\tsn_attention_mask = batchh[\"sn_attention_mask\"].to(device)\n",
        "\t\t\t\tsp_input_ids = batchh[\"sp_input_ids\"].to(device)\n",
        "\t\t\t\tsp_attention_mask = batchh[\"sp_attention_mask\"].to(device)\n",
        "\t\t\t\tsp_pos = batchh[\"sp_pos\"].to(device)\n",
        "\t\t\t\tsp_landing_pos = batchh[\"sp_landing_pos\"].to(device) # [256, 40]\n",
        "\t\t\t\tsp_fix_dur = (batchh[\"sp_fix_dur\"]/1000).to(device) # [256, 40]\n",
        "\t\t\t\tsn_word_len = batchh[\"sn_word_len\"].to(device)\n",
        "\n",
        "\t\t\t\t# normalize gaze features (z-score normalisation)\n",
        "\t\t\t\tmask = ~torch.eq(sp_fix_dur, 0)\n",
        "\t\t\t\tsp_fix_dur = (sp_fix_dur-fix_dur_mean)/fix_dur_std * mask\n",
        "\t\t\t\tsp_fix_dur = torch.nan_to_num(sp_fix_dur) # [256, 40]\n",
        "\t\t\t\tsp_landing_pos = (sp_landing_pos - landing_pos_mean)/landing_pos_std * mask\n",
        "\t\t\t\tsp_landing_pos = torch.nan_to_num(sp_landing_pos)\n",
        "\t\t\t\tsn_word_len = (sn_word_len - sn_word_len_mean)/sn_word_len_std\n",
        "\t\t\t\tsn_word_len = torch.nan_to_num(sn_word_len)\n",
        "\n",
        "\t\t\t\t# zero old gradients\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\t# predict output with DNN\n",
        "\t\t\t\tlocation_preds, duration_preds, landing_pos_preds, atten_weights = dnn(sn_emd=sn_input_ids,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_mask=sn_attention_mask,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_emd=sp_input_ids,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_pos=sp_pos,\n",
        "\t\t\t\t\t\t\t\t\t\t\tword_ids_sn=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\tword_ids_sp=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_fix_dur=sp_fix_dur,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_landing_pos=sp_landing_pos,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_word_len = sn_word_len,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_pred=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_word_freq=None\n",
        "\t\t\t\t\t\t\t\t\t\t\t                                            )#[batch, step, dec_o_dim]\n",
        "\n",
        "\t\t\t\tlocation_preds = location_preds.permute(0,2,1)              #[batch, dec_o_dim, step]\n",
        "\n",
        "\t\t\t\t#prepare label and mask\n",
        "\t\t\t\t# Compute loss for fixation locations\n",
        "\t\t\t\tpad_mask, label = load_label(sp_pos, cf, le, device)\n",
        "\t\t\t\tloss = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\t\t\t\tbatch_location_error = torch.mean(torch.masked_select(loss(location_preds, label), ~pad_mask))\n",
        "\n",
        "\t\t\t\t# Compute loss for fixation durations\n",
        "\t\t\t\tduration_labels = sp_fix_dur[:, :39] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\tduration_preds = duration_preds.squeeze(-1)  # Remove extra dimension (from [256, 39, 1] to [256, 39])\n",
        "\t\t\t\tdur_loss = nn.MSELoss(reduction=\"none\")\n",
        "\t\t\t\tbatch_duration_error = torch.mean(dur_loss(duration_preds, duration_labels))\n",
        "\n",
        "\t\t\t\t# Compute loss for landing position\n",
        "\t\t\t\tlanding_pos_labels = sp_landing_pos[:, :39] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\tlanding_pos_preds = landing_pos_preds.squeeze(-1)  # Remove extra dimension (from [256, 39, 1] to [256, 39])\n",
        "\t\t\t\tland_pos_loss = nn.MSELoss(reduction=\"none\")\n",
        "\t\t\t\tbatch_land_pos_error = torch.mean(land_pos_loss(landing_pos_preds, landing_pos_labels))\n",
        "\n",
        "\t\t\t\t# Combined loss for both location and duration\n",
        "\t\t\t\tbatch_error = batch_location_error + batch_duration_error + batch_land_pos_error\n",
        "\n",
        "\t\t\t\t# backpropagate loss\n",
        "\t\t\t\tbatch_error.backward()\n",
        "\t\t\t\t# clip gradients\n",
        "\t\t\t\tgradient_clipping(dnn, cf[\"max_grad_norm\"])\n",
        "\n",
        "\t\t\t\t#learn\n",
        "\t\t\t\toptimizer.step()\n",
        "\t\t\t\tav_location_score.append(batch_location_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tav_duration_score.append(batch_duration_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tav_land_pos_score.append(batch_land_pos_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tav_score.append(batch_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tprint('counter:',counter)\n",
        "\t\t\t\tprint('\\rSample {}\\tLocation Loss: {:.10f}\\tDuration Loss: {:.10f}\\tLanding position Loss: {:.10f}'.format(\n",
        "          counter, np.mean(av_location_score), np.mean(av_duration_score), np.mean(av_land_pos_score)), end=\" \")\n",
        "\t\t\tloss_dict['train_loss'].append(np.mean(av_score))\n",
        "\n",
        "\t\t\tlocation_val_loss = []\n",
        "\t\t\tduration_val_loss = []\n",
        "\t\t\tland_pos_val_loss = []\n",
        "\t\t\tval_loss = []\n",
        "\t\t\tdnn.eval()\n",
        "\t\t\tfor batchh in val_dataloaderr:\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tsn_ids_val = batchh[\"sn_ids\"].to(device)\n",
        "\t\t\t\t\tsn_input_ids_val = batchh[\"sn_input_ids\"].to(device)\n",
        "\t\t\t\t\tsn_attention_mask_val = batchh[\"sn_attention_mask\"].to(device)\n",
        "\t\t\t\t\tsp_input_ids_val = batchh[\"sp_input_ids\"].to(device)\n",
        "\t\t\t\t\tsp_attention_mask_val = batchh[\"sp_attention_mask\"].to(device)\n",
        "\t\t\t\t\tsp_pos_val = batchh[\"sp_pos\"].to(device)\n",
        "\t\t\t\t\tsp_landing_pos_val = batchh[\"sp_landing_pos\"].to(device)\n",
        "\t\t\t\t\tsp_fix_dur_val = (batchh[\"sp_fix_dur\"]/1000).to(device)\n",
        "\t\t\t\t\tsn_word_len_val = batchh[\"sn_word_len\"].to(device)\n",
        "\n",
        "\t\t\t\t\t#normalize gaze features\n",
        "\t\t\t\t\tmask = ~torch.eq(sp_fix_dur_val, 0)\n",
        "\t\t\t\t\tsp_fix_dur_val = (sp_fix_dur_val-fix_dur_mean)/fix_dur_std * mask\n",
        "\t\t\t\t\tsp_landing_pos_val = (sp_landing_pos_val - landing_pos_mean)/landing_pos_std * mask\n",
        "\t\t\t\t\tsp_fix_dur_val = torch.nan_to_num(sp_fix_dur_val)\n",
        "\t\t\t\t\tsp_landing_pos_val = torch.nan_to_num(sp_landing_pos_val)\n",
        "\t\t\t\t\tsn_word_len_val = (sn_word_len_val - sn_word_len_mean)/sn_word_len_std\n",
        "\t\t\t\t\tsn_word_len_val = torch.nan_to_num(sn_word_len_val)\n",
        "\n",
        "\t\t\t\t\tlocation_preds_val, duration_preds_val, landing_pos_preds_val, atten_weights_val = dnn(sn_emd=sn_input_ids_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_mask=sn_attention_mask_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_emd=sp_input_ids_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_pos=sp_pos_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sn=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sp=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_fix_dur=sp_fix_dur_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_landing_pos=sp_landing_pos_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_len = sn_word_len_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_pred = None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_freq = None)#[batch, step, dec_o_dim]\n",
        "\t\t\t\t\tlocation_preds_val = location_preds_val.permute(0,2,1)              #[batch, dec_o_dim, step\n",
        "\n",
        "\t\t\t\t\t# Compute location prediction error\n",
        "\t\t\t\t\tloss = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\t\t\t\t\tpad_mask_val, label_val = load_label(sp_pos_val, cf, le, device)\n",
        "\t\t\t\t\tlocation_error_val = torch.mean(torch.masked_select(loss(location_preds_val, label_val), ~pad_mask_val))\n",
        "\t\t\t\t\tlocation_val_loss.append(location_error_val.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\t\t\t# Compute duration prediction error\n",
        "\t\t\t\t\tduration_labels_val = sp_fix_dur_val[:, :39] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\t\tduration_preds_val = duration_preds_val.squeeze(-1)\n",
        "\t\t\t\t\tduration_error_val = torch.mean(dur_loss(duration_preds_val, duration_labels_val))\n",
        "\t\t\t\t\tduration_val_loss.append(duration_error_val.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\t\t\t# Compute loss for landing position\n",
        "\t\t\t\t\tlanding_pos_labels_val = sp_landing_pos_val[:, :39] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\t\tlanding_pos_preds_val = landing_pos_preds_val.squeeze(-1)  # Remove extra dimension (from [256, 39, 1] to [256, 39])\n",
        "\t\t\t\t\tland_pos_error_val = torch.mean(land_pos_loss(landing_pos_preds_val, landing_pos_labels_val))\n",
        "\t\t\t\t\tland_pos_val_loss.append(land_pos_error_val.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\t\t\tcombined_loss = location_error_val + duration_error_val + land_pos_error_val\n",
        "\t\t\t\t\tval_loss.append(combined_loss.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\tprint('\\nValidation loss for locations {} \\n'.format(np.mean(location_val_loss)))\n",
        "\t\t\tprint('\\nValidation loss for duration {} \\n'.format(np.mean(duration_val_loss)))\n",
        "\t\t\tprint('\\nValidation loss for landing position {} \\n'.format(np.mean(land_pos_val_loss)))\n",
        "\t\t\tloss_dict['val_loss'].append(np.mean(val_loss))\n",
        "\n",
        "\t\t\tif np.mean(val_loss) < old_score:\n",
        "\t\t\t\t# save model if val loss is smallest\n",
        "\t\t\t\ttorch.save(dnn.state_dict(), '{}/BSC_3head_arch_new_sentence_{}.pth'.format(save_data_folder, fold_indx))\n",
        "\t\t\t\told_score = np.mean(val_loss)\n",
        "\t\t\t\tprint('\\nsaved model state dict\\n')\n",
        "\t\t\t\tsave_ep_couter = episode_i\n",
        "\t\t\telse:\n",
        "\t\t\t\t#early stopping\n",
        "\t\t\t\tif episode_i - save_ep_couter >= cf[\"earlystop_patience\"]:\n",
        "\t\t\t\t\tbreak\n",
        "\t\tfold_indx += 1\n",
        "\n",
        "\t\t#evaluation\n",
        "\t\tdnn.eval()\n",
        "\t\tres_llh=[]\n",
        "\t\tres_mse_dur = []\n",
        "\t\tres_mse_land_pos = []\n",
        "\t\tres_central_scasim_human = []\n",
        "\t\tres_central_scasim_dnn = []\n",
        "\t\tres_scasim_human = []\n",
        "\t\tres_scasim_dnn = []\n",
        "\t\tuniform_central_scasim_scores = []\n",
        "\t\tuniform_scasim_scores = []\n",
        "\t\tuniform_nll_scores = []\n",
        "\t\tuniform_mse_dur_scores = []\n",
        "\t\tuniform_mse_land_pos_scores = []\n",
        "\t\tez_reader_central_scasim_scores = []\n",
        "\t\tez_reader_scasim_scores = []\n",
        "\t\tez_reader_nll_scores = []\n",
        "\t\tez_reader_mse_dur_scores = []\n",
        "\t\tez_reader_mse_land_pos_scores = []\n",
        "\t\tdnn.load_state_dict(torch.load(os.path.join(save_data_folder, f'BSC_3head_arch_new_sentence_{fold_indx}.pth'), map_location='cpu'))\n",
        "\t\tdnn.to(device)\n",
        "\t\tbatch_indx = 0\n",
        "\t\tprint(\"Evaluating for fold\", fold_indx)\n",
        "\t\tfor batchh in test_dataloaderr:\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tsn_ids_test = batchh[\"sn_ids\"].to(device)\n",
        "\t\t\t\tsn_input_ids_test = batchh[\"sn_input_ids\"].to(device)\n",
        "\t\t\t\tsn_attention_mask_test = batchh[\"sn_attention_mask\"].to(device)\n",
        "\t\t\t\tsp_input_ids_test = batchh[\"sp_input_ids\"].to(device)\n",
        "\t\t\t\tsp_attention_mask_test = batchh[\"sp_attention_mask\"].to(device)\n",
        "\t\t\t\tsp_pos_test = batchh[\"sp_pos\"].to(device) # 28: '<Sep>', 29: '<'Pad'>'\n",
        "\t\t\t\tsp_landing_pos_test = batchh[\"sp_landing_pos\"].to(device)\n",
        "\t\t\t\tsp_fix_dur_test = (batchh[\"sp_fix_dur\"]/1000).to(device)\n",
        "\t\t\t\tsn_word_len_test = batchh[\"sn_word_len\"].to(device)\n",
        "\n",
        "\t\t\t\t#normalize gaze features\n",
        "\t\t\t\tmask = ~torch.eq(sp_fix_dur_test, 0)\n",
        "\t\t\t\tsp_fix_dur_test = (sp_fix_dur_test-fix_dur_mean)/fix_dur_std * mask\n",
        "\t\t\t\tsp_landing_pos_test = (sp_landing_pos_test - landing_pos_mean)/landing_pos_std * mask\n",
        "\t\t\t\tsp_fix_dur_test = torch.nan_to_num(sp_fix_dur_test)\n",
        "\t\t\t\tsp_landing_pos_test = torch.nan_to_num(sp_landing_pos_test)\n",
        "\t\t\t\tsn_word_len_test = (sn_word_len_test - sn_word_len_mean)/sn_word_len_std\n",
        "\t\t\t\tsn_word_len_test = torch.nan_to_num(sn_word_len_test)\n",
        "\n",
        "\t\t\t\tlocation_preds_test, duration_preds_test, landing_pos_preds_test, atten_weights_test = dnn(sn_emd=sn_input_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_mask=sn_attention_mask_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_emd=sp_input_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_pos=sp_pos_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sn=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sp=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_fix_dur=sp_fix_dur_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_landing_pos=sp_landing_pos_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_len = sn_word_len_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_pred = None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_freq = None\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t) #[batch, step, dec_o_dim]\n",
        "\n",
        "\n",
        "\t\t\t\t########## Evaluate location predictions ##########\n",
        "\t\t\t\tm = nn.Softmax(dim=2)\n",
        "\t\t\t\tlocation_preds_test = m(location_preds_test).detach().to('cpu').numpy()\n",
        "\n",
        "\t\t\t\t#prepare label and mask\n",
        "\t\t\t\tpad_mask_test, label_test = load_label(sp_pos_test, cf, le, 'cpu')\n",
        "\t\t\t\t#compute log likelihood for the batch samples\n",
        "\t\t\t\tres_batch = eval_log_llh(location_preds_test, label_test, pad_mask_test)\n",
        "\t\t\t\tres_llh.append(np.array(res_batch))\n",
        "\n",
        "\t\t\t\tuniform_output = construct_uniform_tensor(location_preds_test)\n",
        "\t\t\t\tuniform_nll = eval_log_llh(uniform_output, label_test, pad_mask_test)\n",
        "\t\t\t\tuniform_nll_scores.append(np.array(uniform_nll))\n",
        "\t\t\t\tprint(\"uniform_nll\", np.mean(uniform_nll), uniform_nll)\n",
        "\n",
        "\t\t\t\tprint(\"######### Eyettention 2.0 model evaluation ##########\")\n",
        "\t\t\t\tduration_preds_test = duration_preds_test.squeeze(-1)\n",
        "\t\t\t\tduration_labels_test = sp_fix_dur_test[:, :39]\n",
        "\t\t\t\ttest_mask = mask[:, :39]\n",
        "\t\t\t\tmse_dur = eval_mse(duration_preds_test, duration_labels_test, test_mask)\n",
        "\t\t\t\tprint(\"MSE for durations\", np.mean(mse_dur))\n",
        "\t\t\t\tres_mse_dur.append(np.array(mse_dur))\n",
        "\n",
        "\t\t\t\tlanding_pos_preds_test = landing_pos_preds_test.squeeze(-1)\n",
        "\t\t\t\tlanding_pos_labels_test = sp_landing_pos_test[:, :39]\n",
        "\t\t\t\tmse_landing_pos = eval_mse(landing_pos_preds_test, landing_pos_labels_test, test_mask)\n",
        "\t\t\t\tprint(\"MSE for landing positions\", np.mean(mse_landing_pos))\n",
        "\t\t\t\tres_mse_land_pos.append(np.array(mse_landing_pos))\n",
        "\n",
        "\t\t\t\tif bool(scanpath_gen_flag) == True:\n",
        "\t\t\t\t\tsn_len = (torch.sum(sn_attention_mask_test, axis=1) - 2).detach().to('cpu').numpy()\n",
        "\t\t\t\t\t# compute the scan path generated from the model when the first CLS token is given\n",
        "\t\t\t\t\tsp_dnn, _, dur_dnn, land_pos_dnn = dnn.scanpath_generation(sn_emd=sn_input_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_mask=sn_attention_mask_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t word_ids_sn=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_word_len = sn_word_len_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t le=le,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_word_freq = None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_pred = None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sp_fix_dur=sp_fix_dur_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sp_landing_pos = sp_landing_pos_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t max_pred_len=cf['max_pred_len'])\n",
        "\n",
        "\t\t\t\t\tsp_dnn, sp_human = prepare_scanpath(sp_dnn.detach().to('cpu').numpy(),\n",
        "                                              dur_dnn.detach().to('cpu').numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tland_pos_dnn.detach().to('cpu').numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_len, sp_pos_test,\n",
        "                                              sp_fix_dur_test, sp_landing_pos_test, cf, sn_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfix_dur_mean, fix_dur_std, landing_pos_mean, landing_pos_std)\n",
        "\n",
        "\t\t\t\t\tsp_dnn_list.extend(sp_dnn)\n",
        "\t\t\t\t\tsp_human_list.extend(sp_human)\n",
        "\n",
        "\t\t\t\t\tsp_dnn = convert_sp_to_lists(sp_dnn)\n",
        "\t\t\t\t\tsp_human = convert_sp_to_lists(sp_human)\n",
        "\t\t\t\t\tsp_human = modify_landing_pos(sp_human.copy())\n",
        "\t\t\t\t\tsp_dnn = modify_landing_pos(sp_dnn.copy())\n",
        "\t\t\t\t\trandom_sp = sample_random_sp(\"BSC\", sp_human)\n",
        "\t\t\t\t\trandom_sp = convert_sp_to_lists(random_sp)\n",
        "\t\t\t\t\trandom_sp = modify_landing_pos(random_sp.copy())\n",
        "\n",
        "\t\t\t\t\tscasim_scores_dnn = compute_scasim(sp_dnn, sp_human)\n",
        "\t\t\t\t\tres_scasim_dnn.append(scasim_scores_dnn)\n",
        "\t\t\t\t\tprint(\"Mean scasim dnn\", np.mean(scasim_scores_dnn))\n",
        "\t\t\t\t\tscasim_scores_human = compute_scasim(sp_human, random_sp)\n",
        "\t\t\t\t\tres_scasim_human.append(scasim_scores_human)\n",
        "\t\t\t\t\tprint(\"Mean scasim human\", np.mean(scasim_scores_human))\n",
        "\n",
        "\t\t\t\t\tcentral_scasim_scores_dnn = compute_central_scasim(\"BSC_most_central_sp.txt\", sp_dnn)\n",
        "\t\t\t\t\tcentral_scasim_scores_human = compute_central_scasim(\"BSC_most_central_sp.txt\", sp_human)\n",
        "\t\t\t\t\tres_central_scasim_dnn.append(np.array(central_scasim_scores_dnn))\n",
        "\t\t\t\t\tres_central_scasim_human.append(np.array(central_scasim_scores_human))\n",
        "\t\t\t\t\tprint(\"Mean central scasim dnn\", np.mean(central_scasim_scores_dnn))\n",
        "\t\t\t\t\tprint(\"Mean central scasim human\", np.mean(central_scasim_scores_human))\n",
        "\n",
        "\t\t\t\t\tprint(\"######### Uniform baseline model evaluation ##########\")\n",
        "\t\t\t\t\tmean_dur_uniform, std_dur_uniform, mean_land_pos_uniform, std_land_pos_uniform = compute_mean_std_uniform(\"baseline/uniform/BSC_uniform_results.csv\")\n",
        "\t\t\t\t\tuniform_central_scasim, uniform_scasim, dur_mse_scores, land_pos_mse_scores = evaluate_uniform_model(\"BSC\", sp_human, landing_pos_mean, landing_pos_std, fix_dur_mean, fix_dur_std, mean_land_pos_uniform, std_land_pos_uniform, mean_dur_uniform, std_dur_uniform)\n",
        "\t\t\t\t\tuniform_central_scasim_scores.append(np.array(uniform_central_scasim))\n",
        "\t\t\t\t\tuniform_scasim_scores.append(np.array(uniform_scasim))\n",
        "\t\t\t\t\tuniform_mse_dur_scores.append(np.array(dur_mse_scores))\n",
        "\t\t\t\t\tuniform_mse_land_pos_scores.append(np.array(land_pos_mse_scores))\n",
        "\t\t\t\t\tprint(\"Uniform mean central Scasim score:\", np.mean(uniform_central_scasim))\n",
        "\t\t\t\t\tprint(\"Uniform mean Scasim score:\", np.mean(uniform_scasim))\n",
        "\t\t\t\t\tprint(\"MSE for uniform durations\", np.mean(dur_mse_scores))\n",
        "\t\t\t\t\tprint(\"MSE for uniform landing pos\", np.mean(land_pos_mse_scores))\n",
        "\n",
        "\t\t\t\t\tprint(\"######### E-Z Reader model evaluation ##########\")\n",
        "\t\t\t\t\tmean_dur_ez_reader, std_dur_ez_reader, mean_land_pos_ez_reader, std_land_pos_ez_reader = compute_mean_std_ez_reader(\"baseline/E-Z_Reader/BSCSimulationResults.txt\")\n",
        "\t\t\t\t\tcentral_scasim_ez_reader, scasim_ez_reader, dur_mse_ez_reader, land_pos_mse_ez_reader = evaluate_ez_reader(\"BSC\", \"baseline/E-Z_Reader/BSCSimulationResults.txt\", sp_human, landing_pos_mean, landing_pos_std, fix_dur_mean, fix_dur_std, mean_land_pos_ez_reader, std_land_pos_ez_reader, mean_dur_ez_reader, std_dur_ez_reader)\n",
        "\t\t\t\t\tez_reader_central_scasim_scores.append(np.array(central_scasim_ez_reader))\n",
        "\t\t\t\t\tprint(\"Central scasim E-Z Reader\", np.mean(central_scasim_ez_reader))\n",
        "\t\t\t\t\tez_reader_scasim_scores.append(np.array(scasim_ez_reader))\n",
        "\t\t\t\t\tprint(\"Mean scasim score E-Z Reader\", np.mean(scasim_ez_reader))\n",
        "\t\t\t\t\tez_reader_mse_dur_scores.append(np.array(dur_mse_ez_reader))\n",
        "\t\t\t\t\tprint(\"MSE for durations E-Z Reader\", np.mean(dur_mse_ez_reader))\n",
        "\t\t\t\t\tez_reader_mse_land_pos_scores.append(np.array(land_pos_mse_ez_reader))\n",
        "\t\t\t\t\tprint(\"MSE for landing pos E-Z Reader\", np.mean(land_pos_mse_ez_reader))\n",
        "\n",
        "\t\t\t\tbatch_indx +=1\n",
        "\n",
        "\t\tres_llh = np.concatenate(res_llh).ravel()\n",
        "\t\tloss_dict['test_ll'].append(res_llh)\n",
        "\t\tres_mse_dur = np.concatenate(res_mse_dur).ravel()\n",
        "\t\tloss_dict['test_mse_dur'].append(res_mse_dur)\n",
        "\t\tres_mse_land_pos = np.concatenate(res_mse_land_pos).ravel()\n",
        "\t\tloss_dict['test_mse_land_pos'].append(res_mse_land_pos)\n",
        "\n",
        "\t\tres_central_scasim_dnn = np.concatenate(res_central_scasim_dnn).ravel()\n",
        "\t\tloss_dict['central_scasim_dnn'].append(res_central_scasim_dnn)\n",
        "\t\tres_central_scasim_human = np.concatenate(res_central_scasim_human).ravel()\n",
        "\t\tloss_dict['central_scasim_human'].append(res_central_scasim_human)\n",
        "\t\tres_scasim_dnn = np.concatenate(res_scasim_dnn).ravel()\n",
        "\t\tloss_dict['scasim_dnn'].append(res_scasim_dnn)\n",
        "\t\tres_scasim_human = np.concatenate(res_scasim_human).ravel()\n",
        "\t\tloss_dict['scasim_human'].append(res_scasim_human)\n",
        "\n",
        "\t\tloss_dict['fix_dur_mean'] = fix_dur_mean\n",
        "\t\tloss_dict['fix_dur_std'] = fix_dur_std\n",
        "\t\tloss_dict['landing_pos_mean'] = landing_pos_mean\n",
        "\t\tloss_dict['landing_pos_std'] = landing_pos_std\n",
        "\t\tloss_dict['sn_word_len_mean'] = sn_word_len_mean\n",
        "\t\tloss_dict['sn_word_len_std'] = sn_word_len_std\n",
        "\n",
        "\t\tuniform_central_scasim_scores = np.concatenate(uniform_central_scasim_scores).ravel()\n",
        "\t\tloss_dict['uniform_central_scasim'].append(uniform_central_scasim_scores)\n",
        "\t\tuniform_scasim_scores = np.concatenate(uniform_scasim_scores).ravel()\n",
        "\t\tloss_dict['uniform_scasim'].append(uniform_scasim_scores)\n",
        "\n",
        "\t\tuniform_mse_dur_scores = np.concatenate(uniform_mse_dur_scores).ravel()\n",
        "\t\tloss_dict['uniform_mse_dur'].append(uniform_mse_dur_scores)\n",
        "\t\tuniform_mse_land_pos_scores = np.concatenate(uniform_mse_land_pos_scores).ravel()\n",
        "\t\tloss_dict['uniform_mse_land_pos'].append(uniform_mse_land_pos_scores)\n",
        "\t\tuniform_nll_scores = np.concatenate(uniform_nll_scores).ravel()\n",
        "\t\tloss_dict['uniform_nll'].append(uniform_nll_scores)\n",
        "\n",
        "\t\tez_reader_central_scasim_scores = np.concatenate(ez_reader_central_scasim_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_central_scasim'].append(ez_reader_central_scasim_scores)\n",
        "\t\tez_reader_scasim_scores = np.concatenate(ez_reader_scasim_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_scasim'].append(ez_reader_scasim_scores)\n",
        "\n",
        "\t\tez_reader_mse_dur_scores = np.concatenate(ez_reader_mse_dur_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_mse_dur'].append(ez_reader_mse_dur_scores)\n",
        "\t\tez_reader_mse_land_pos_scores = np.concatenate(ez_reader_mse_land_pos_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_mse_land_pos'].append(ez_reader_mse_land_pos_scores)\n",
        "\n",
        "\t\tprint('Test likelihood is {}'.format(np.mean(res_llh)))\n",
        "\t\tloss_dict['test_ll_SE'].append(np.std(res_llh)/ np.sqrt(len(res_llh)))\n",
        "\t\tprint(\"Standard error for NLL\", np.std(res_llh)/ np.sqrt(len(res_llh)))\n",
        "\n",
        "\t\tprint('Test MSE for durations is {}'.format(np.mean(res_mse_dur)))\n",
        "\t\tloss_dict['test_mse_dur_SE'].append(np.std(res_mse_dur)/ np.sqrt(len(res_mse_dur)))\n",
        "\t\tprint(\"Standard error for MSE dur\", np.std(res_mse_dur) / np.sqrt(len(res_mse_dur)))\n",
        "\n",
        "\t\tprint('Test MSE for landing positions is {}'.format(np.mean(res_mse_land_pos)))\n",
        "\t\tloss_dict['test_mse_land_pos_SE'].append(np.std(res_mse_land_pos)/ np.sqrt(len(res_mse_land_pos)))\n",
        "\t\tprint(\"Standard error for MSE land pos\", np.std(res_mse_land_pos) / np.sqrt(len(res_mse_land_pos)))\n",
        "\n",
        "\t\tprint(\"Central Scasim dnn\", np.mean(loss_dict['central_scasim_dnn']))\n",
        "\t\tloss_dict['central_scasim_dnn_SE'].append(np.std(res_central_scasim_dnn)/ np.sqrt(len(res_central_scasim_dnn)))\n",
        "\t\tprint(\"Standard error for Central scasim DNN\", np.std(res_central_scasim_dnn) / np.sqrt(len(res_central_scasim_dnn)))\n",
        "\n",
        "\t\tprint(\"Central Scasim human\", np.mean(loss_dict['central_scasim_human']))\n",
        "\t\tloss_dict['central_scasim_human_SE'].append(np.std(res_central_scasim_human)/ np.sqrt(len(res_central_scasim_human)))\n",
        "\t\tprint(\"Standard error for Central scasim human\", np.std(res_central_scasim_human) / np.sqrt(len(res_central_scasim_human)))\n",
        "\n",
        "\t\tprint(\"Scasim dnn\", np.mean(loss_dict['scasim_dnn']))\n",
        "\t\tloss_dict['scasim_dnn_SE'].append(np.std(res_scasim_dnn)/ np.sqrt(len(res_scasim_dnn)))\n",
        "\t\tprint(\"Standard error for scasim dnn\", np.std(res_scasim_dnn) / np.sqrt(len(res_scasim_dnn)))\n",
        "\n",
        "\t\tprint(\"Scasim human\", np.mean(loss_dict['scasim_human']))\n",
        "\t\tloss_dict['scasim_human_SE'].append(np.std(res_scasim_human)/ np.sqrt(len(res_scasim_human)))\n",
        "\t\tprint(\"Standard error for scasim human\", np.std(res_scasim_human) / np.sqrt(len(res_scasim_human)))\n",
        "\n",
        "\t\tprint(\"Uniform central scasim\", np.mean(loss_dict['uniform_central_scasim']))\n",
        "\t\tloss_dict['uniform_central_scasim_SE'].append(np.std(uniform_central_scasim_scores)/ np.sqrt(len(uniform_central_scasim_scores)))\n",
        "\t\tprint(\"Standard error for uniform central scasim\", np.std(uniform_central_scasim_scores) / np.sqrt(len(uniform_central_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform scasim\", np.mean(loss_dict['uniform_scasim']))\n",
        "\t\tloss_dict['uniform_scasim_SE'].append(np.std(uniform_scasim_scores)/ np.sqrt(len(uniform_scasim_scores)))\n",
        "\t\tprint(\"Standard error for uniform scasim\", np.std(uniform_scasim_scores) / np.sqrt(len(uniform_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform MSE durations\", np.mean(loss_dict['uniform_mse_dur']))\n",
        "\t\tloss_dict['uniform_mse_dur_SE'].append(np.std(dur_mse_scores)/ np.sqrt(len(dur_mse_scores)))\n",
        "\t\tprint(\"Standard error for uniform MSE durations\", np.std(dur_mse_scores) / np.sqrt(len(dur_mse_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform MSE landing pos\", np.mean(loss_dict['uniform_mse_land_pos']))\n",
        "\t\tloss_dict['uniform_mse_land_pos_SE'].append(np.std(land_pos_mse_scores)/ np.sqrt(len(land_pos_mse_scores)))\n",
        "\t\tprint(\"Standard error for uniform MSE landing pos\", np.std(land_pos_mse_scores) / np.sqrt(len(land_pos_mse_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform NLL\", np.mean(loss_dict['uniform_nll']))\n",
        "\t\tloss_dict['uniform_nll_SE'].append(np.std(uniform_nll_scores)/ np.sqrt(len(uniform_nll_scores)))\n",
        "\t\tprint(\"Standard error for uniform NLL\", np.std(uniform_nll_scores) / np.sqrt(len(uniform_nll_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader central scasim\", np.mean(loss_dict['ez_reader_central_scasim']))\n",
        "\t\tloss_dict['ez_reader_central_scasim_SE'].append(np.std(ez_reader_central_scasim_scores)/ np.sqrt(len(ez_reader_central_scasim_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader central scasim\", np.std(ez_reader_central_scasim_scores) / np.sqrt(len(ez_reader_central_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader scasim\", np.mean(loss_dict['ez_reader_scasim']))\n",
        "\t\tloss_dict['ez_reader_scasim_SE'].append(np.std(ez_reader_scasim_scores)/ np.sqrt(len(ez_reader_scasim_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader scasim\", np.std(ez_reader_scasim_scores) / np.sqrt(len(ez_reader_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader MSE durations\", np.mean(loss_dict['ez_reader_mse_dur']))\n",
        "\t\tloss_dict['ez_reader_mse_dur_SE'].append(np.std(ez_reader_mse_dur_scores)/ np.sqrt(len(ez_reader_mse_dur_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader MSE durations\", np.std(ez_reader_mse_dur_scores) / np.sqrt(len(ez_reader_mse_dur_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader MSE landing pos\", np.mean(loss_dict['ez_reader_mse_land_pos']))\n",
        "\t\tloss_dict['ez_reader_mse_land_pos_SE'].append(np.std(ez_reader_mse_land_pos_scores)/ np.sqrt(len(ez_reader_mse_land_pos_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader MSE landing pos\", np.std(ez_reader_mse_land_pos_scores) / np.sqrt(len(ez_reader_mse_land_pos_scores)))\n",
        "\n",
        "\t\t#save results\n",
        "\t\twith open('{}/res_BSC_{}_eyettention_Fold{}.pickle'.format(save_data_folder, test_mode, fold_indx), 'wb') as handle:\n",
        "\t\t\tpickle.dump(loss_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\t\tfold_indx += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical tests**"
      ],
      "metadata": {
        "id": "93pvVPGoLVc5"
      },
      "id": "93pvVPGoLVc5"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/BSC/New_Reader/res_BSC_subject_eyettention_Fold{fold_index}.pickle', 'rb') as handle:\n",
        "      print(fold_index)\n",
        "      fold_results = pickle.load(handle)\n",
        "\n",
        "      # NLL\n",
        "      test_ll = fold_results['test_ll'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/New_Reader/res_BSC_original_eyettention_fold{fold_index}.pickle', 'rb') as original:\n",
        "        original_results = pickle.load(original)\n",
        "        original_ll = original_results['test_ll'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(test_ll, original_ll)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in NLL scores between Eyettention 2.0 and original Eyettention scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and original Eyettention NLL scores.\")\n",
        "\n",
        "      uniform_nll = fold_results['uniform_nll'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(test_ll, uniform_nll)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in NLL scores between predicted and uniformly generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between the predicted and uniform NLL scores.\")\n",
        "\n",
        "      # Scasim\n",
        "      scasim_dnn = fold_results['scasim_dnn'][0]\n",
        "      scasim_human = fold_results['scasim_human'][0]\n",
        "      ez_reader_scasim = fold_results['ez_reader_scasim'][0]\n",
        "      uniform_scasim = fold_results['uniform_scasim'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(scasim_dnn, ez_reader_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader generated Scasim scores.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(scasim_dnn, uniform_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform Scasim scores.\")\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(scasim_dnn, scasim_human)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the human scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and human Scasim scores.\")\n",
        "\n",
        "      # Central Scasim\n",
        "      central_scasim_dnn = fold_results['central_scasim_dnn'][0]\n",
        "      central_scasim_human = fold_results['central_scasim_human'][0]\n",
        "      ez_reader_central_scasim = fold_results['ez_reader_central_scasim'][0]\n",
        "      uniform_central_scasim = fold_results['uniform_central_scasim'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, ez_reader_central_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader Central Scasim scores.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, uniform_central_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform Central Scasim scores.\")\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, central_scasim_human)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the human scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and human Central Scasim scores.\")\n",
        "\n",
        "      # MSE durations\n",
        "      test_mse_dur = fold_results['test_mse_dur'][0]\n",
        "      ez_reader_mse_dur = fold_results['ez_reader_mse_dur'][0]\n",
        "      uniform_mse_dur = fold_results['uniform_mse_dur'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_dur, ez_reader_mse_dur)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader MSE scores for durations.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_dur, uniform_mse_dur)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform MSE scores for durations.\")\n",
        "\n",
        "      # MSE landing pos\n",
        "      test_mse_land_pos = fold_results['test_mse_land_pos'][0]\n",
        "      ez_reader_mse_land_pos = fold_results['ez_reader_mse_land_pos'][0]\n",
        "      uniform_mse_land_pos = fold_results['uniform_mse_land_pos'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_land_pos, ez_reader_mse_land_pos)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader MSE scores for landing pos.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_land_pos, uniform_mse_land_pos)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform MSE scores for landing pos.\")\n",
        "\n",
        "      fold_index += 1\n"
      ],
      "metadata": {
        "id": "5EuS0Od5cNWk"
      },
      "id": "5EuS0Od5cNWk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical tests for Eyettention Reader model ###"
      ],
      "metadata": {
        "id": "3pkTage1pUEO"
      },
      "id": "3pkTage1pUEO"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/BSC/New_Sentence/res_BSC_text_eyettention_Fold{fold_index}.pickle', 'rb') as handle:\n",
        "      print(fold_index)\n",
        "      fold_results = pickle.load(handle)\n",
        "\n",
        "      # NLL\n",
        "      test_ll = fold_results['test_ll'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "        reader_results = pickle.load(original)\n",
        "        reader_ll = reader_results['test_ll'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(test_ll, reader_ll)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in NLL scores between Eyettention 2.0 and Eyettention Reader scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader NLL scores.\")\n",
        "\n",
        "      # Scasim\n",
        "      scasim_dnn = fold_results['scasim_dnn'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "        reader_results = pickle.load(original)\n",
        "        reader_scasim = reader_results['scasim_dnn'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(scasim_dnn, reader_scasim)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader Scasim scores.\")\n",
        "\n",
        "      # Central Scasim\n",
        "      central_scasim_dnn = fold_results['central_scasim_dnn'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "        reader_results = pickle.load(original)\n",
        "        reader_central_scasim = reader_results['central_scasim_dnn'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, reader_central_scasim)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between Eyettention 2.0 and Eyettention Reader scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader Central Scasim scores.\")\n",
        "\n",
        "\n",
        "      # MSE durations\n",
        "      test_mse_dur = fold_results['test_mse_dur'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "        reader_results = pickle.load(original)\n",
        "        reader_test_mse_dur = reader_results['test_mse_dur'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(test_mse_dur, reader_test_mse_dur)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in MSE scores for dur between Eyettention 2.0 and Eyettention Reader scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader MSE scores for dur.\")\n",
        "\n",
        "\n",
        "      # MSE landing pos\n",
        "      test_mse_land_pos = fold_results['test_mse_land_pos'][0]\n",
        "\n",
        "      with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "        reader_results = pickle.load(original)\n",
        "        reader_mse_land_pos = reader_results['test_mse_land_pos'][0]\n",
        "        t_statistic, p_value = stats.ttest_ind(test_mse_land_pos, reader_mse_land_pos)\n",
        "        # Interpretation based on p-value\n",
        "        if p_value < 0.05:\n",
        "          print(\"Reject null hypothesis: Statistically significant difference in MSE scores for land pos between Eyettention 2.0 and Eyettention Reader scanpaths.\")\n",
        "        else:\n",
        "          print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and Eyettention Reader MSE scores for land pos.\")\n",
        "\n",
        "      fold_index += 1"
      ],
      "metadata": {
        "id": "TTvCduYjotxs"
      },
      "id": "TTvCduYjotxs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "      print(fold_index)\n",
        "      reader_results = pickle.load(original)\n",
        "      print(\"Mean NLL\", np.mean(reader_results['test_ll'][0]))\n",
        "      print(\"Standard error NLL\", np.std(reader_results['test_ll'][0]) / np.sqrt(len(reader_results['test_ll'][0])))\n",
        "\n",
        "      print(\"Mean MSE dur\", np.mean(reader_results['test_mse_dur'][0]))\n",
        "      print(\"Standard error MSE dur\", np.std(reader_results['test_mse_dur'][0]) / np.sqrt(len(reader_results['test_mse_dur'][0])))\n",
        "\n",
        "      print(\"Mean MSE landing pos\", np.mean(reader_results['test_mse_land_pos'][0]))\n",
        "      print(\"Standard error MSE landing pos\", np.std(reader_results['test_mse_land_pos'][0]) / np.sqrt(len(reader_results['test_mse_land_pos'][0])))\n",
        "\n",
        "      print(\"Mean scasim\", np.mean(reader_results['scasim_dnn'][0]))\n",
        "      print(\"Standard error scasim\", np.std(reader_results['scasim_dnn'][0]) / np.sqrt(len(reader_results['scasim_dnn'][0])))\n",
        "\n",
        "      print(\"Mean Central scasim\", np.mean(reader_results['central_scasim_dnn'][0]))\n",
        "      print(\"Standard error Central scasim\", np.std(reader_results['central_scasim_dnn'][0]) / np.sqrt(len(reader_results['central_scasim_dnn'][0])))\n",
        "\n",
        "      fold_index += 1\n"
      ],
      "metadata": {
        "id": "2QoUIf6wLtyb"
      },
      "id": "2QoUIf6wLtyb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Analysing the results\n",
        "test_ll_all_folds = []\n",
        "test_mse_dur_all_folds = []\n",
        "test_mse_land_pos_all_folds = []\n",
        "scasim_dnn_all_folds = []\n",
        "scasim_human_all_folds = []\n",
        "central_scasim_dnn_all_folds = []\n",
        "central_scasim_human_all_folds = []\n",
        "\n",
        "uniform_ll_all_folds = []\n",
        "uniform_mse_dur_all_folds = []\n",
        "uniform_mse_land_pos_all_folds = []\n",
        "uniform_scasim_all_folds = []\n",
        "uniform_central_scasim_all_folds = []\n",
        "\n",
        "ez_reader_mse_dur_all_folds = []\n",
        "ez_reader_mse_land_pos_all_folds = []\n",
        "ez_reader_scasim_all_folds = []\n",
        "ez_reader_central_scasim_all_folds = []\n",
        "\n",
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/BSC/NRS/res_BSC_eyettention_NRS_Fold{fold_index}.pickle', 'rb') as original:\n",
        "      reader_results = pickle.load(original)\n",
        "      test_ll_all_folds.extend(reader_results['test_ll'][0])\n",
        "      test_mse_dur_all_folds.extend(reader_results['test_mse_dur'][0])\n",
        "      test_mse_land_pos_all_folds.extend(reader_results['test_mse_land_pos'][0])\n",
        "      scasim_dnn_all_folds.extend(reader_results['scasim_dnn'][0])\n",
        "      scasim_human_all_folds.extend(reader_results['scasim_human'][0])\n",
        "      central_scasim_dnn_all_folds.extend(reader_results['central_scasim_dnn'][0])\n",
        "      central_scasim_human_all_folds.extend(reader_results['central_scasim_human'][0])\n",
        "\n",
        "      uniform_ll_all_folds.extend(reader_results['uniform_nll'][0])\n",
        "      uniform_mse_dur_all_folds.extend(reader_results['uniform_mse_dur'][0])\n",
        "      uniform_mse_land_pos_all_folds.extend(reader_results['uniform_mse_land_pos'][0])\n",
        "      uniform_scasim_all_folds.extend(reader_results['uniform_scasim'][0])\n",
        "      uniform_central_scasim_all_folds.extend(reader_results['uniform_central_scasim'][0])\n",
        "\n",
        "      ez_reader_mse_dur_all_folds.extend(reader_results['ez_reader_mse_dur'][0])\n",
        "      ez_reader_mse_land_pos_all_folds.extend(reader_results['ez_reader_mse_land_pos'][0])\n",
        "      ez_reader_scasim_all_folds.extend(reader_results['ez_reader_scasim'][0])\n",
        "      ez_reader_central_scasim_all_folds.extend(reader_results['ez_reader_central_scasim'][0])\n",
        "\n",
        "      fold_index += 1\n",
        "\n",
        "# Calculate overall statistics\n",
        "def calculate_mean_and_se(values):\n",
        "    mean_val = np.mean(values)\n",
        "    se_val = np.std(values) / np.sqrt(len(values))\n",
        "    return mean_val, se_val\n",
        "\n",
        "mean_nll, se_nll = calculate_mean_and_se(test_ll_all_folds)\n",
        "mean_mse_dur, se_mse_dur = calculate_mean_and_se(test_mse_dur_all_folds)\n",
        "mean_mse_land_pos, se_mse_land_pos = calculate_mean_and_se(test_mse_land_pos_all_folds)\n",
        "mean_scasim, se_scasim = calculate_mean_and_se(scasim_dnn_all_folds)\n",
        "mean_central_scasim, se_central_scasim = calculate_mean_and_se(central_scasim_dnn_all_folds)\n",
        "mean_scasim_human, se_scasim_human = calculate_mean_and_se(scasim_human_all_folds)\n",
        "mean_central_scasim_human, se_central_scasim_human = calculate_mean_and_se(central_scasim_human_all_folds)\n",
        "\n",
        "mean_nll_uniform, se_nll_uniform = calculate_mean_and_se(uniform_ll_all_folds)\n",
        "mean_mse_dur_uniform, se_mse_dur_uniform = calculate_mean_and_se(uniform_mse_dur_all_folds)\n",
        "mean_mse_land_pos_uniform, se_mse_land_pos_uniform = calculate_mean_and_se(uniform_mse_land_pos_all_folds)\n",
        "mean_scasim_uniform, se_scasim_uniform = calculate_mean_and_se(uniform_scasim_all_folds)\n",
        "mean_central_scasim_uniform, se_central_scasim_uniform = calculate_mean_and_se(uniform_central_scasim_all_folds)\n",
        "\n",
        "mean_nll_ez_reader, se_nll_ez_reader = calculate_mean_and_se(ez_reader_mse_dur_all_folds)\n",
        "mean_mse_dur_ez_reader, se_mse_dur_ez_reader = calculate_mean_and_se(ez_reader_mse_dur_all_folds)\n",
        "mean_mse_land_pos_ez_reader, se_mse_land_pos_ez_reader = calculate_mean_and_se(ez_reader_mse_land_pos_all_folds)\n",
        "mean_scasim_ez_reader, se_scasim_ez_reader = calculate_mean_and_se(ez_reader_scasim_all_folds)\n",
        "mean_central_scasim_ez_reader, se_central_scasim_ez_reader = calculate_mean_and_se(ez_reader_central_scasim_all_folds)\n",
        "\n",
        "\n",
        "print(\"Overall Mean NLL:\", mean_nll)\n",
        "print(\"Overall Standard error NLL:\", se_nll)\n",
        "\n",
        "print(\"Overall Mean MSE dur:\", mean_mse_dur)\n",
        "print(\"Overall Standard error MSE dur:\", se_mse_dur)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos:\", mean_mse_land_pos)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos)\n",
        "\n",
        "print(\"Overall Mean scasim:\", mean_scasim)\n",
        "print(\"Overall Standard error scasim:\", se_scasim)\n",
        "\n",
        "print(\"Overall Mean Central scasim:\", mean_central_scasim)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim)\n",
        "\n",
        "print(\"Human Mean scasim:\", mean_scasim_human)\n",
        "print(\"Human Standard error scasim:\", se_scasim_human)\n",
        "\n",
        "print(\"Human Mean central scasim:\", mean_central_scasim_human)\n",
        "print(\"Human Standard error central scasim:\", se_central_scasim_human)\n",
        "\n",
        "\n",
        "print(\"Overall Mean NLL Uniform:\", mean_nll_uniform)\n",
        "print(\"Overall Standard error NLL:\", se_nll_uniform)\n",
        "\n",
        "print(\"Overall Mean MSE dur Uniform:\", mean_mse_dur_uniform)\n",
        "print(\"Overall Standard error MSE dur Uniform:\", se_mse_dur_uniform)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos Uniform:\", mean_mse_land_pos_uniform)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos_uniform)\n",
        "\n",
        "print(\"Overall Mean scasim Uniform:\", mean_scasim_uniform)\n",
        "print(\"Overall Standard error scasim:\", se_scasim_uniform)\n",
        "\n",
        "print(\"Overall Mean Central scasim Uniform:\", mean_central_scasim_uniform)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim_uniform)\n",
        "\n",
        "\n",
        "print(\"Overall Mean MSE dur E-Z Reader:\", mean_mse_dur_ez_reader)\n",
        "print(\"Overall Standard error MSE dur E-Z Reader:\", se_mse_dur_ez_reader)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos E-Z Reader:\", mean_mse_land_pos_ez_reader)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos_ez_reader)\n",
        "\n",
        "print(\"Overall Mean scasim E-Z Reader:\", mean_scasim_ez_reader)\n",
        "print(\"Overall Standard error scasim:\", se_scasim_ez_reader)\n",
        "\n",
        "print(\"Overall Mean Central scasim E-Z Reader:\", mean_central_scasim_ez_reader)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim_ez_reader)\n"
      ],
      "metadata": {
        "id": "zp6ZOF4LN1fa"
      },
      "id": "zp6ZOF4LN1fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Analysing the results\n",
        "test_ll_all_folds = []\n",
        "test_mse_dur_all_folds = []\n",
        "test_mse_land_pos_all_folds = []\n",
        "scasim_dnn_all_folds = []\n",
        "scasim_human_all_folds = []\n",
        "central_scasim_dnn_all_folds = []\n",
        "central_scasim_human_all_folds = []\n",
        "\n",
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/BSC/Eyettention_Reader/emb_size_64/res_BSC_eyettention_reader_Fold{fold_index}.pickle', 'rb') as original:\n",
        "      reader_results = pickle.load(original)\n",
        "      test_ll_all_folds.extend(reader_results['test_ll'][0])\n",
        "      test_mse_dur_all_folds.extend(reader_results['test_mse_dur'][0])\n",
        "      test_mse_land_pos_all_folds.extend(reader_results['test_mse_land_pos'][0])\n",
        "      scasim_dnn_all_folds.extend(reader_results['scasim_dnn'][0])\n",
        "      scasim_human_all_folds.extend(reader_results['scasim_human'][0])\n",
        "      central_scasim_dnn_all_folds.extend(reader_results['central_scasim_dnn'][0])\n",
        "      central_scasim_human_all_folds.extend(reader_results['central_scasim_human'][0])\n",
        "\n",
        "      fold_index += 1\n",
        "\n",
        "# Calculate overall statistics\n",
        "def calculate_mean_and_se(values):\n",
        "    mean_val = np.mean(values)\n",
        "    se_val = np.std(values) / np.sqrt(len(values))\n",
        "    return mean_val, se_val\n",
        "\n",
        "mean_nll, se_nll = calculate_mean_and_se(test_ll_all_folds)\n",
        "mean_mse_dur, se_mse_dur = calculate_mean_and_se(test_mse_dur_all_folds)\n",
        "mean_mse_land_pos, se_mse_land_pos = calculate_mean_and_se(test_mse_land_pos_all_folds)\n",
        "mean_scasim, se_scasim = calculate_mean_and_se(scasim_dnn_all_folds)\n",
        "mean_central_scasim, se_central_scasim = calculate_mean_and_se(central_scasim_dnn_all_folds)\n",
        "mean_scasim_human, se_scasim_human = calculate_mean_and_se(scasim_human_all_folds)\n",
        "mean_central_scasim_human, se_central_scasim_human = calculate_mean_and_se(central_scasim_human_all_folds)\n",
        "\n",
        "\n",
        "print(\"Overall Mean NLL:\", mean_nll)\n",
        "print(\"Overall Standard error NLL:\", se_nll)\n",
        "\n",
        "print(\"Overall Mean MSE dur:\", mean_mse_dur)\n",
        "print(\"Overall Standard error MSE dur:\", se_mse_dur)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos:\", mean_mse_land_pos)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos)\n",
        "\n",
        "print(\"Overall Mean scasim:\", mean_scasim)\n",
        "print(\"Overall Standard error scasim:\", se_scasim)\n",
        "\n",
        "print(\"Overall Mean Central scasim:\", mean_central_scasim)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim)\n",
        "\n",
        "print(\"Human Mean scasim:\", mean_scasim_human)\n",
        "print(\"Human Standard error scasim:\", se_scasim_human)\n",
        "\n",
        "print(\"Human Mean central scasim:\", mean_central_scasim_human)\n",
        "print(\"Human Standard error central scasim:\", se_central_scasim_human)"
      ],
      "metadata": {
        "id": "e8TKfhG_8Nb6"
      },
      "id": "e8TKfhG_8Nb6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}