{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "nuclear-dream",
      "metadata": {
        "id": "nuclear-dream"
      },
      "source": [
        "# Eyettention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0978c758-ea14-4df4-97d0-6e0c1ed18428",
      "metadata": {
        "id": "0978c758-ea14-4df4-97d0-6e0c1ed18428"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import model\n",
        "import torch\n",
        "from torch.utils import model_zoo\n",
        "import pandas as pd\n",
        "from utils import *\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, RMSprop\n",
        "from transformers import BertTokenizerFast\n",
        "from model import Eyettention\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from torch.nn.functional import cross_entropy, softmax\n",
        "from collections import deque, Counter\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import random\n",
        "from scasim import *\n",
        "from evaluate_e_z_reader_model import *\n",
        "from uniform_model import *\n",
        "from evaluate_swift import *\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8dfb0f8-8775-4609-b75c-6052916ca9c0",
      "metadata": {
        "id": "c8dfb0f8-8775-4609-b75c-6052916ca9c0"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "save_data_folder = \"./drive/MyDrive/results/celer/NRS\"\n",
        "#DEVICE = 'cuda'\n",
        "DEVICE = 'cpu'\n",
        "scanpath_gen_flag = True\n",
        "atten_type = \"local_g\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4l7DoNdYwUn9",
      "metadata": {
        "id": "4l7DoNdYwUn9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e085041-7c16-44b4-9129-bf667f64c552",
      "metadata": {
        "id": "6e085041-7c16-44b4-9129-bf667f64c552"
      },
      "source": [
        "**Training loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28WYhnrePEZj",
      "metadata": {
        "id": "28WYhnrePEZj"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\tgpu = 0\n",
        "\n",
        "\ttorch.set_default_tensor_type('torch.FloatTensor')\n",
        "\tavailbl = torch.cuda.is_available()\n",
        "\tif availbl:\n",
        "\t\tdevice = f'cuda:{gpu}'\n",
        "\telse:\n",
        "\t\tdevice = 'cpu'\n",
        "\t#torch.cuda.set_device(gpu)\n",
        "\n",
        "\tcf = {\"model_pretrained\": \"bert-base-cased\",\n",
        "\t\t\t\"lr\": 1e-3,\n",
        "\t\t\t\"max_grad_norm\": 10,\n",
        "\t\t\t\"n_epochs\": 170,  # 1000\n",
        "\t\t\t\"n_folds\": 5,\n",
        "\t\t\t\"dataset\": 'celer',\n",
        "\t\t\t\"atten_type\": 'local-g',\n",
        "\t\t\t\"batch_size\": 256,\n",
        "\t\t\t\"max_sn_len\": 24, #include start token and end token\n",
        "\t\t\t\"max_sn_token\": 35,\n",
        "\t\t\t\"max_sp_len\": 52, #include start token and end token\n",
        "\t\t\t\"max_sp_token\": 395,\n",
        "\t\t\t\"norm_type\": \"z-score\",\n",
        "\t\t\t\"earlystop_patience\": 20,\n",
        "\t\t\t\"max_pred_len\": 60\n",
        "\t\t\t}\n",
        "\n",
        "\t#Encode the label into interger categories, setting the exclusive category 'cf[\"max_sn_len\"]-1' as the end sign\n",
        "\tle = LabelEncoder()\n",
        "\tle.fit(np.append(np.arange(-cf[\"max_sn_len\"]+3, cf[\"max_sn_len\"]-1), cf[\"max_sn_len\"]-1))\n",
        "\t#le.classes_\n",
        "\n",
        "\t#load corpus\n",
        "\tword_info_df, _, eyemovement_df = load_corpus(cf[\"dataset\"])\n",
        "\n",
        "\treader_list = celer_load_native_speaker()\n",
        "\t#Make list with sentence index\n",
        "\tsn_list = np.unique(word_info_df[word_info_df['list'].isin(reader_list)].sentenceid.values).tolist()\n",
        "\n",
        "\tsent_dict = convert_sent_id(sn_list)\n",
        "\n",
        "\trandom.seed(0)\n",
        "\tfold_indx = 0\n",
        "\t#for scanpath generation\n",
        "\tsp_dnn_list = []\n",
        "\tsp_human_list = []\n",
        "\tfor i in range(5):\n",
        "\t\tprint('Sampling time:', i)\n",
        "\t\tloss_dict = {'val_loss':[], 'train_loss':[], 'test_ll':[], 'test_ll_SE':[], 'test_mse_dur':[], 'test_mse_dur_SE':[], 'test_mse_land_pos':[], 'test_mse_land_pos_SE':[], 'central_scasim_dnn':[], 'central_scasim_dnn_SE':[], 'central_scasim_human':[], 'central_scasim_human_SE':[], 'scasim_dnn':[], 'scasim_dnn_SE':[], 'scasim_human':[], 'scasim_human_SE':[], 'uniform_scasim':[], 'uniform_scasim_SE':[], 'uniform_central_scasim':[], 'uniform_central_scasim_SE':[], 'uniform_nll_SE':[], 'uniform_nll':[], 'uniform_mse_dur_SE':[], 'uniform_mse_dur':[], 'uniform_mse_land_pos_SE':[], 'uniform_mse_land_pos':[],\n",
        "\t\t             'ez_reader_scasim':[], 'ez_reader_scasim_SE':[], 'ez_reader_central_scasim':[], 'ez_reader_central_scasim_SE':[], 'ez_reader_mse_dur_SE':[], 'ez_reader_mse_dur':[], 'ez_reader_mse_land_pos_SE':[], 'ez_reader_mse_land_pos':[], 'SWIFT_scasim':[], 'SWIFT_scasim_SE':[], 'SWIFT_central_scasim':[], 'SWIFT_central_scasim_SE':[], 'SWIFT_nll_SE':[], 'SWIFT_nll':[], 'SWIFT_mse_dur_SE':[], 'SWIFT_mse_dur':[], 'SWIFT_mse_land_pos_SE':[], 'SWIFT_mse_land_pos':[]}\n",
        "\t\treaders_test = random.sample(reader_list, int(np.ceil(len(reader_list)*0.3)))\n",
        "\t\treader_list_val = random.sample(readers_test, int(np.ceil(len(readers_test)*0.3)))\n",
        "\t\treader_list_test = set(readers_test) - set(reader_list_val)\n",
        "\t\treader_list_test = list(reader_list_test)\n",
        "\t\treader_list_train = set(reader_list) - set(reader_list_test) - set(reader_list_val)\n",
        "\t\treader_list_train = list(reader_list_train)\n",
        "\t\t#Make the order of the test samples unchanged on each re-run\n",
        "\t\treader_list_test.sort()\n",
        "\n",
        "\t\tsn_test = random.sample(sn_list, int(np.ceil(len(sn_list)*0.3)))\n",
        "\t\tsn_list_val = random.sample(sn_test, int(np.ceil(len(sn_test)*0.3)))\n",
        "\t\tsn_list_test = set(sn_test) - set(sn_list_val)\n",
        "\t\tsn_list_test = list(sn_list_test)\n",
        "\t\tsn_list_train = set(sn_list) - set(sn_list_test) - set(sn_list_val)\n",
        "\t\tsn_list_train = list(sn_list_train)\n",
        "\t\t#Make the order of the test samples unchanged on each re-run\n",
        "\t\tsn_list_test.sort()\n",
        "\n",
        "\t\t#initialize tokenizer\n",
        "\t\ttokenizer = BertTokenizerFast.from_pretrained(cf['model_pretrained'])\n",
        "\n",
        "\t\tfiltered_sent_dict_train = get_relevant_sent_dict(sent_dict, sn_list_train)\n",
        "\t\tfiltered_sent_dict_val = get_relevant_sent_dict(sent_dict, sn_list_val)\n",
        "\t\tfiltered_sent_dict_test = get_relevant_sent_dict(sent_dict, sn_list_test)\n",
        "\n",
        "\t\t#Preparing batch data\n",
        "\t\tdataset_train = celerdataset(word_info_df, eyemovement_df, cf, reader_list_train, sn_list_train, tokenizer, filtered_sent_dict_train)\n",
        "\t\ttrain_dataloaderr = DataLoader(dataset_train, batch_size = cf[\"batch_size\"], shuffle = True, drop_last=True)\n",
        "\n",
        "\t\tdataset_val = celerdataset(word_info_df, eyemovement_df, cf, reader_list_val, sn_list_val, tokenizer, filtered_sent_dict_val)\n",
        "\t\tval_dataloaderr = DataLoader(dataset_val, batch_size = cf[\"batch_size\"], shuffle = False, drop_last=True)\n",
        "\n",
        "\t\tdataset_test = celerdataset(word_info_df, eyemovement_df, cf, reader_list_test, sn_list_test, tokenizer, filtered_sent_dict_test)\n",
        "\t\ttest_dataloaderr = DataLoader(dataset_test, batch_size = cf[\"batch_size\"], shuffle = False, drop_last=False)\n",
        "\n",
        "\t\t#z-score normalization for gaze features\n",
        "\t\tfix_dur_mean, fix_dur_std = calculate_mean_std(dataloader=train_dataloaderr, feat_key=\"sp_fix_dur\", padding_value=0, scale=1000)\n",
        "\t\tlanding_pos_mean, landing_pos_std = calculate_mean_std(dataloader=train_dataloaderr, feat_key=\"sp_landing_pos\", padding_value=0)\n",
        "\t\tsn_word_len_mean, sn_word_len_std = calculate_mean_std(dataloader=train_dataloaderr, feat_key=\"sn_word_len\")\n",
        "\n",
        "\t\t# load model\n",
        "\t\tdnn = Eyettention(cf)\n",
        "\t\t#training\n",
        "\t\tepisode = 0\n",
        "\t\toptimizer = Adam(dnn.parameters(), lr=cf[\"lr\"])\n",
        "\t\tdnn.train()\n",
        "\t\tdnn.to(device)\n",
        "\t\tav_score = deque(maxlen=100)\n",
        "\t\tav_location_score = deque(maxlen=100)\n",
        "\t\tav_duration_score = deque(maxlen=100)\n",
        "\t\tav_land_pos_score = deque(maxlen=100)\n",
        "\t\told_score = 1e10\n",
        "\t\tsave_ep_couter = 0\n",
        "\t\tprint('Start training')\n",
        "\t\tfor episode_i in range(episode, cf[\"n_epochs\"]+1):\n",
        "\t\t\tdnn.train()\n",
        "\t\t\tprint('episode:', episode_i)\n",
        "\t\t\tcounter = 0\n",
        "\t\t\tfor batchh in train_dataloaderr:\n",
        "\t\t\t\tcounter += 1\n",
        "\t\t\t\tbatchh.keys()\n",
        "\t\t\t\tsn_ids = batchh[\"sn_ids\"].to(device)\n",
        "\t\t\t\tsn_input_ids = batchh[\"sn_input_ids\"].to(device)\n",
        "\t\t\t\tsn_attention_mask = batchh[\"sn_attention_mask\"].to(device)\n",
        "\t\t\t\tword_ids_sn = batchh[\"word_ids_sn\"].to(device)\n",
        "\t\t\t\tsn_word_len = batchh[\"sn_word_len\"].to(device)\n",
        "\n",
        "\t\t\t\tsp_input_ids = batchh[\"sp_input_ids\"].to(device)\n",
        "\t\t\t\tsp_attention_mask = batchh[\"sp_attention_mask\"].to(device)\n",
        "\t\t\t\tword_ids_sp = batchh[\"word_ids_sp\"].to(device)\n",
        "\n",
        "\t\t\t\tsp_pos = batchh[\"sp_pos\"].to(device)\n",
        "\t\t\t\tsp_landing_pos = batchh[\"sp_landing_pos\"].to(device) # [256, 40]\n",
        "\t\t\t\tsp_fix_dur = (batchh[\"sp_fix_dur\"]/1000).to(device) # [256, 40]\n",
        "\n",
        "\t\t\t\t# normalize gaze features (z-score normalisation)\n",
        "\t\t\t\tmask = ~torch.eq(sp_fix_dur, 0)\n",
        "\t\t\t\tsp_fix_dur = (sp_fix_dur-fix_dur_mean)/fix_dur_std * mask\n",
        "\t\t\t\tsp_fix_dur = torch.nan_to_num(sp_fix_dur) # [256, 40]\n",
        "\t\t\t\tsp_landing_pos = (sp_landing_pos - landing_pos_mean)/landing_pos_std * mask\n",
        "\t\t\t\tsp_landing_pos = torch.nan_to_num(sp_landing_pos)\n",
        "\t\t\t\tsn_word_len = (sn_word_len - sn_word_len_mean)/sn_word_len_std\n",
        "\t\t\t\tsn_word_len = torch.nan_to_num(sn_word_len)\n",
        "\t\t\t\tsub_id = batchh[\"sub_id\"].to(device)\n",
        "\n",
        "\t\t\t\t# zero old gradients\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\t# predict output with DNN\n",
        "\t\t\t\tlocation_preds, duration_preds, landing_pos_preds, atten_weights = dnn(sn_emd=sn_input_ids,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_mask=sn_attention_mask,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_emd=sp_input_ids,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_pos=sp_pos,\n",
        "\t\t\t\t\t\t\t\t\t\t\tword_ids_sn=word_ids_sn,\n",
        "\t\t\t\t\t\t\t\t\t\t\tword_ids_sp=word_ids_sp,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_fix_dur=sp_fix_dur,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsp_landing_pos=sp_landing_pos,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_word_len = sn_word_len,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_word_freq= None,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsn_pred = None)#[batch, step, dec_o_dim]\n",
        "\n",
        "\t\t\t\tlocation_preds = location_preds.permute(0,2,1)              #[batch, dec_o_dim, step]\n",
        "\n",
        "\t\t\t\t#prepare label and mask\n",
        "\t\t\t\t# Compute loss for fixation locations\n",
        "\t\t\t\tpad_mask, label = load_label(sp_pos, cf, le, device)\n",
        "\t\t\t\tloss = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\t\t\t\tbatch_location_error = torch.mean(torch.masked_select(loss(location_preds, label), ~pad_mask))\n",
        "\n",
        "\t\t\t\t# Compute loss for fixation durations\n",
        "\t\t\t\tduration_labels = sp_fix_dur[:, :51] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\tduration_preds = duration_preds.squeeze(-1)  # Remove extra dimension (from [256, 39, 1] to [256, 39])\n",
        "\t\t\t\tdur_loss = nn.MSELoss(reduction=\"none\")\n",
        "\t\t\t\tbatch_duration_error = torch.mean(dur_loss(duration_preds, duration_labels))\n",
        "\n",
        "\t\t\t\t# Compute loss for landing position\n",
        "\t\t\t\tlanding_pos_labels = sp_landing_pos[:, :51] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\tlanding_pos_preds = landing_pos_preds.squeeze(-1)  # Remove extra dimension (from [256, 39, 1] to [256, 39])\n",
        "\t\t\t\tland_pos_loss = nn.MSELoss(reduction=\"none\")\n",
        "\t\t\t\tbatch_land_pos_error = torch.mean(land_pos_loss(landing_pos_preds, landing_pos_labels))\n",
        "\n",
        "\t\t\t\t# Combined loss for both location and duration\n",
        "\t\t\t\tbatch_error = batch_location_error +  batch_duration_error + batch_land_pos_error\n",
        "\n",
        "\t\t\t\t# backpropagate loss\n",
        "\t\t\t\tbatch_error.backward()\n",
        "\t\t\t\t# clip gradients\n",
        "\t\t\t\tgradient_clipping(dnn, cf[\"max_grad_norm\"])\n",
        "\n",
        "\t\t\t\t#learn\n",
        "\t\t\t\toptimizer.step()\n",
        "\t\t\t\tav_location_score.append(batch_location_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tav_duration_score.append(batch_duration_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tav_land_pos_score.append(batch_land_pos_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tav_score.append(batch_error.to('cpu').detach().numpy())\n",
        "\t\t\t\tprint('counter:',counter)\n",
        "\t\t\t\tprint('\\rSample {}\\tLocation Loss: {:.10f}\\tDuration Loss: {:.10f}\\tLanding position Loss: {:.10f}'.format(\n",
        "          \tcounter, np.mean(av_location_score), np.mean(av_duration_score), np.mean(av_land_pos_score)), end=\" \")\n",
        "\t\t\tloss_dict['train_loss'].append(np.mean(av_score))\n",
        "\t\t\tif np.mean(av_score) < old_score:\n",
        "\t\t\t\t# save model if val loss is smallest\n",
        "\t\t\t\ttorch.save(dnn.state_dict(), '{}/CELER_3head_arch_NRS_{}.pth'.format(save_data_folder, fold_indx))\n",
        "\t\t\t\told_score = np.mean(av_score)\n",
        "\t\t\t\tprint('\\nsaved model state dict\\n')\n",
        "\t\t\t\tsave_ep_couter = episode_i\n",
        "\t\t\telse:\n",
        "\t\t\t\t#early stopping\n",
        "\t\t\t\tif episode_i - save_ep_couter >= cf[\"earlystop_patience\"]:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\tlocation_val_loss = []\n",
        "\t\t\tduration_val_loss = []\n",
        "\t\t\tland_pos_val_loss = []\n",
        "\t\t\tval_loss = []\n",
        "\t\t\tdnn.eval()\n",
        "\t\t\tfor batchh in val_dataloaderr:\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tsn_ids_val = batchh[\"sn_ids\"].to(device)\n",
        "\t\t\t\t\tsn_input_ids_val = batchh[\"sn_input_ids\"].to(device)\n",
        "\t\t\t\t\tsn_attention_mask_val = batchh[\"sn_attention_mask\"].to(device)\n",
        "\t\t\t\t\tword_ids_sn_val = batchh[\"word_ids_sn\"].to(device)\n",
        "\t\t\t\t\tsn_word_len_val = batchh[\"sn_word_len\"].to(device)\n",
        "\n",
        "\t\t\t\t\tsp_input_ids_val = batchh[\"sp_input_ids\"].to(device)\n",
        "\t\t\t\t\tsp_attention_mask_val = batchh[\"sp_attention_mask\"].to(device)\n",
        "\t\t\t\t\tword_ids_sp_val = batchh[\"word_ids_sp\"].to(device)\n",
        "\n",
        "\t\t\t\t\tsp_pos_val = batchh[\"sp_pos\"].to(device)\n",
        "\t\t\t\t\tsp_landing_pos_val = batchh[\"sp_landing_pos\"].to(device)\n",
        "\t\t\t\t\tsp_fix_dur_val = (batchh[\"sp_fix_dur\"]/1000).to(device)\n",
        "\n",
        "\t\t\t\t\t#normalize gaze features\n",
        "\t\t\t\t\tmask = ~torch.eq(sp_fix_dur_val, 0)\n",
        "\t\t\t\t\tsp_fix_dur_val = (sp_fix_dur_val-fix_dur_mean)/fix_dur_std * mask\n",
        "\t\t\t\t\tsp_landing_pos_val = (sp_landing_pos_val - landing_pos_mean)/landing_pos_std * mask\n",
        "\t\t\t\t\tsp_fix_dur_val = torch.nan_to_num(sp_fix_dur_val)\n",
        "\t\t\t\t\tsp_landing_pos_val = torch.nan_to_num(sp_landing_pos_val)\n",
        "\t\t\t\t\tsn_word_len_val = (sn_word_len_val - sn_word_len_mean)/sn_word_len_std\n",
        "\t\t\t\t\tsn_word_len_val = torch.nan_to_num(sn_word_len_val)\n",
        "\t\t\t\t\tsub_id_val = batchh[\"sub_id\"].to(device)\n",
        "\n",
        "\t\t\t\t\tlocation_preds_val, duration_preds_val, landing_pos_preds_val, atten_weights_val = dnn(sn_emd=sn_input_ids_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_mask=sn_attention_mask_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_emd=sp_input_ids_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_pos=sp_pos_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sn=word_ids_sn_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sp=word_ids_sp_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_fix_dur=sp_fix_dur_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_landing_pos=sp_landing_pos_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_len = sn_word_len_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_freq= None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t      sn_pred = None)#[batch, step, dec_o_dim]\n",
        "\t\t\t\t\tlocation_preds_val = location_preds_val.permute(0,2,1)              #[batch, dec_o_dim, step\n",
        "\n",
        "\t\t\t\t\t# Compute location prediction error\n",
        "\t\t\t\t\tloss = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\t\t\t\t\tpad_mask_val, label_val = load_label(sp_pos_val, cf, le, device)\n",
        "\t\t\t\t\tlocation_error_val = torch.mean(torch.masked_select(loss(location_preds_val, label_val), ~pad_mask_val))\n",
        "\t\t\t\t\tlocation_val_loss.append(location_error_val.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\t\t\t# Compute duration prediction error\n",
        "\t\t\t\t\tduration_labels_val = sp_fix_dur_val[:, :51] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\t\tduration_preds_val = duration_preds_val.squeeze(-1)\n",
        "\t\t\t\t\tdur_loss = nn.MSELoss(reduction=\"none\")\n",
        "\t\t\t\t\tduration_error_val = torch.mean(dur_loss(duration_preds_val, duration_labels_val))\n",
        "\t\t\t\t\tduration_val_loss.append(duration_error_val.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\t\t\t# Compute loss for landing position\n",
        "\t\t\t\t\tlanding_pos_labels_val = sp_landing_pos_val[:, :51] # Adjust duration_labels to match the sequence length of duration_preds\n",
        "\t\t\t\t\tlanding_pos_preds_val = landing_pos_preds_val.squeeze(-1)  # Remove extra dimension (from [256, 39, 1] to [256, 39])\n",
        "\t\t\t\t\tland_pos_error_val = torch.mean(land_pos_loss(landing_pos_preds_val, landing_pos_labels_val))\n",
        "\t\t\t\t\tland_pos_val_loss.append(land_pos_error_val.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\t\t\tcombined_loss = location_error_val + duration_error_val + land_pos_error_val\n",
        "\t\t\t\t\tval_loss.append(combined_loss.detach().to('cpu').numpy())\n",
        "\n",
        "\t\t\tprint('\\nValidation loss for locations {} \\n'.format(np.mean(location_val_loss)))\n",
        "\t\t\tprint('\\nValidation loss for duration {} \\n'.format(np.mean(duration_val_loss)))\n",
        "\t\t\tprint('\\nValidation loss for landing position {} \\n'.format(np.mean(land_pos_val_loss)))\n",
        "\t\t\tloss_dict['val_loss'].append(np.mean(val_loss))\n",
        "\n",
        "\t\t\tif np.mean(val_loss) < old_score:\n",
        "\t\t\t\t# save model if val loss is smallest\n",
        "\t\t\t\ttorch.save(dnn.state_dict(), '{}/CELER_3head_arch_NRS_{}.pth'.format(save_data_folder, fold_indx))\n",
        "\t\t\t\told_score = np.mean(val_loss)\n",
        "\t\t\t\tprint('\\nsaved model state dict\\n')\n",
        "\t\t\t\tsave_ep_couter = episode_i\n",
        "\t\t\telse:\n",
        "\t\t\t\t#early stopping\n",
        "\t\t\t\tif episode_i - save_ep_couter >= cf[\"earlystop_patience\"]:\n",
        "\t\t\t\t\tbreak\n",
        "\t\tfold_indx += 1\n",
        "\n",
        "\t\t#evaluation\n",
        "\t\tdnn.eval()\n",
        "\t\tres_llh=[]\n",
        "\t\tres_mse_dur = []\n",
        "\t\tres_mse_land_pos = []\n",
        "\t\tres_central_scasim_human = []\n",
        "\t\tres_central_scasim_dnn = []\n",
        "\t\tres_scasim_human = []\n",
        "\t\tres_scasim_dnn = []\n",
        "\t\tuniform_central_scasim_scores = []\n",
        "\t\tuniform_scasim_scores = []\n",
        "\t\tuniform_nll_scores = []\n",
        "\t\tuniform_mse_dur_scores = []\n",
        "\t\tuniform_mse_land_pos_scores = []\n",
        "\t\tez_reader_central_scasim_scores = []\n",
        "\t\tez_reader_scasim_scores = []\n",
        "\t\tez_reader_nll_scores = []\n",
        "\t\tez_reader_mse_dur_scores = []\n",
        "\t\tez_reader_mse_land_pos_scores = []\n",
        "\t\tswift_central_scasim_scores = []\n",
        "\t\tswift_scasim_scores = []\n",
        "\t\tswift_nll_scores = []\n",
        "\t\tswift_mse_dur_scores = []\n",
        "\t\tswift_mse_land_pos_scores = []\n",
        "\t\tdnn.load_state_dict(torch.load(os.path.join(save_data_folder, f'CELER_3head_arch_NRS_{fold_indx}.pth'), map_location='cpu'))\n",
        "\t\tdnn.to(device)\n",
        "\t\tbatch_indx = 0\n",
        "\t\tfor batchh in test_dataloaderr:\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tsn_ids_test = batchh[\"sn_ids\"].to(device)\n",
        "\t\t\t\tsn_input_ids_test = batchh[\"sn_input_ids\"].to(device)\n",
        "\t\t\t\tsn_attention_mask_test = batchh[\"sn_attention_mask\"].to(device)\n",
        "\t\t\t\tword_ids_sn_test = batchh[\"word_ids_sn\"].to(device)\n",
        "\t\t\t\tsn_word_len_test = batchh[\"sn_word_len\"].to(device)\n",
        "\n",
        "\t\t\t\tsp_input_ids_test = batchh[\"sp_input_ids\"].to(device)\n",
        "\t\t\t\tsp_attention_mask_test = batchh[\"sp_attention_mask\"].to(device)\n",
        "\t\t\t\tword_ids_sp_test = batchh[\"word_ids_sp\"].to(device)\n",
        "\n",
        "\t\t\t\tsp_pos_test = batchh[\"sp_pos\"].to(device) # 28: '<Sep>', 29: '<'Pad'>'\n",
        "\t\t\t\tsp_landing_pos_test = batchh[\"sp_landing_pos\"].to(device)\n",
        "\t\t\t\tsp_fix_dur_test = (batchh[\"sp_fix_dur\"]/1000).to(device)\n",
        "\n",
        "\t\t\t\t#normalize gaze features\n",
        "\t\t\t\tmask = ~torch.eq(sp_fix_dur_test, 0)\n",
        "\t\t\t\tsp_fix_dur_test = (sp_fix_dur_test-fix_dur_mean)/fix_dur_std * mask\n",
        "\t\t\t\tsp_landing_pos_test = (sp_landing_pos_test - landing_pos_mean)/landing_pos_std * mask\n",
        "\t\t\t\tsp_fix_dur_test = torch.nan_to_num(sp_fix_dur_test)\n",
        "\t\t\t\tsp_landing_pos_test = torch.nan_to_num(sp_landing_pos_test)\n",
        "\t\t\t\tsn_word_len_test = (sn_word_len_test - sn_word_len_mean)/sn_word_len_std\n",
        "\t\t\t\tsn_word_len_test = torch.nan_to_num(sn_word_len_test)\n",
        "\n",
        "\t\t\t\tlocation_preds_test, duration_preds_test, landing_pos_preds_test, atten_weights_test = dnn(sn_emd=sn_input_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_mask=sn_attention_mask_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_emd=sp_input_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_pos=sp_pos_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sn=word_ids_sn_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tword_ids_sp=word_ids_sp_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_fix_dur=sp_fix_dur_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsp_landing_pos=sp_landing_pos_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_word_len = sn_word_len_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tsn_pred=None,\n",
        "                            sn_word_freq=None) #[batch, step, dec_o_dim]\n",
        "\n",
        "\n",
        "\t\t\t\t########## Evaluate location predictions ##########\n",
        "\t\t\t\tm = nn.Softmax(dim=2)\n",
        "\t\t\t\tlocation_preds_test = m(location_preds_test).detach().to('cpu').numpy()\n",
        "\t\t\t\t#prepare label and mask\n",
        "\t\t\t\tpad_mask_test, label_test = load_label(sp_pos_test, cf, le, 'cpu')\n",
        "\t\t\t\t#compute log likelihood for the batch samples\n",
        "\t\t\t\tres_batch = eval_log_llh(location_preds_test, label_test, pad_mask_test)\n",
        "\t\t\t\tres_llh.append(np.array(res_batch))\n",
        "\n",
        "\t\t\t\tuniform_output = construct_uniform_tensor(location_preds_test)\n",
        "\t\t\t\tuniform_nll = eval_log_llh(uniform_output, label_test, pad_mask_test)\n",
        "\t\t\t\tuniform_nll_scores.append(np.array(uniform_nll))\n",
        "\t\t\t\tprint(\"Uniform_nll\", np.mean(uniform_nll), uniform_nll)\n",
        "\n",
        "\t\t\t\tduration_preds_test = duration_preds_test.squeeze(-1)\n",
        "\t\t\t\tduration_labels_test = sp_fix_dur_test[:, :51]\n",
        "\t\t\t\ttest_mask = mask[:, :51]\n",
        "\t\t\t\tmse_dur = eval_mse(duration_preds_test, duration_labels_test, test_mask)\n",
        "\t\t\t\tprint(\"MSE for durations\", np.mean(mse_dur))\n",
        "\t\t\t\tres_mse_dur.append(np.array(mse_dur))\n",
        "\n",
        "\t\t\t\tlanding_pos_preds_test = landing_pos_preds_test.squeeze(-1)\n",
        "\t\t\t\tlanding_pos_labels_test = sp_landing_pos_test[:, :51]\n",
        "\t\t\t\tmse_landing_pos = eval_mse(landing_pos_preds_test, landing_pos_labels_test, test_mask)\n",
        "\t\t\t\tprint(\"MSE for landing positions\", np.mean(mse_landing_pos))\n",
        "\t\t\t\tres_mse_land_pos.append(np.array(mse_landing_pos))\n",
        "\n",
        "\t\t\t\tif bool(scanpath_gen_flag) == True:\n",
        "\t\t\t\t\tsn_len = (torch.max(torch.nan_to_num(word_ids_sn_test), dim=1)[0]+1-2).detach().to('cpu').numpy()\n",
        "\t\t\t\t\t# compute the scan path generated from the model when the first CLS token is given\n",
        "\t\t\t\t\tsp_dnn, _, dur_dnn, land_pos_dnn = dnn.scanpath_generation(sn_emd=sn_input_ids_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_mask=sn_attention_mask_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t word_ids_sn=word_ids_sn_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_word_len = sn_word_len_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t le=le,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sn_word_freq=None,\n",
        "                             sn_pred=None,\n",
        "                             sp_fix_dur=sp_fix_dur_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sp_landing_pos = sp_landing_pos_test,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t max_pred_len=cf['max_pred_len'])\n",
        "\n",
        "\n",
        "\t\t\t\t\tsp_dnn, sp_human = prepare_scanpath(sp_dnn.detach().to('cpu').numpy(),\n",
        "                                              dur_dnn.detach().to('cpu').numpy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tland_pos_dnn.detach().to('cpu').numpy(), sn_len, sp_pos_test,\n",
        "                                              sp_fix_dur_test, sp_landing_pos_test, cf, sn_ids_test, fix_dur_mean, fix_dur_std, landing_pos_mean, landing_pos_std)\n",
        "\n",
        "\t\t\t\t\tsp_dnn_list.extend(sp_dnn)\n",
        "\t\t\t\t\tsp_human_list.extend(sp_human)\n",
        "\n",
        "\t\t\t\t\tsp_dnn = filter_sp(sp_dnn)\n",
        "\t\t\t\t\tsp_human = filter_sp(sp_human)\n",
        "\n",
        "\t\t\t\t\tif len(sp_dnn[\"sent_id\"]) != 0:\n",
        "\t\t\t\t\t\t# evaluate only on sent that were read by multiple subjects\n",
        "\t\t\t\t\t\tsp_dnn = convert_sp_to_lists(sp_dnn)\n",
        "\t\t\t\t\t\tsp_human = convert_sp_to_lists(sp_human)\n",
        "\t\t\t\t\t\tsp_human = modify_landing_pos(sp_human.copy())\n",
        "\t\t\t\t\t\tsp_dnn = modify_landing_pos(sp_dnn.copy())\n",
        "\t\t\t\t\t\trandom_sp = sample_random_sp(\"CELER\", sp_human, 'data_splits/CELER_sent_dict.txt')\n",
        "\t\t\t\t\t\trandom_sp = convert_sp_to_lists(random_sp)\n",
        "\t\t\t\t\t\trandom_sp = modify_landing_pos(random_sp.copy())\n",
        "\n",
        "\n",
        "\t\t\t\t\t\tprint(\"######### Eyettention model evaluation ##########\")\n",
        "\t\t\t\t\t\tscasim_scores_dnn = compute_scasim(sp_dnn, sp_human)\n",
        "\t\t\t\t\t\tres_scasim_dnn.append(scasim_scores_dnn)\n",
        "\t\t\t\t\t\tprint(\"Mean scasim dnn\", np.mean(scasim_scores_dnn))\n",
        "\t\t\t\t\t\tscasim_scores_human = compute_scasim(sp_human, random_sp)\n",
        "\t\t\t\t\t\tres_scasim_human.append(scasim_scores_human)\n",
        "\t\t\t\t\t\tprint(\"Mean scasim human\", np.mean(scasim_scores_human))\n",
        "\n",
        "\t\t\t\t\t\tcentral_scasim_scores_dnn = compute_central_scasim(\"CELER_most_central_sp.txt\", sp_dnn, 'data_splits/CELER_sent_dict.txt')\n",
        "\t\t\t\t\t\tcentral_scasim_scores_human = compute_central_scasim(\"CELER_most_central_sp.txt\", sp_human, 'data_splits/CELER_sent_dict.txt')\n",
        "\t\t\t\t\t\tres_central_scasim_dnn.append(np.array(central_scasim_scores_dnn))\n",
        "\t\t\t\t\t\tres_central_scasim_human.append(np.array(central_scasim_scores_human))\n",
        "\t\t\t\t\t\tprint(\"Mean central scasim dnn\", np.mean(central_scasim_scores_dnn))\n",
        "\t\t\t\t\t\tprint(\"Mean central scasim human\", np.mean(central_scasim_scores_human))\n",
        "\n",
        "\t\t\t\t\t\tprint(\"######### Uniform baseline model evaluation ##########\")\n",
        "\t\t\t\t\t\tmean_dur_uniform, std_dur_uniform, mean_land_pos_uniform, std_land_pos_uniform = compute_mean_std_uniform(\"drive/MyDrive/baseline/Uniform/CELER_uniform_results.csv\")\n",
        "\t\t\t\t\t\tuniform_central_scasim, uniform_scasim, dur_mse_scores, land_pos_mse_scores = evaluate_uniform_model(\"CELER\", sp_human, landing_pos_mean, landing_pos_std, fix_dur_mean, fix_dur_std, mean_land_pos_uniform, std_land_pos_uniform, mean_dur_uniform, std_dur_uniform)\n",
        "\t\t\t\t\t\tuniform_central_scasim_scores.append(np.array(uniform_central_scasim))\n",
        "\t\t\t\t\t\tuniform_scasim_scores.append(np.array(uniform_scasim))\n",
        "\t\t\t\t\t\tuniform_mse_dur_scores.append(np.array(dur_mse_scores))\n",
        "\t\t\t\t\t\tuniform_mse_land_pos_scores.append(np.array(land_pos_mse_scores))\n",
        "\t\t\t\t\t\tprint(\"Uniform mean central Scasim score:\", np.mean(uniform_central_scasim))\n",
        "\t\t\t\t\t\tprint(\"Uniform mean Scasim score:\", np.mean(uniform_scasim))\n",
        "\t\t\t\t\t\tprint(\"MSE for uniform durations\", np.mean(dur_mse_scores))\n",
        "\t\t\t\t\t\tprint(\"MSE for uniform landing pos\", np.mean(land_pos_mse_scores))\n",
        "\n",
        "\t\t\t\t\t\tprint(\"######### E-Z Reader model evaluation ##########\")\n",
        "\t\t\t\t\t\tmean_dur_ez_reader, std_dur_ez_reader, mean_land_pos_ez_reader, std_land_pos_ez_reader = compute_mean_std_ez_reader(\"drive/MyDrive/baseline/CELERSimulationResultsNew.txt\")\n",
        "\t\t\t\t\t\tprint(mean_dur_ez_reader, std_dur_ez_reader, mean_land_pos_ez_reader, std_land_pos_ez_reader)\n",
        "\t\t\t\t\t\tcentral_scasim_ez_reader, scasim_ez_reader, dur_mse_ez_reader, land_pos_mse_ez_reader = evaluate_ez_reader(\"CELER\", \"drive/MyDrive/baseline/CELERSimulationResultsNew.txt\", sp_human, landing_pos_mean, landing_pos_std, fix_dur_mean, fix_dur_std, mean_land_pos_ez_reader, std_land_pos_ez_reader, mean_dur_ez_reader, std_dur_ez_reader)\n",
        "\t\t\t\t\t\tez_reader_central_scasim_scores.append(np.array(central_scasim_ez_reader))\n",
        "\t\t\t\t\t\tprint(\"Central scasim E-Z Reader\", np.mean(central_scasim_ez_reader))\n",
        "\t\t\t\t\t\tez_reader_scasim_scores.append(np.array(scasim_ez_reader))\n",
        "\t\t\t\t\t\tprint(\"Mean scasim score E-Z Reader\", np.mean(scasim_ez_reader))\n",
        "\t\t\t\t\t\tez_reader_mse_dur_scores.append(np.array(dur_mse_ez_reader))\n",
        "\t\t\t\t\t\tprint(\"MSE for durations E-Z Reader\", np.mean(dur_mse_ez_reader))\n",
        "\t\t\t\t\t\tez_reader_mse_land_pos_scores.append(np.array(land_pos_mse_ez_reader))\n",
        "\t\t\t\t\t\tprint(\"MSE for landing pos E-Z Reader\", np.mean(land_pos_mse_ez_reader))\n",
        "\n",
        "\t\t\t\t\t\tprint(\"######### SWIFT model evaluation ##########\")\n",
        "\t\t\t\t\t\tmean_dur_swift, std_dur_swift, mean_land_pos_swift, std_land_pos_swift = compute_mean_std_swift()\n",
        "\t\t\t\t\t\tcentral_scasim_swift, scasim_swift, dur_mse_swift, land_pos_mse_swift = evaluate_swift(\"celer\", sp_human, landing_pos_mean, landing_pos_std, fix_dur_mean, fix_dur_std, mean_land_pos_swift, std_land_pos_swift, mean_dur_swift, std_dur_swift)\n",
        "\t\t\t\t\t\tswift_central_scasim_scores.append(np.array(central_scasim_swift))\n",
        "\t\t\t\t\t\tprint(\"Central scasim SWIFT\", np.mean(central_scasim_swift))\n",
        "\t\t\t\t\t\tswift_scasim_scores.append(np.array(scasim_swift))\n",
        "\t\t\t\t\t\tprint(\"Mean scasim score SWIFT\", np.mean(scasim_swift))\n",
        "\t\t\t\t\t\tswift_mse_dur_scores.append(np.array(dur_mse_swift))\n",
        "\t\t\t\t\t\tprint(\"MSE for durations SWIFT\", np.mean(dur_mse_swift))\n",
        "\t\t\t\t\t\tswift_mse_land_pos_scores.append(np.array(land_pos_mse_swift))\n",
        "\t\t\t\t\t\tprint(\"MSE for landing pos SWIFT\", np.mean(land_pos_mse_swift))\n",
        "\t\t\t\t\t\tswift_nll = evaluate_swift_nll(\"baseline/swift/SWIFT_NLL.xlsx\", sp_human)\n",
        "\t\t\t\t\t\tswift_nll_scores.append(np.array(swift_nll))\n",
        "\t\t\t\t\t\tif len(swift_nll) != 0:\n",
        "\t\t\t\t\t\t\tprint(\"NLL SWIFT\", np.mean(swift_nll))\n",
        "\n",
        "\t\t\t\tbatch_indx +=1\n",
        "\n",
        "\t\tres_llh = np.concatenate(res_llh).ravel()\n",
        "\t\tloss_dict['test_ll'].append(res_llh)\n",
        "\t\tres_mse_dur = np.concatenate(res_mse_dur).ravel()\n",
        "\t\tloss_dict['test_mse_dur'].append(res_mse_dur)\n",
        "\t\tres_mse_land_pos = np.concatenate(res_mse_land_pos).ravel()\n",
        "\t\tloss_dict['test_mse_land_pos'].append(res_mse_land_pos)\n",
        "\n",
        "\t\tres_central_scasim_dnn = np.concatenate(res_central_scasim_dnn).ravel()\n",
        "\t\tloss_dict['central_scasim_dnn'].append(res_central_scasim_dnn)\n",
        "\t\tres_central_scasim_human = np.concatenate(res_central_scasim_human).ravel()\n",
        "\t\tloss_dict['central_scasim_human'].append(res_central_scasim_human)\n",
        "\t\tres_scasim_dnn = np.concatenate(res_scasim_dnn).ravel()\n",
        "\t\tloss_dict['scasim_dnn'].append(res_scasim_dnn)\n",
        "\t\tres_scasim_human = np.concatenate(res_scasim_human).ravel()\n",
        "\t\tloss_dict['scasim_human'].append(res_scasim_human)\n",
        "\n",
        "\t\tloss_dict['fix_dur_mean'] = fix_dur_mean\n",
        "\t\tloss_dict['fix_dur_std'] = fix_dur_std\n",
        "\t\tloss_dict['landing_pos_mean'] = landing_pos_mean\n",
        "\t\tloss_dict['landing_pos_std'] = landing_pos_std\n",
        "\t\tloss_dict['sn_word_len_mean'] = sn_word_len_mean\n",
        "\t\tloss_dict['sn_word_len_std'] = sn_word_len_std\n",
        "\n",
        "\t\tuniform_central_scasim_scores = np.concatenate(uniform_central_scasim_scores).ravel()\n",
        "\t\tloss_dict['uniform_central_scasim'].append(uniform_central_scasim_scores)\n",
        "\t\tuniform_scasim_scores = np.concatenate(uniform_scasim_scores).ravel()\n",
        "\t\tloss_dict['uniform_scasim'].append(uniform_scasim_scores)\n",
        "\n",
        "\t\tuniform_mse_dur_scores = np.concatenate(uniform_mse_dur_scores).ravel()\n",
        "\t\tloss_dict['uniform_mse_dur'].append(uniform_mse_dur_scores)\n",
        "\t\tuniform_mse_land_pos_scores = np.concatenate(uniform_mse_land_pos_scores).ravel()\n",
        "\t\tloss_dict['uniform_mse_land_pos'].append(uniform_mse_land_pos_scores)\n",
        "\t\tuniform_nll_scores = np.concatenate(uniform_nll_scores).ravel()\n",
        "\t\tloss_dict['uniform_nll'].append(uniform_nll_scores)\n",
        "\n",
        "\t\tez_reader_central_scasim_scores = np.concatenate(ez_reader_central_scasim_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_central_scasim'].append(ez_reader_central_scasim_scores)\n",
        "\t\tez_reader_scasim_scores = np.concatenate(ez_reader_scasim_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_scasim'].append(ez_reader_scasim_scores)\n",
        "\n",
        "\t\tez_reader_mse_dur_scores = np.concatenate(ez_reader_mse_dur_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_mse_dur'].append(ez_reader_mse_dur_scores)\n",
        "\t\tez_reader_mse_land_pos_scores = np.concatenate(ez_reader_mse_land_pos_scores).ravel()\n",
        "\t\tloss_dict['ez_reader_mse_land_pos'].append(ez_reader_mse_land_pos_scores)\n",
        "\n",
        "\t\tswift_mse_dur_scores = np.concatenate(swift_mse_dur_scores).ravel()\n",
        "\t\tloss_dict['SWIFT_mse_dur'].append(swift_mse_dur_scores)\n",
        "\t\tswift_mse_land_pos_scores = np.concatenate(swift_mse_land_pos_scores).ravel()\n",
        "\t\tloss_dict['SWIFT_mse_land_pos'].append(swift_mse_land_pos_scores)\n",
        "\t\tswift_nll_scores = np.concatenate(swift_nll_scores).ravel()\n",
        "\t\tloss_dict['SWIFT_nll'].append(swift_nll_scores)\n",
        "\t\tswift_central_scasim_scores = np.concatenate(swift_central_scasim_scores).ravel()\n",
        "\t\tloss_dict['SWIFT_central_scasim'].append(swift_central_scasim_scores)\n",
        "\t\tswift_scasim_scores = np.concatenate(swift_scasim_scores).ravel()\n",
        "\t\tloss_dict['SWIFT_scasim'].append(swift_scasim_scores)\n",
        "\n",
        "\t\tprint('Test likelihood is {}'.format(np.mean(res_llh)))\n",
        "\t\tloss_dict['test_ll_SE'].append(np.std(res_llh)/ np.sqrt(len(res_llh)))\n",
        "\t\tprint(\"Standard error for NLL\", np.std(res_llh)/ np.sqrt(len(res_llh)))\n",
        "\n",
        "\t\tprint('Test MSE for durations is {}'.format(np.mean(res_mse_dur)))\n",
        "\t\tloss_dict['test_mse_dur_SE'].append(np.std(res_mse_dur)/ np.sqrt(len(res_mse_dur)))\n",
        "\t\tprint(\"Standard error for MSE dur\", np.std(res_mse_dur) / np.sqrt(len(res_mse_dur)))\n",
        "\n",
        "\t\tprint('Test MSE for landing positions is {}'.format(np.mean(res_mse_land_pos)))\n",
        "\t\tloss_dict['test_mse_land_pos_SE'].append(np.std(res_mse_land_pos)/ np.sqrt(len(res_mse_land_pos)))\n",
        "\t\tprint(\"Standard error for MSE land pos\", np.std(res_mse_land_pos) / np.sqrt(len(res_mse_land_pos)))\n",
        "\n",
        "\t\tprint(\"Central Scasim dnn\", np.mean(loss_dict['central_scasim_dnn']))\n",
        "\t\tloss_dict['central_scasim_dnn_SE'].append(np.std(res_central_scasim_dnn)/ np.sqrt(len(res_central_scasim_dnn)))\n",
        "\t\tprint(\"Standard error for Central scasim DNN\", np.std(res_central_scasim_dnn) / np.sqrt(len(res_central_scasim_dnn)))\n",
        "\n",
        "\t\tprint(\"Central Scasim human\", np.mean(loss_dict['central_scasim_human']))\n",
        "\t\tloss_dict['central_scasim_human_SE'].append(np.std(res_central_scasim_human)/ np.sqrt(len(res_central_scasim_human)))\n",
        "\t\tprint(\"Standard error for Central scasim human\", np.std(res_central_scasim_human) / np.sqrt(len(res_central_scasim_human)))\n",
        "\n",
        "\t\tprint(\"Scasim dnn\", np.mean(loss_dict['scasim_dnn']))\n",
        "\t\tloss_dict['scasim_dnn_SE'].append(np.std(res_scasim_dnn)/ np.sqrt(len(res_scasim_dnn)))\n",
        "\t\tprint(\"Standard error for scasim dnn\", np.std(res_scasim_dnn) / np.sqrt(len(res_scasim_dnn)))\n",
        "\n",
        "\t\tprint(\"Scasim human\", np.mean(loss_dict['scasim_human']))\n",
        "\t\tloss_dict['scasim_human_SE'].append(np.std(res_scasim_human)/ np.sqrt(len(res_scasim_human)))\n",
        "\t\tprint(\"Standard error for scasim human\", np.std(res_scasim_human) / np.sqrt(len(res_scasim_human)))\n",
        "\n",
        "\t\tprint(\"Uniform central scasim\", np.mean(loss_dict['uniform_central_scasim']))\n",
        "\t\tloss_dict['uniform_central_scasim_SE'].append(np.std(uniform_central_scasim_scores)/ np.sqrt(len(uniform_central_scasim_scores)))\n",
        "\t\tprint(\"Standard error for uniform central scasim\", np.std(uniform_central_scasim_scores) / np.sqrt(len(uniform_central_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform scasim\", np.mean(loss_dict['uniform_scasim']))\n",
        "\t\tloss_dict['uniform_scasim_SE'].append(np.std(uniform_scasim_scores)/ np.sqrt(len(uniform_scasim_scores)))\n",
        "\t\tprint(\"Standard error for uniform scasim\", np.std(uniform_scasim_scores) / np.sqrt(len(uniform_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform MSE durations\", np.mean(loss_dict['uniform_mse_dur']))\n",
        "\t\tloss_dict['uniform_mse_dur_SE'].append(np.std(dur_mse_scores)/ np.sqrt(len(dur_mse_scores)))\n",
        "\t\tprint(\"Standard error for uniform MSE durations\", np.std(dur_mse_scores) / np.sqrt(len(dur_mse_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform MSE landing pos\", np.mean(loss_dict['uniform_mse_land_pos']))\n",
        "\t\tloss_dict['uniform_mse_land_pos_SE'].append(np.std(land_pos_mse_scores)/ np.sqrt(len(land_pos_mse_scores)))\n",
        "\t\tprint(\"Standard error for uniform MSE landing pos\", np.std(land_pos_mse_scores) / np.sqrt(len(land_pos_mse_scores)))\n",
        "\n",
        "\t\tprint(\"Uniform NLL\", np.mean(loss_dict['uniform_nll']))\n",
        "\t\tloss_dict['uniform_nll_SE'].append(np.std(uniform_nll_scores)/ np.sqrt(len(uniform_nll_scores)))\n",
        "\t\tprint(\"Standard error for uniform NLL\", np.std(uniform_nll_scores) / np.sqrt(len(uniform_nll_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader central scasim\", np.mean(loss_dict['ez_reader_central_scasim']))\n",
        "\t\tloss_dict['ez_reader_central_scasim_SE'].append(np.std(ez_reader_central_scasim_scores)/ np.sqrt(len(ez_reader_central_scasim_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader central scasim\", np.std(ez_reader_central_scasim_scores) / np.sqrt(len(ez_reader_central_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader scasim\", np.mean(loss_dict['ez_reader_scasim']))\n",
        "\t\tloss_dict['ez_reader_scasim_SE'].append(np.std(ez_reader_scasim_scores)/ np.sqrt(len(ez_reader_scasim_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader scasim\", np.std(ez_reader_scasim_scores) / np.sqrt(len(ez_reader_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader MSE durations\", np.mean(loss_dict['ez_reader_mse_dur']))\n",
        "\t\tloss_dict['ez_reader_mse_dur_SE'].append(np.std(ez_reader_mse_dur_scores)/ np.sqrt(len(ez_reader_mse_dur_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader MSE durations\", np.std(ez_reader_mse_dur_scores) / np.sqrt(len(ez_reader_mse_dur_scores)))\n",
        "\n",
        "\t\tprint(\"E-Z Reader MSE landing pos\", np.mean(loss_dict['ez_reader_mse_land_pos']))\n",
        "\t\tloss_dict['ez_reader_mse_land_pos_SE'].append(np.std(ez_reader_mse_land_pos_scores)/ np.sqrt(len(ez_reader_mse_land_pos_scores)))\n",
        "\t\tprint(\"Standard error for E-Z Reader MSE landing pos\", np.std(ez_reader_mse_land_pos_scores) / np.sqrt(len(ez_reader_mse_land_pos_scores)))\n",
        "\n",
        "\t\tprint(\"SWIFT NLL\", np.mean(loss_dict['SWIFT_nll']))\n",
        "\t\tloss_dict['SWIFT_nll_SE'].append(np.std(swift_nll_scores)/ np.sqrt(len(swift_nll_scores)))\n",
        "\t\tprint(\"Standard error for SWIFT NLL\", np.std(swift_nll_scores) / np.sqrt(len(swift_nll_scores)))\n",
        "\n",
        "\t\tprint(\"SWIFT central scasim\", np.mean(loss_dict['SWIFT_central_scasim']))\n",
        "\t\tloss_dict['SWIFT_central_scasim_SE'].append(np.std(swift_central_scasim_scores)/ np.sqrt(len(swift_central_scasim_scores)))\n",
        "\t\tprint(\"Standard error for SWIFT central scasim\", np.std(swift_central_scasim_scores) / np.sqrt(len(swift_central_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"SWIFT scasim\", np.mean(loss_dict['SWIFT_scasim']))\n",
        "\t\tloss_dict['SWIFT_scasim_SE'].append(np.std(swift_scasim_scores)/ np.sqrt(len(swift_scasim_scores)))\n",
        "\t\tprint(\"Standard error for SWIFT scasim\", np.std(swift_scasim_scores) / np.sqrt(len(swift_scasim_scores)))\n",
        "\n",
        "\t\tprint(\"SWIFT MSE durations\", np.mean(loss_dict['SWIFT_mse_dur']))\n",
        "\t\tloss_dict['SWIFT_mse_dur_SE'].append(np.std(swift_mse_dur_scores)/ np.sqrt(len(swift_mse_dur_scores)))\n",
        "\t\tprint(\"Standard error for SWIFT MSE durations\", np.std(swift_mse_dur_scores) / np.sqrt(len(swift_mse_dur_scores)))\n",
        "\n",
        "\t\tprint(\"SWIFT MSE landing pos\", np.mean(loss_dict['SWIFT_mse_land_pos']))\n",
        "\t\tloss_dict['SWIFT_mse_land_pos_SE'].append(np.std(swift_mse_land_pos_scores)/ np.sqrt(len(swift_mse_land_pos_scores)))\n",
        "\t\tprint(\"Standard error for SWIFT MSE landing pos\", np.std(swift_mse_land_pos_scores) / np.sqrt(len(swift_mse_land_pos_scores)))\n",
        "\n",
        "\t\t#save results\n",
        "\t\twith open('{}/res_CELER_eyettention_Fold{}.pickle'.format(save_data_folder, fold_indx), 'wb') as handle:\n",
        "\t\t\tpickle.dump(loss_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\t\tfold_indx += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Analysing the results\n",
        "test_ll_all_folds = []\n",
        "test_mse_dur_all_folds = []\n",
        "test_mse_land_pos_all_folds = []\n",
        "scasim_dnn_all_folds = []\n",
        "scasim_human_all_folds = []\n",
        "central_scasim_dnn_all_folds = []\n",
        "central_scasim_human_all_folds = []\n",
        "\n",
        "uniform_ll_all_folds = []\n",
        "uniform_mse_dur_all_folds = []\n",
        "uniform_mse_land_pos_all_folds = []\n",
        "uniform_scasim_all_folds = []\n",
        "uniform_central_scasim_all_folds = []\n",
        "\n",
        "ez_reader_mse_dur_all_folds = []\n",
        "ez_reader_mse_land_pos_all_folds = []\n",
        "ez_reader_scasim_all_folds = []\n",
        "ez_reader_central_scasim_all_folds = []\n",
        "\n",
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/celer/NRS/res_CELER_eyettention_Fold{fold_index}.pickle', 'rb') as original:\n",
        "      reader_results = pickle.load(original)\n",
        "      test_ll_all_folds.extend(reader_results['test_ll'][0])\n",
        "      test_mse_dur_all_folds.extend(reader_results['test_mse_dur'][0])\n",
        "      test_mse_land_pos_all_folds.extend(reader_results['test_mse_land_pos'][0])\n",
        "      scasim_dnn_all_folds.extend(reader_results['scasim_dnn'][0])\n",
        "      scasim_human_all_folds.extend(reader_results['scasim_human'][0])\n",
        "      central_scasim_dnn_all_folds.extend(reader_results['central_scasim_dnn'][0])\n",
        "      central_scasim_human_all_folds.extend(reader_results['central_scasim_human'][0])\n",
        "\n",
        "      uniform_ll_all_folds.extend(reader_results['uniform_nll'][0])\n",
        "      uniform_mse_dur_all_folds.extend(reader_results['uniform_mse_dur'][0])\n",
        "      uniform_mse_land_pos_all_folds.extend(reader_results['uniform_mse_land_pos'][0])\n",
        "      uniform_scasim_all_folds.extend(reader_results['uniform_scasim'][0])\n",
        "      uniform_central_scasim_all_folds.extend(reader_results['uniform_central_scasim'][0])\n",
        "\n",
        "      ez_reader_mse_dur_all_folds.extend(reader_results['ez_reader_mse_dur'][0])\n",
        "      ez_reader_mse_land_pos_all_folds.extend(reader_results['ez_reader_mse_land_pos'][0])\n",
        "      ez_reader_scasim_all_folds.extend(reader_results['ez_reader_scasim'][0])\n",
        "      ez_reader_central_scasim_all_folds.extend(reader_results['ez_reader_central_scasim'][0])\n",
        "\n",
        "      fold_index += 1\n",
        "\n",
        "# Calculate overall statistics\n",
        "def calculate_mean_and_se(values):\n",
        "    mean_val = np.mean(values)\n",
        "    se_val = np.std(values) / np.sqrt(len(values))\n",
        "    return mean_val, se_val\n",
        "\n",
        "mean_nll, se_nll = calculate_mean_and_se(test_ll_all_folds)\n",
        "mean_mse_dur, se_mse_dur = calculate_mean_and_se(test_mse_dur_all_folds)\n",
        "mean_mse_land_pos, se_mse_land_pos = calculate_mean_and_se(test_mse_land_pos_all_folds)\n",
        "mean_scasim, se_scasim = calculate_mean_and_se(scasim_dnn_all_folds)\n",
        "mean_central_scasim, se_central_scasim = calculate_mean_and_se(central_scasim_dnn_all_folds)\n",
        "mean_scasim_human, se_scasim_human = calculate_mean_and_se(scasim_human_all_folds)\n",
        "mean_central_scasim_human, se_central_scasim_human = calculate_mean_and_se(central_scasim_human_all_folds)\n",
        "\n",
        "mean_nll_uniform, se_nll_uniform = calculate_mean_and_se(uniform_ll_all_folds)\n",
        "mean_mse_dur_uniform, se_mse_dur_uniform = calculate_mean_and_se(uniform_mse_dur_all_folds)\n",
        "mean_mse_land_pos_uniform, se_mse_land_pos_uniform = calculate_mean_and_se(uniform_mse_land_pos_all_folds)\n",
        "mean_scasim_uniform, se_scasim_uniform = calculate_mean_and_se(uniform_scasim_all_folds)\n",
        "mean_central_scasim_uniform, se_central_scasim_uniform = calculate_mean_and_se(uniform_central_scasim_all_folds)\n",
        "\n",
        "mean_nll_ez_reader, se_nll_ez_reader = calculate_mean_and_se(ez_reader_mse_dur_all_folds)\n",
        "mean_mse_dur_ez_reader, se_mse_dur_ez_reader = calculate_mean_and_se(ez_reader_mse_dur_all_folds)\n",
        "mean_mse_land_pos_ez_reader, se_mse_land_pos_ez_reader = calculate_mean_and_se(ez_reader_mse_land_pos_all_folds)\n",
        "mean_scasim_ez_reader, se_scasim_ez_reader = calculate_mean_and_se(ez_reader_scasim_all_folds)\n",
        "mean_central_scasim_ez_reader, se_central_scasim_ez_reader = calculate_mean_and_se(ez_reader_central_scasim_all_folds)\n",
        "\n",
        "\n",
        "print(\"Overall Mean NLL:\", mean_nll)\n",
        "print(\"Overall Standard error NLL:\", se_nll)\n",
        "\n",
        "print(\"Overall Mean MSE dur:\", mean_mse_dur)\n",
        "print(\"Overall Standard error MSE dur:\", se_mse_dur)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos:\", mean_mse_land_pos)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos)\n",
        "\n",
        "print(\"Overall Mean scasim:\", mean_scasim)\n",
        "print(\"Overall Standard error scasim:\", se_scasim)\n",
        "\n",
        "print(\"Overall Mean Central scasim:\", mean_central_scasim)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim)\n",
        "\n",
        "print(\"Human Mean scasim:\", mean_scasim_human)\n",
        "print(\"Human Standard error scasim:\", se_scasim_human)\n",
        "\n",
        "print(\"Human Mean central scasim:\", mean_central_scasim_human)\n",
        "print(\"Human Standard error central scasim:\", se_central_scasim_human)\n",
        "\n",
        "\n",
        "print(\"Overall Mean NLL Uniform:\", mean_nll_uniform)\n",
        "print(\"Overall Standard error NLL:\", se_nll_uniform)\n",
        "\n",
        "print(\"Overall Mean MSE dur Uniform:\", mean_mse_dur_uniform)\n",
        "print(\"Overall Standard error MSE dur Uniform:\", se_mse_dur_uniform)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos Uniform:\", mean_mse_land_pos_uniform)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos_uniform)\n",
        "\n",
        "print(\"Overall Mean scasim Uniform:\", mean_scasim_uniform)\n",
        "print(\"Overall Standard error scasim:\", se_scasim_uniform)\n",
        "\n",
        "print(\"Overall Mean Central scasim Uniform:\", mean_central_scasim_uniform)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim_uniform)\n",
        "\n",
        "\n",
        "print(\"Overall Mean MSE dur E-Z Reader:\", mean_mse_dur_ez_reader)\n",
        "print(\"Overall Standard error MSE dur E-Z Reader:\", se_mse_dur_ez_reader)\n",
        "\n",
        "print(\"Overall Mean MSE landing pos E-Z Reader:\", mean_mse_land_pos_ez_reader)\n",
        "print(\"Overall Standard error MSE landing pos:\", se_mse_land_pos_ez_reader)\n",
        "\n",
        "print(\"Overall Mean scasim E-Z Reader:\", mean_scasim_ez_reader)\n",
        "print(\"Overall Standard error scasim:\", se_scasim_ez_reader)\n",
        "\n",
        "print(\"Overall Mean Central scasim E-Z Reader:\", mean_central_scasim_ez_reader)\n",
        "print(\"Overall Standard error Central scasim:\", se_central_scasim_ez_reader)"
      ],
      "metadata": {
        "id": "EQRA2Ejss5ZX"
      },
      "id": "EQRA2Ejss5ZX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "fold_index = 0\n",
        "for i in range(5):\n",
        "  with open(f'drive/MyDrive/results/celer/NRS/res_CELER_eyettention_Fold{fold_index}.pickle', 'rb') as handle:\n",
        "      print(fold_index)\n",
        "      fold_results = pickle.load(handle)\n",
        "\n",
        "      # NLL\n",
        "      test_ll = fold_results['test_ll'][0]\n",
        "\n",
        "  #    with open(f'drive/MyDrive/results/celer/New_Sentence/res_CELER_original_eyettention_fold{fold_index}.pickle', 'rb') as original:\n",
        "   #     original_results = pickle.load(original)\n",
        "   #     original_ll = original_results['test_ll'][0]\n",
        "    #    t_statistic, p_value = stats.ttest_ind(test_ll, original_ll)\n",
        "        # Interpretation based on p-value\n",
        "  #      if p_value < 0.05:\n",
        "   #       print(\"Reject null hypothesis: Statistically significant difference in NLL scores between Eyettention 2.0 and original Eyettention scanpaths.\")\n",
        "    #    else:\n",
        "     #     print(\"Fail to reject null hypothesis: No statistically significant difference found between Eyettention 2.0 and original Eyettention NLL scores.\")\n",
        "\n",
        "      uniform_nll = fold_results['uniform_nll'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(test_ll, uniform_nll)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in NLL scores between predicted and uniformly generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between the predicted and uniform NLL scores.\")\n",
        "\n",
        "      # Scasim\n",
        "      scasim_dnn = fold_results['scasim_dnn'][0]\n",
        "      scasim_human = fold_results['scasim_human'][0]\n",
        "      ez_reader_scasim = fold_results['ez_reader_scasim'][0]\n",
        "      uniform_scasim = fold_results['uniform_scasim'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(scasim_dnn, ez_reader_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader generated Scasim scores.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(scasim_dnn, uniform_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform Scasim scores.\")\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(scasim_dnn, scasim_human)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Scasim scores between predicted and the human scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and human Scasim scores.\")\n",
        "\n",
        "      # Central Scasim\n",
        "      central_scasim_dnn = fold_results['central_scasim_dnn'][0]\n",
        "      central_scasim_human = fold_results['central_scasim_human'][0]\n",
        "      ez_reader_central_scasim = fold_results['ez_reader_central_scasim'][0]\n",
        "      uniform_central_scasim = fold_results['uniform_central_scasim'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, ez_reader_central_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader Central Scasim scores.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, uniform_central_scasim)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform Central Scasim scores.\")\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(central_scasim_dnn, central_scasim_human)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in Central Scasim scores between predicted and the human scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and human Central Scasim scores.\")\n",
        "\n",
        "      # MSE durations\n",
        "      test_mse_dur = fold_results['test_mse_dur'][0]\n",
        "      ez_reader_mse_dur = fold_results['ez_reader_mse_dur'][0]\n",
        "      uniform_mse_dur = fold_results['uniform_mse_dur'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_dur, ez_reader_mse_dur)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader MSE scores for durations.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_dur, uniform_mse_dur)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for durations between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform MSE scores for durations.\")\n",
        "\n",
        "      # MSE landing pos\n",
        "      test_mse_land_pos = fold_results['test_mse_land_pos'][0]\n",
        "      ez_reader_mse_land_pos = fold_results['ez_reader_mse_land_pos'][0]\n",
        "      uniform_mse_land_pos = fold_results['uniform_mse_land_pos'][0]\n",
        "\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_land_pos, ez_reader_mse_land_pos)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and E-Z Reader generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and E-Z Reader MSE scores for landing pos.\")\n",
        "      t_statistic, p_value = stats.ttest_ind(test_mse_land_pos, uniform_mse_land_pos)\n",
        "      # Interpretation based on p-value\n",
        "      if p_value < 0.05:\n",
        "        print(\"Reject null hypothesis: Statistically significant difference in MSE scores for landing pos between predicted and the uniform generated scanpaths.\")\n",
        "      else:\n",
        "        print(\"Fail to reject null hypothesis: No statistically significant difference between predicted and uniform MSE scores for landing pos.\")\n",
        "\n",
        "      fold_index += 1"
      ],
      "metadata": {
        "id": "s1DBHokNu3R_"
      },
      "id": "s1DBHokNu3R_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}